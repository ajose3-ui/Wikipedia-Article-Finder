{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajose3-ui/Wikipedia-Article-Finder/blob/main/WIKIPROJECT_THING.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run this if you want to save the generated CSVs to your drive:"
      ],
      "metadata": {
        "id": "fplaj7XbpZY0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhFSRXkk8gJi",
        "cellView": "form",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install these so that everything works properly:"
      ],
      "metadata": {
        "id": "75LFKT9tplhq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "!pip install textblob vaderSentiment textstat\n",
        "!pip install itables"
      ],
      "metadata": {
        "cellView": "form",
        "id": "5vChMDPsXFAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following tool may help you find articles. Please keep in mind that it may not always be accurate."
      ],
      "metadata": {
        "id": "tDu45yFyyYSE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output, HTML\n",
        "import requests\n",
        "import time\n",
        "import re\n",
        "import random\n",
        "\n",
        "def search_wikipedia_multiple(query, top_n=3):\n",
        "    \"\"\"\n",
        "    Search Wikipedia, get top 20 results, randomly sample 3 for variety.\n",
        "    \"\"\"\n",
        "    url = \"https://en.wikipedia.org/w/api.php\"\n",
        "\n",
        "    params = {\n",
        "        \"action\": \"query\",\n",
        "        \"list\": \"search\",\n",
        "        \"srsearch\": query,\n",
        "        \"srlimit\": 10,  # Get top 20 results\n",
        "        \"format\": \"json\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        r = requests.get(url, params=params, headers=HEADERS, timeout=30)\n",
        "        data = r.json()\n",
        "\n",
        "        all_results = []\n",
        "        for item in data.get(\"query\", {}).get(\"search\", []):\n",
        "            all_results.append({\n",
        "                \"title\": item[\"title\"],\n",
        "                \"snippet\": item.get(\"snippet\", \"\")\n",
        "            })\n",
        "\n",
        "        # Randomly sample 3 from the results\n",
        "        if len(all_results) >= top_n:\n",
        "            return random.sample(all_results, top_n)\n",
        "        else:\n",
        "            return all_results\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error searching Wikipedia: {e}\")\n",
        "        return []\n",
        "\n",
        "def get_conceptnet_related(query, limit=15):\n",
        "    \"\"\"\n",
        "    Get related concepts from ConceptNet API.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        encoded_query = query.replace(' ', '_').lower()\n",
        "        url = f\"http://api.conceptnet.io/query?node=/c/en/{encoded_query}&limit=100\"\n",
        "\n",
        "        r = requests.get(url, timeout=30)\n",
        "\n",
        "        if r.status_code != 200:\n",
        "            return []\n",
        "\n",
        "        data = r.json()\n",
        "\n",
        "        related_concepts = set()\n",
        "\n",
        "        for edge in data.get(\"edges\", []):\n",
        "            weight = edge.get(\"weight\", 0)\n",
        "\n",
        "            if weight < 1.0:\n",
        "                continue\n",
        "\n",
        "            start = edge.get(\"start\", {})\n",
        "            end = edge.get(\"end\", {})\n",
        "\n",
        "            for node in [start, end]:\n",
        "                label = node.get(\"label\", \"\")\n",
        "                language = node.get(\"language\", \"\")\n",
        "                node_term = node.get(\"term\", \"\")\n",
        "\n",
        "                if language == \"en\":\n",
        "                    if not label and node_term:\n",
        "                        term_parts = node_term.split('/')\n",
        "                        if len(term_parts) >= 3:\n",
        "                            label = term_parts[-1].replace('_', ' ')\n",
        "\n",
        "                    if label:\n",
        "                        clean_label = label.replace(\"_\", \" \").strip()\n",
        "                        if (clean_label.lower() != query.lower() and\n",
        "                            len(clean_label) > 2 and\n",
        "                            clean_label not in related_concepts):\n",
        "                            related_concepts.add(clean_label)\n",
        "\n",
        "        result = sorted(list(related_concepts))[:limit]\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing ConceptNet: {e}\")\n",
        "        return []\n",
        "\n",
        "def make_wikipedia_link(title):\n",
        "    \"\"\"\n",
        "    Create a clickable HTML link to a Wikipedia article.\n",
        "    \"\"\"\n",
        "    url_title = title.replace(' ', '_')\n",
        "    url = f\"https://en.wikipedia.org/wiki/{url_title}\"\n",
        "    return f'<a href=\"{url}\" target=\"_blank\">{title}</a>'\n",
        "\n",
        "def get_see_also_links(article_title, max_links=10):\n",
        "    \"\"\"\n",
        "    Extract links from See Also section.\n",
        "    \"\"\"\n",
        "    url = \"https://en.wikipedia.org/w/api.php\"\n",
        "\n",
        "    parse_params = {\n",
        "        \"action\": \"parse\",\n",
        "        \"page\": article_title,\n",
        "        \"prop\": \"sections|wikitext\",\n",
        "        \"format\": \"json\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        r = requests.get(url, params=parse_params, headers=HEADERS, timeout=30)\n",
        "        data = r.json()\n",
        "\n",
        "        if \"parse\" not in data:\n",
        "            return []\n",
        "\n",
        "        wikitext = data[\"parse\"].get(\"wikitext\", {}).get(\"*\", \"\")\n",
        "\n",
        "        section_pattern = r\"==\\s*See also\\s*==(.*?)(?:==|$)\"\n",
        "        section_match = re.search(section_pattern, wikitext, re.IGNORECASE | re.DOTALL)\n",
        "\n",
        "        if not section_match:\n",
        "            return []\n",
        "\n",
        "        see_also_text = section_match.group(1)\n",
        "        links = re.findall(r'\\[\\[([^]|]+)(?:\\|[^]]+)?\\]\\]', see_also_text)\n",
        "\n",
        "        clean_links = []\n",
        "        for link in links:\n",
        "            if not link.startswith(('Category:', 'File:', 'Image:', 'Wikipedia:')):\n",
        "                clean_links.append(link)\n",
        "\n",
        "        return clean_links[:max_links]\n",
        "\n",
        "    except Exception as e:\n",
        "        return []\n",
        "\n",
        "def get_filtered_links_from_article(article_title, max_links=12):\n",
        "    \"\"\"\n",
        "    Get conceptual links from the article.\n",
        "    \"\"\"\n",
        "    url = \"https://en.wikipedia.org/w/api.php\"\n",
        "\n",
        "    links_params = {\n",
        "        \"action\": \"query\",\n",
        "        \"titles\": article_title,\n",
        "        \"prop\": \"links\",\n",
        "        \"pllimit\": 300,\n",
        "        \"plnamespace\": 0,\n",
        "        \"format\": \"json\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        r = requests.get(url, params=links_params, headers=HEADERS, timeout=30)\n",
        "        data = r.json()\n",
        "        page = next(iter(data.get(\"query\", {}).get(\"pages\", {}).values()))\n",
        "\n",
        "        if \"links\" not in page:\n",
        "            return []\n",
        "\n",
        "        all_links = [link[\"title\"] for link in page[\"links\"]]\n",
        "\n",
        "        conceptual_keywords = [\n",
        "            'theory', 'philosophy', 'studies', 'criticism', 'ism',\n",
        "            'epistemology', 'methodology', 'approach', 'framework',\n",
        "            'perspective', 'analysis', 'research', 'science'\n",
        "        ]\n",
        "\n",
        "        exclude_patterns = [\n",
        "            r'^\\d{4}$',\n",
        "            r'List of',\n",
        "            r'Index of',\n",
        "            r'^[A-Z]{2,4}$',\n",
        "            r'University',\n",
        "            r'Press$',\n",
        "            r'Publishing',\n",
        "            r'Books$',\n",
        "            r'ISBN'\n",
        "        ]\n",
        "\n",
        "        filtered = []\n",
        "        for link in all_links:\n",
        "            if any(re.search(pattern, link) for pattern in exclude_patterns):\n",
        "                continue\n",
        "\n",
        "            if any(keyword in link.lower() for keyword in conceptual_keywords):\n",
        "                filtered.append(link)\n",
        "\n",
        "        if len(filtered) > max_links:\n",
        "            return random.sample(filtered, max_links)\n",
        "        return filtered\n",
        "\n",
        "    except Exception as e:\n",
        "        return []\n",
        "\n",
        "def get_category_siblings(article_title, max_results=10):\n",
        "    \"\"\"\n",
        "    Get other articles in the same meaningful categories.\n",
        "    \"\"\"\n",
        "    url = \"https://en.wikipedia.org/w/api.php\"\n",
        "\n",
        "    cat_params = {\n",
        "        \"action\": \"query\",\n",
        "        \"titles\": article_title,\n",
        "        \"prop\": \"categories\",\n",
        "        \"cllimit\": 50,\n",
        "        \"format\": \"json\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        r = requests.get(url, params=cat_params, headers=HEADERS, timeout=30)\n",
        "        cat_data = r.json()\n",
        "        page = next(iter(cat_data.get(\"query\", {}).get(\"pages\", {}).values()))\n",
        "        categories = [cat[\"title\"] for cat in page.get(\"categories\", [])]\n",
        "\n",
        "        skip_terms = ['stub', 'articles', 'pages', 'wikipedia', 'template',\n",
        "                     'all ', 'cs1', 'webarchive', 'coordinates', 'commons',\n",
        "                     'use dmy', 'use mdy', 'living', 'year', 'century']\n",
        "\n",
        "        meaningful_cats = []\n",
        "        for cat in categories:\n",
        "            if not any(skip in cat.lower() for skip in skip_terms):\n",
        "                meaningful_cats.append(cat)\n",
        "\n",
        "        all_siblings = []\n",
        "\n",
        "        for category in meaningful_cats[:3]:\n",
        "            member_params = {\n",
        "                \"action\": \"query\",\n",
        "                \"list\": \"categorymembers\",\n",
        "                \"cmtitle\": category,\n",
        "                \"cmlimit\": 20,\n",
        "                \"cmnamespace\": 0,\n",
        "                \"format\": \"json\"\n",
        "            }\n",
        "\n",
        "            r = requests.get(url, params=member_params, headers=HEADERS, timeout=30)\n",
        "            member_data = r.json()\n",
        "\n",
        "            for member in member_data.get(\"query\", {}).get(\"categorymembers\", []):\n",
        "                title = member[\"title\"]\n",
        "                if title != article_title and title not in all_siblings:\n",
        "                    all_siblings.append(title)\n",
        "\n",
        "            time.sleep(0.1)\n",
        "\n",
        "        if len(all_siblings) > max_results:\n",
        "            return random.sample(all_siblings, max_results)\n",
        "        return all_siblings\n",
        "\n",
        "    except Exception as e:\n",
        "        return []\n",
        "\n",
        "def explore_topic_multiple_paths(query):\n",
        "    \"\"\"\n",
        "    Get related topics from multiple sources and pathways.\n",
        "    \"\"\"\n",
        "    wiki_results = search_wikipedia_multiple(query, top_n=3)\n",
        "    conceptnet_related = get_conceptnet_related(query, limit=12)\n",
        "\n",
        "    pathways = []\n",
        "\n",
        "    for i, result in enumerate(wiki_results):\n",
        "        article_title = result[\"title\"]\n",
        "\n",
        "        pathway = {\n",
        "            \"title\": article_title,\n",
        "            \"see_also\": get_see_also_links(article_title, 8),\n",
        "            \"related_concepts\": get_filtered_links_from_article(article_title, 10),\n",
        "            \"category_siblings\": get_category_siblings(article_title, 8)\n",
        "        }\n",
        "\n",
        "        pathways.append(pathway)\n",
        "        time.sleep(0.2)\n",
        "\n",
        "    return {\n",
        "        \"wikipedia_matches\": [r[\"title\"] for r in wiki_results],\n",
        "        \"pathways\": pathways,\n",
        "        \"conceptnet_alternatives\": conceptnet_related\n",
        "    }\n",
        "\n",
        "def create_smart_search_explorer():\n",
        "\n",
        "    topic_input = widgets.Text(\n",
        "        value='feminist theory',\n",
        "        placeholder='Enter a topic',\n",
        "        description='Topic:',\n",
        "        style={'description_width': '100px'},\n",
        "        layout=widgets.Layout(width='500px')\n",
        "    )\n",
        "\n",
        "    search_button = widgets.Button(\n",
        "        description='Explore Topic',\n",
        "        button_style='info',\n",
        "        layout=widgets.Layout(width='200px', height='40px')\n",
        "    )\n",
        "\n",
        "    output = widgets.Output()\n",
        "\n",
        "    def on_search_clicked(b):\n",
        "        with output:\n",
        "            clear_output()\n",
        "            query = topic_input.value.strip()\n",
        "\n",
        "            if not query:\n",
        "                print(\"Please enter a topic.\")\n",
        "                return\n",
        "\n",
        "            print(\"TOPIC EXPLORATION\")\n",
        "            print(\"=\"*70)\n",
        "            print(f\"Exploring: '{query}'\")\n",
        "            print(\"=\"*70)\n",
        "            print(\"\\nAnalyzing multiple pathways (30-40 seconds)...\\n\")\n",
        "\n",
        "            results = explore_topic_multiple_paths(query)\n",
        "\n",
        "            # Build HTML output with clickable links\n",
        "            html_output = \"<div style='font-family: monospace;'>\"\n",
        "\n",
        "            # Wikipedia matches\n",
        "            html_output += \"<h3>WIKIPEDIA ARTICLES FOUND</h3>\"\n",
        "            html_output += \"<hr>\"\n",
        "            if results[\"wikipedia_matches\"]:\n",
        "                html_output += \"<ol>\"\n",
        "                for match in results[\"wikipedia_matches\"]:\n",
        "                    html_output += f\"<li>{make_wikipedia_link(match)}</li>\"\n",
        "                html_output += \"</ol>\"\n",
        "            else:\n",
        "                html_output += \"<p>No Wikipedia articles found.</p>\"\n",
        "\n",
        "            # Pathways\n",
        "            for i, pathway in enumerate(results[\"pathways\"], 1):\n",
        "                html_output += f\"<h3>PATHWAY {i}: Based on '{pathway['title']}'</h3>\"\n",
        "                html_output += \"<hr>\"\n",
        "\n",
        "                if pathway[\"see_also\"]:\n",
        "                    html_output += \"<h4>Editor-curated related topics:</h4>\"\n",
        "                    html_output += \"<ol>\"\n",
        "                    for topic in pathway[\"see_also\"]:\n",
        "                        html_output += f\"<li>{make_wikipedia_link(topic)}</li>\"\n",
        "                    html_output += \"</ol>\"\n",
        "\n",
        "                if pathway[\"related_concepts\"]:\n",
        "                    html_output += \"<h4>Related concepts from article:</h4>\"\n",
        "                    html_output += \"<ol>\"\n",
        "                    for topic in pathway[\"related_concepts\"]:\n",
        "                        html_output += f\"<li>{make_wikipedia_link(topic)}</li>\"\n",
        "                    html_output += \"</ol>\"\n",
        "\n",
        "                if pathway[\"category_siblings\"]:\n",
        "                    html_output += \"<h4>Similar topics (same categories):</h4>\"\n",
        "                    html_output += \"<ol>\"\n",
        "                    for topic in pathway[\"category_siblings\"]:\n",
        "                        html_output += f\"<li>{make_wikipedia_link(topic)}</li>\"\n",
        "                    html_output += \"</ol>\"\n",
        "\n",
        "            # ConceptNet alternatives\n",
        "            if results[\"conceptnet_alternatives\"]:\n",
        "                html_output += \"<h3>ALTERNATIVE EXPLORATION ANGLES</h3>\"\n",
        "                html_output += \"<hr>\"\n",
        "                html_output += \"<p>Related concepts from semantic knowledge graph:</p>\"\n",
        "                html_output += \"<ol>\"\n",
        "                for concept in results[\"conceptnet_alternatives\"]:\n",
        "                    html_output += f\"<li>{make_wikipedia_link(concept)}</li>\"\n",
        "                html_output += \"</ol>\"\n",
        "\n",
        "            html_output += \"<hr>\"\n",
        "            html_output += \"<p><strong>Exploration complete. Click any link to open the Wikipedia article.</strong></p>\"\n",
        "            html_output += \"</div>\"\n",
        "\n",
        "            display(HTML(html_output))\n",
        "\n",
        "    search_button.on_click(on_search_clicked)\n",
        "\n",
        "    ui = widgets.VBox([\n",
        "        widgets.HTML(\"<h2>Multi-Path Topic Explorer</h2>\"),\n",
        "        widgets.HTML(\"<p>Discovers related topics from Wikipedia and semantic knowledge graphs.</p>\"),\n",
        "        topic_input,\n",
        "        search_button,\n",
        "        output\n",
        "    ])\n",
        "\n",
        "    display(ui)\n",
        "\n",
        "create_smart_search_explorer()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "OcVETTEw606Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Search for topics that intrest you:"
      ],
      "metadata": {
        "id": "CU8MR-qFptNi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta, timezone\n",
        "from urllib.parse import quote\n",
        "import time\n",
        "import re\n",
        "from IPython.display import display\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from textblob import TextBlob\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "import textstat\n",
        "\n",
        "EMAIL = \"amjose05@gmail.com\" #@param {type:\"string\"}\n",
        "\n",
        "HEADERS = {\n",
        "    \"User-Agent\": f\"Wikiproject_article_finder/1.0 (Educational research; {EMAIL}; Python/requests)\"\n",
        "}\n",
        "\n",
        "SEARCH_QUERY = \"feminist theory\" #@param {type:\"string\"}\n",
        "MAX_ARTICLES = \"10\" #@param {type:\"string\"}\n",
        "\n",
        "try:\n",
        "  MAX_ARTICLES = int(MAX_ARTICLES)\n",
        "except (ValueError, TypeError):\n",
        "  MAX_ARTICLES = None\n",
        "\n",
        "LANG = \"en\"\n",
        "REQUEST_DELAY = 0.03\n",
        "CHECKPOINT_INTERVAL = 100\n",
        "MAX_WORKERS = 3\n",
        "\n",
        "def search_wikipedia_articles(query, max_articles=None):\n",
        "    \"\"\"Search Wikipedia for articles containing specific keywords.\"\"\"\n",
        "    url = \"https://en.wikipedia.org/w/api.php\"\n",
        "    print(f\"Searching for articles related to: '{query}'\")\n",
        "\n",
        "    titles = []\n",
        "    offset = 0\n",
        "\n",
        "    while True:\n",
        "        params = {\n",
        "            \"action\": \"query\",\n",
        "            \"list\": \"search\",\n",
        "            \"srsearch\": query,\n",
        "            \"srlimit\": 50,\n",
        "            \"sroffset\": offset,\n",
        "            \"format\": \"json\"\n",
        "        }\n",
        "\n",
        "        r = requests.get(url, params=params, headers=HEADERS, timeout=30)\n",
        "\n",
        "        if not r.headers.get(\"Content-Type\", \"\").startswith(\"application/json\"):\n",
        "            print(\"Non-JSON response, retrying...\")\n",
        "            time.sleep(2)\n",
        "            continue\n",
        "\n",
        "        data = r.json()\n",
        "        results = data.get(\"query\", {}).get(\"search\", [])\n",
        "\n",
        "        if not results:\n",
        "            break\n",
        "\n",
        "        print(f\"  Found {len(results)} results at offset {offset}\")\n",
        "\n",
        "        for result in results:\n",
        "            title = result[\"title\"]\n",
        "            titles.append(title)\n",
        "\n",
        "            if max_articles and len(titles) >= max_articles:\n",
        "                print(f\"✓ Reached limit of {max_articles} articles\")\n",
        "                return titles[:max_articles]\n",
        "\n",
        "        if \"continue\" in data:\n",
        "            offset = data[\"continue\"][\"sroffset\"]\n",
        "            time.sleep(REQUEST_DELAY)\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    print(f\"✓ Total articles found: {len(titles)}\")\n",
        "    return titles\n",
        "\n",
        "def get_all_article_data(title):\n",
        "    \"\"\"Fetch ALL data for an article in one go to minimize API calls.\"\"\"\n",
        "    url = \"https://en.wikipedia.org/w/api.php\"\n",
        "\n",
        "    params = {\n",
        "        \"action\": \"query\",\n",
        "        \"titles\": title,\n",
        "        \"redirects\": True,\n",
        "        \"prop\": \"revisions|extracts|images|categories\",\n",
        "        \"rvprop\": \"content|timestamp\",\n",
        "        \"rvslots\": \"main\",\n",
        "        \"explaintext\": True,\n",
        "        \"exlimit\": 1,\n",
        "        \"imlimit\": 500,\n",
        "        \"cllimit\": 500,\n",
        "        \"format\": \"json\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        r = requests.get(url, params=params, headers=HEADERS, timeout=30)\n",
        "        if not r.headers.get(\"Content-Type\", \"\").startswith(\"application/json\"):\n",
        "            return None\n",
        "\n",
        "        data = r.json()\n",
        "        page = next(iter(data[\"query\"][\"pages\"].values()))\n",
        "\n",
        "        wikitext = page.get(\"revisions\", [{}])[0].get(\"slots\", {}).get(\"main\", {}).get(\"*\", \"\")\n",
        "        plaintext = page.get(\"extract\", \"\")\n",
        "        timestamp = page.get(\"revisions\", [{}])[0].get(\"timestamp\", \"\")\n",
        "        images = len(page.get(\"images\", []))\n",
        "        categories = len(page.get(\"categories\", []))\n",
        "\n",
        "        return {\n",
        "            \"wikitext\": wikitext,\n",
        "            \"plaintext\": plaintext,\n",
        "            \"timestamp\": timestamp,\n",
        "            \"images\": images,\n",
        "            \"categories\": categories\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching data for {title}: {e}\")\n",
        "        return None\n",
        "\n",
        "def analyze_from_cached_data(title, cached_data):\n",
        "    \"\"\"Run all analyses using cached data\"\"\"\n",
        "    if not cached_data:\n",
        "        return get_empty_metrics()\n",
        "\n",
        "    wikitext = cached_data[\"wikitext\"]\n",
        "    plaintext = cached_data[\"plaintext\"]\n",
        "    timestamp = cached_data[\"timestamp\"]\n",
        "\n",
        "    metrics = {}\n",
        "\n",
        "    # Basic metadata\n",
        "    days_since_edit = 0\n",
        "    if timestamp:\n",
        "        last_edit = datetime.strptime(timestamp, \"%Y-%m-%dT%H:%M:%SZ\")\n",
        "        days_since_edit = round((datetime.now(timezone.utc).replace(tzinfo=None) - last_edit).days)\n",
        "\n",
        "    word_count = len(plaintext.split()) if plaintext else 0\n",
        "    citation_needed_count = wikitext.lower().count(\"citation needed\")\n",
        "\n",
        "    metrics[\"Days Since Last Edit\"] = days_since_edit\n",
        "    metrics[\"Word Count\"] = word_count\n",
        "    metrics[\"Citations Needed\"] = citation_needed_count\n",
        "    metrics[\"Images\"] = cached_data[\"images\"]\n",
        "    metrics[\"Categories\"] = cached_data[\"categories\"]\n",
        "\n",
        "    # Source Quality Analysis\n",
        "    source_metrics = analyze_source_quality_from_text(wikitext)\n",
        "    metrics.update(source_metrics)\n",
        "\n",
        "    # Neutrality Detection\n",
        "    neutrality_metrics = analyze_neutrality_from_text(plaintext)\n",
        "    metrics.update(neutrality_metrics)\n",
        "\n",
        "    # Reading Level\n",
        "    readability_metrics = analyze_readability_from_text(plaintext)\n",
        "    metrics.update(readability_metrics)\n",
        "\n",
        "    # Sentiment\n",
        "    sentiment_metrics = detect_sentiment_bias_from_text(plaintext)\n",
        "    metrics.update(sentiment_metrics)\n",
        "\n",
        "    # Citations\n",
        "    metrics[\"Citations\"] = get_citation_count_from_text(wikitext)\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def analyze_source_quality_from_text(wikitext):\n",
        "    \"\"\"Analyze source quality from wikitext - works with all citation formats\"\"\"\n",
        "\n",
        "    # Count different citation template types\n",
        "    cite_patterns = {\n",
        "        \"journal\": r'{{cite journal',\n",
        "        \"book\": r'{{cite book',\n",
        "        \"web\": r'{{cite web',\n",
        "        \"news\": r'{{cite news',\n",
        "    }\n",
        "\n",
        "    source_types = {k: len(re.findall(v, wikitext, re.IGNORECASE)) for k, v in cite_patterns.items()}\n",
        "    total_typed_sources = sum(source_types.values())\n",
        "\n",
        "    # Extract years from ALL citation formats\n",
        "    years = []\n",
        "    current_year = datetime.now().year\n",
        "\n",
        "    # Method 1: Years from {{cite}} templates\n",
        "    citation_blocks = re.findall(r'{{cite[^}]+}}', wikitext, re.IGNORECASE | re.DOTALL)\n",
        "    for block in citation_blocks:\n",
        "        year_matches = re.findall(r'\\|(?:year|date|publication-date|access-date)\\s*=\\s*[^\\d]*(\\d{4})', block, re.IGNORECASE)\n",
        "        for year_str in year_matches:\n",
        "            year = int(year_str)\n",
        "            if 1800 <= year <= current_year:\n",
        "                years.append(year)\n",
        "                break  # Only take first year per citation\n",
        "\n",
        "    # Method 2: Years from <ref> tags\n",
        "    ref_blocks = re.findall(r'<ref[^>]*>(.*?)</ref>', wikitext, re.IGNORECASE | re.DOTALL)\n",
        "    for ref in ref_blocks:\n",
        "        # Look for years in common formats: (2020), 2020., \"2020\"\n",
        "        year_patterns = [\n",
        "            r'\\((\\d{4})\\)',           # (2020)\n",
        "            r'[,\\s](\\d{4})[,\\.\\s]',   # , 2020, or . 2020.\n",
        "            r'\"(\\d{4})\"',              # \"2020\"\n",
        "            r'(\\d{4})-\\d{2}-\\d{2}',   # 2020-01-15 (date format)\n",
        "        ]\n",
        "\n",
        "        for pattern in year_patterns:\n",
        "            year_matches = re.findall(pattern, ref)\n",
        "            if year_matches:\n",
        "                year = int(year_matches[0])\n",
        "                if 1800 <= year <= current_year:\n",
        "                    years.append(year)\n",
        "                    break  # Only take first year per ref\n",
        "\n",
        "    # Method 3: Years from {{sfn}}, {{harvnb}}, {{harv}} templates\n",
        "    short_footnotes = re.findall(r'{{(?:sfn|harvnb|harv)[^}]*\\|[^}]*?(\\d{4})', wikitext, re.IGNORECASE)\n",
        "    for year_str in short_footnotes:\n",
        "        year = int(year_str)\n",
        "        if 1800 <= year <= current_year:\n",
        "            years.append(year)\n",
        "\n",
        "    # Method 4: Years from {{citation}} templates (alternative to {{cite}})\n",
        "    citation_templates = re.findall(r'{{citation[^}]+}}', wikitext, re.IGNORECASE | re.DOTALL)\n",
        "    for block in citation_templates:\n",
        "        year_matches = re.findall(r'\\|(?:year|date|publication-date)\\s*=\\s*[^\\d]*(\\d{4})', block, re.IGNORECASE)\n",
        "        for year_str in year_matches:\n",
        "            year = int(year_str)\n",
        "            if 1800 <= year <= current_year:\n",
        "                years.append(year)\n",
        "                break\n",
        "\n",
        "    # Remove duplicates while preserving order (in case same source cited multiple times)\n",
        "    # We keep duplicates because multiple citations of same year is valid\n",
        "    # But we can deduplicate if needed - for now keep all\n",
        "\n",
        "    # Calculate metrics\n",
        "    recent_sources = len([y for y in years if current_year - y <= 5]) if years else 0\n",
        "    avg_source_age = (current_year - sum(years) / len(years)) if years else 0\n",
        "\n",
        "    quality_score = 0\n",
        "\n",
        "    if total_typed_sources > 0:\n",
        "        scholarly_ratio = (source_types[\"journal\"] + source_types[\"book\"]) / total_typed_sources\n",
        "        quality_score += scholarly_ratio * 50\n",
        "\n",
        "    source_diversity = len([v for v in source_types.values() if v > 0])\n",
        "    quality_score += source_diversity * 10\n",
        "\n",
        "    if avg_source_age < 10:\n",
        "        quality_score += 25\n",
        "    elif avg_source_age < 20:\n",
        "        quality_score += 15\n",
        "\n",
        "    return {\n",
        "        \"Journal Sources\": source_types[\"journal\"],\n",
        "        \"Book Sources\": source_types[\"book\"],\n",
        "        \"Web Sources\": source_types[\"web\"],\n",
        "        \"News Sources\": source_types[\"news\"],\n",
        "        \"Avg Source Age\": round(avg_source_age, 1),\n",
        "        \"Recent Sources (5yr)\": recent_sources,\n",
        "        \"Source Quality Score\": round(min(quality_score, 100), 1)\n",
        "    }\n",
        "\n",
        "def analyze_neutrality_from_text(text):\n",
        "    \"\"\"Detect POV/bias issues\"\"\"\n",
        "    if not text:\n",
        "        return {\n",
        "            \"Hedging Words\": 0,\n",
        "            \"Peacock Words\": 0,\n",
        "            \"Weasel Words\": 0,\n",
        "            \"Value Judgments\": 0,\n",
        "            \"Neutrality Score\": 100\n",
        "        }\n",
        "\n",
        "    text_lower = text.lower()\n",
        "\n",
        "    peacock_terms = [\n",
        "        \"legendary\", \"iconic\", \"acclaimed\", \"prestigious\", \"renowned\",\n",
        "        \"celebrated\", \"world-class\", \"premier\", \"leading\", \"foremost\",\n",
        "        \"groundbreaking\", \"seminal\", \"pivotal\", \"revolutionary\", \"extraordinary\",\n",
        "        \"remarkable\", \"outstanding\", \"exceptional\", \"unparalleled\", \"best\"\n",
        "    ]\n",
        "    peacock_count = sum(text_lower.count(term) for term in peacock_terms)\n",
        "\n",
        "    weasel_patterns = [\n",
        "        r'\\bsome (people|experts|scholars|critics|observers|say|believe)',\n",
        "        r'\\bmany (believe|argue|claim|suggest|think|feel)',\n",
        "        r'\\bit (is said|has been said|is believed|is widely|is commonly)',\n",
        "        r'\\bmost (people|experts|scholars)',\n",
        "        r'\\bwidely (regarded|considered|accepted|believed)',\n",
        "        r'\\boften (considered|regarded|viewed)',\n",
        "        r'\\bgenerally (accepted|believed|considered)',\n",
        "    ]\n",
        "    weasel_count = sum(len(re.findall(pattern, text_lower)) for pattern in weasel_patterns)\n",
        "\n",
        "    hedging_words = [\"perhaps\", \"possibly\", \"maybe\", \"might\", \"could\", \"may\", \"seemingly\"]\n",
        "    hedging_count = sum(text_lower.count(word) for word in hedging_words)\n",
        "\n",
        "    value_words = [\n",
        "        \"unfortunately\", \"fortunately\", \"clearly\", \"obviously\", \"naturally\",\n",
        "        \"of course\", \"undoubtedly\", \"certainly\", \"arguably\", \"notably\",\n",
        "        \"importantly\", \"surprisingly\", \"interestingly\", \"regrettably\"\n",
        "    ]\n",
        "    value_count = sum(text_lower.count(word) for word in value_words)\n",
        "\n",
        "    word_count = len(text.split())\n",
        "    neutrality_score = 100\n",
        "\n",
        "    if word_count > 0:\n",
        "        neutrality_score -= (peacock_count / word_count * 1000) * 10\n",
        "        neutrality_score -= (weasel_count / word_count * 1000) * 15\n",
        "        neutrality_score -= (value_count / word_count * 1000) * 8\n",
        "\n",
        "    neutrality_score = max(0, min(100, neutrality_score))\n",
        "\n",
        "    return {\n",
        "        \"Hedging Words\": hedging_count,\n",
        "        \"Peacock Words\": peacock_count,\n",
        "        \"Weasel Words\": weasel_count,\n",
        "        \"Value Judgments\": value_count,\n",
        "        \"Neutrality Score\": round(neutrality_score, 1)\n",
        "    }\n",
        "\n",
        "def analyze_readability_from_text(text):\n",
        "    \"\"\"Calculate reading level\"\"\"\n",
        "    if not text or len(text) < 100:\n",
        "        return {\n",
        "            \"Flesch-Kincaid Grade\": 0,\n",
        "            \"Reading Level\": \"Unknown\"\n",
        "        }\n",
        "\n",
        "    try:\n",
        "        flesch_grade = textstat.flesch_kincaid_grade(text)\n",
        "        flesch_ease = textstat.flesch_reading_ease(text)\n",
        "\n",
        "        if flesch_ease >= 90:\n",
        "            level = \"Elementary (5th grade)\"\n",
        "        elif flesch_ease >= 80:\n",
        "            level = \"Middle School (6-7th)\"\n",
        "        elif flesch_ease >= 70:\n",
        "            level = \"High School (8-9th)\"\n",
        "        elif flesch_ease >= 60:\n",
        "            level = \"High School (10-12th)\"\n",
        "        elif flesch_ease >= 50:\n",
        "            level = \"College\"\n",
        "        elif flesch_ease >= 30:\n",
        "            level = \"College Graduate\"\n",
        "        else:\n",
        "            level = \"Professional/Academic\"\n",
        "\n",
        "        return {\n",
        "            \"Flesch-Kincaid Grade\": round(flesch_grade, 1),\n",
        "            \"Reading Level\": level\n",
        "        }\n",
        "    except:\n",
        "        return {\n",
        "            \"Flesch-Kincaid Grade\": 0,\n",
        "            \"Reading Level\": \"Error\"\n",
        "        }\n",
        "\n",
        "def detect_sentiment_bias_from_text(text):\n",
        "    \"\"\"Detect sentiment bias\"\"\"\n",
        "    if not text:\n",
        "        return {\n",
        "            \"Polarity\": 0,\n",
        "            \"Subjectivity\": 0,\n",
        "            \"VADER Compound\": 0,\n",
        "            \"Sentiment\": \"Neutral\"\n",
        "        }\n",
        "\n",
        "    try:\n",
        "        blob = TextBlob(text)\n",
        "        polarity = blob.sentiment.polarity\n",
        "        subjectivity = blob.sentiment.subjectivity\n",
        "\n",
        "        analyzer = SentimentIntensityAnalyzer()\n",
        "        vader_scores = analyzer.polarity_scores(text)\n",
        "\n",
        "        if abs(polarity) < 0.1 and subjectivity < 0.3:\n",
        "            sentiment = \"Neutral & Objective\"\n",
        "        elif abs(polarity) < 0.1:\n",
        "            sentiment = \"Neutral but Subjective\"\n",
        "        elif polarity > 0.2:\n",
        "            sentiment = \"Positive Bias Detected\"\n",
        "        elif polarity < -0.2:\n",
        "            sentiment = \"Negative Bias Detected\"\n",
        "        else:\n",
        "            sentiment = \"Slight Bias\"\n",
        "\n",
        "        return {\n",
        "            \"Polarity\": round(polarity, 3),\n",
        "            \"Subjectivity\": round(subjectivity, 3),\n",
        "            \"VADER Compound\": round(vader_scores['compound'], 3),\n",
        "            \"Sentiment\": sentiment\n",
        "        }\n",
        "    except:\n",
        "        return {\n",
        "            \"Polarity\": 0,\n",
        "            \"Subjectivity\": 0,\n",
        "            \"VADER Compound\": 0,\n",
        "            \"Sentiment\": \"Error\"\n",
        "        }\n",
        "\n",
        "def get_citation_count_from_text(wikitext):\n",
        "    \"\"\"Count citations from wikitext\"\"\"\n",
        "    if not wikitext:\n",
        "        return 0\n",
        "\n",
        "    named_refs = set()\n",
        "    unnamed_count = 0\n",
        "    ref_pattern = r'<ref(?:\\s+[^>]*)?>'\n",
        "    all_refs = re.findall(ref_pattern, wikitext, re.IGNORECASE | re.DOTALL)\n",
        "\n",
        "    for ref in all_refs:\n",
        "        if ref.strip().endswith('/>'):\n",
        "            continue\n",
        "        name_match = re.search(r'name\\s*=\\s*[\"\\']([^\"\\']+)[\"\\']', ref, re.IGNORECASE)\n",
        "        if name_match:\n",
        "            named_refs.add(name_match.group(1))\n",
        "        else:\n",
        "            unnamed_count += 1\n",
        "\n",
        "    ref_count = len(named_refs) + unnamed_count\n",
        "\n",
        "    sfn_count = len(re.findall(r'\\{\\{sfn[a-z]*\\|', wikitext, re.IGNORECASE))\n",
        "    harv_count = len(re.findall(r'\\{\\{harv[a-z]*\\|', wikitext, re.IGNORECASE))\n",
        "    r_count = len(re.findall(r'\\{\\{rp?\\|', wikitext, re.IGNORECASE))\n",
        "    efn_count = len(re.findall(r'\\{\\{efn[a-z]*\\|', wikitext, re.IGNORECASE))\n",
        "\n",
        "    footnote_count = sfn_count + efn_count\n",
        "    return max(ref_count, footnote_count, harv_count, r_count)\n",
        "\n",
        "def get_remaining_data(title):\n",
        "    \"\"\"Get data that requires separate API calls\"\"\"\n",
        "    url = \"https://en.wikipedia.org/w/api.php\"\n",
        "\n",
        "    sections = \"\"\n",
        "    try:\n",
        "        params = {\n",
        "            \"action\": \"parse\",\n",
        "            \"page\": title,\n",
        "            \"prop\": \"sections\",\n",
        "            \"redirects\": 1,\n",
        "            \"format\": \"json\"\n",
        "        }\n",
        "        r = requests.get(url, params=params, headers=HEADERS, timeout=30)\n",
        "        if r.headers.get(\"Content-Type\", \"\").startswith(\"application/json\"):\n",
        "            parse_data = r.json().get(\"parse\", {})\n",
        "            if parse_data:\n",
        "                section_list = parse_data.get(\"sections\", [])\n",
        "                sections = \", \".join(s[\"line\"] for s in section_list)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    talk_page_size = 0\n",
        "    try:\n",
        "        params = {\n",
        "            \"action\": \"query\",\n",
        "            \"titles\": f\"Talk:{title}\",\n",
        "            \"redirects\": True,\n",
        "            \"prop\": \"revisions\",\n",
        "            \"rvprop\": \"size\",\n",
        "            \"rvlimit\": 1,\n",
        "            \"format\": \"json\"\n",
        "        }\n",
        "        r = requests.get(url, params=params, headers=HEADERS, timeout=30)\n",
        "        if r.headers.get(\"Content-Type\", \"\").startswith(\"application/json\"):\n",
        "            data = r.json()\n",
        "            pages = data.get(\"query\", {}).get(\"pages\", {})\n",
        "            page = next(iter(pages.values()), {})\n",
        "            if int(page.get(\"pageid\", -1)) > 0:\n",
        "                talk_page_size = page.get(\"revisions\", [{}])[0].get(\"size\", 0)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    num_editors, recent_edits = 0, 0\n",
        "    try:\n",
        "        params = {\n",
        "            \"action\": \"query\",\n",
        "            \"titles\": title,\n",
        "            \"redirects\": True,\n",
        "            \"prop\": \"revisions\",\n",
        "            \"rvprop\": \"timestamp|user\",\n",
        "            \"rvlimit\": 500,\n",
        "            \"format\": \"json\"\n",
        "        }\n",
        "        r = requests.get(url, params=params, headers=HEADERS, timeout=30)\n",
        "        if r.headers.get(\"Content-Type\", \"\").startswith(\"application/json\"):\n",
        "            data = r.json()\n",
        "            pages = data.get(\"query\", {}).get(\"pages\", {})\n",
        "            page = next(iter(pages.values()), {})\n",
        "            revisions = page.get(\"revisions\", [])\n",
        "\n",
        "            unique_editors = set()\n",
        "            one_year_ago = datetime.now(timezone.utc) - timedelta(days=365)\n",
        "\n",
        "            for rev in revisions:\n",
        "                user = rev.get(\"user\", \"\")\n",
        "                if user:\n",
        "                    unique_editors.add(user)\n",
        "                timestamp_str = rev.get(\"timestamp\", \"\")\n",
        "                if timestamp_str:\n",
        "                    timestamp = datetime.strptime(timestamp_str, \"%Y-%m-%dT%H:%M:%SZ\").replace(tzinfo=timezone.utc)\n",
        "                    if timestamp >= one_year_ago:\n",
        "                        recent_edits += 1\n",
        "\n",
        "            num_editors = len(unique_editors)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    views = 0\n",
        "    try:\n",
        "        end = datetime.now(timezone.utc).replace(tzinfo=None)\n",
        "        start = end - timedelta(days=90)\n",
        "        encoded_title = quote(title.replace(' ', '_'))\n",
        "        pv_url = (\n",
        "            f\"https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/\"\n",
        "            f\"en.wikipedia/all-access/user/\"\n",
        "            f\"{encoded_title}/daily/\"\n",
        "            f\"{start:%Y%m%d}/{end:%Y%m%d}\"\n",
        "        )\n",
        "        r = requests.get(pv_url, headers=HEADERS, timeout=30)\n",
        "        if r.headers.get(\"Content-Type\", \"\").startswith(\"application/json\"):\n",
        "            data = r.json()\n",
        "            views = sum(d[\"views\"] for d in data.get(\"items\", []))\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    return {\n",
        "        \"Section Names\": sections,\n",
        "        \"Talk Page Size\": talk_page_size,\n",
        "        \"Total Editors\": num_editors,\n",
        "        \"Edits Last Year\": recent_edits,\n",
        "        \"Last 3 Months Views\": views\n",
        "    }\n",
        "\n",
        "def get_empty_metrics():\n",
        "    \"\"\"Return empty metrics dict for failed articles\"\"\"\n",
        "    return {\n",
        "        \"Days Since Last Edit\": 0,\n",
        "        \"Word Count\": 0,\n",
        "        \"Section Names\": \"\",\n",
        "        \"Citations\": 0,\n",
        "        \"Citations Needed\": 0,\n",
        "        \"Images\": 0,\n",
        "        \"Categories\": 0,\n",
        "        \"Total Editors\": 0,\n",
        "        \"Edits Last Year\": 0,\n",
        "        \"Talk Page Size\": 0,\n",
        "        \"Last 3 Months Views\": 0,\n",
        "        \"Journal Sources\": 0,\n",
        "        \"Book Sources\": 0,\n",
        "        \"Web Sources\": 0,\n",
        "        \"News Sources\": 0,\n",
        "        \"Avg Source Age\": 0,\n",
        "        \"Recent Sources (5yr)\": 0,\n",
        "        \"Source Quality Score\": 0,\n",
        "        \"Hedging Words\": 0,\n",
        "        \"Peacock Words\": 0,\n",
        "        \"Weasel Words\": 0,\n",
        "        \"Value Judgments\": 0,\n",
        "        \"Neutrality Score\": 0,\n",
        "        \"Flesch-Kincaid Grade\": 0,\n",
        "        \"Reading Level\": \"Unknown\",\n",
        "        \"Polarity\": 0,\n",
        "        \"Subjectivity\": 0,\n",
        "        \"VADER Compound\": 0,\n",
        "        \"Sentiment\": \"Unknown\"\n",
        "    }\n",
        "\n",
        "def process_single_article(title):\n",
        "    \"\"\"Process a single article - to be run in parallel\"\"\"\n",
        "    try:\n",
        "        cached_data = get_all_article_data(title)\n",
        "        metrics = analyze_from_cached_data(title, cached_data)\n",
        "        remaining = get_remaining_data(title)\n",
        "        metrics.update(remaining)\n",
        "        metrics[\"Article\"] = title\n",
        "\n",
        "        time.sleep(REQUEST_DELAY)\n",
        "        return metrics\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {title}: {e}\")\n",
        "        empty = get_empty_metrics()\n",
        "        empty[\"Article\"] = title\n",
        "        return empty\n",
        "\n",
        "# ============================================\n",
        "# MAIN EXECUTION\n",
        "# ============================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(f\"Searching Wikipedia for: '{SEARCH_QUERY}'\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "articles = search_wikipedia_articles(SEARCH_QUERY, MAX_ARTICLES)\n",
        "\n",
        "if not articles:\n",
        "    print(\"\\nNo articles found.\")\n",
        "    exit()\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Processing {len(articles)} articles with {MAX_WORKERS} parallel workers...\")\n",
        "print(f\"{'='*60}\\n\")\n",
        "\n",
        "rows = []\n",
        "start_time = time.time()\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
        "    future_to_article = {executor.submit(process_single_article, title): title for title in articles}\n",
        "\n",
        "    for i, future in enumerate(as_completed(future_to_article), 1):\n",
        "        article = future_to_article[future]\n",
        "        try:\n",
        "            result = future.result()\n",
        "            rows.append(result)\n",
        "            print(f\"[{i}/{len(articles)}] Completed: {article}\")\n",
        "\n",
        "            if i % CHECKPOINT_INTERVAL == 0:\n",
        "                df_checkpoint = pd.DataFrame(rows)\n",
        "                timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "                checkpoint_file = f\"{SEARCH_QUERY.replace(' ', '_')}_{timestamp}_checkpoint.csv\"\n",
        "                df_checkpoint.to_csv(checkpoint_file, index=False)\n",
        "                elapsed = time.time() - start_time\n",
        "                remaining_time = (elapsed / i) * (len(articles) - i)\n",
        "                print(f\"Checkpoint saved: {checkpoint_file}\")\n",
        "                print(f\"Elapsed: {elapsed/60:.1f}min | Estimated remaining: {remaining_time/60:.1f}min\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Failed: {article} - {e}\")\n",
        "\n",
        "# ============================================\n",
        "# REORDER COLUMNS AND DISPLAY WITH QGRID\n",
        "# ============================================\n",
        "\n",
        "if rows:\n",
        "    df_unordered = pd.DataFrame(rows)\n",
        "\n",
        "    # Define column order\n",
        "    column_order = [\n",
        "        # Basic info\n",
        "        \"Article\",\n",
        "        \"Last 3 Months Views\",\n",
        "        \"Word Count\",\n",
        "        \"Talk Page Size\",\n",
        "        \"Days Since Last Edit\",\n",
        "        \"Edits Last Year\",\n",
        "        \"Total Editors\",\n",
        "        \"Images\",\n",
        "        \"Citations\",\n",
        "        \"Citations Needed\",\n",
        "\n",
        "        # Source quality\n",
        "        \"Journal Sources\",\n",
        "        \"Book Sources\",\n",
        "        \"Web Sources\",\n",
        "        \"News Sources\",\n",
        "        \"Avg Source Age\",\n",
        "        \"Recent Sources (5yr)\",\n",
        "        \"Source Quality Score\",\n",
        "\n",
        "        # Neutrality and bias\n",
        "        \"Hedging Words\",\n",
        "        \"Peacock Words\",\n",
        "        \"Weasel Words\",\n",
        "        \"Value Judgments\",\n",
        "        \"Neutrality Score\",\n",
        "\n",
        "        # Readability\n",
        "        \"Flesch-Kincaid Grade\",\n",
        "        \"Reading Level\",\n",
        "\n",
        "        # Sentiment\n",
        "        \"Polarity\",\n",
        "        \"Subjectivity\",\n",
        "        \"VADER Compound\",\n",
        "        \"Sentiment\",\n",
        "\n",
        "        # End columns\n",
        "        \"Categories\",\n",
        "        \"Section Names\"\n",
        "    ]\n",
        "\n",
        "    # Reorder (only include columns that exist)\n",
        "    df = df_unordered[[col for col in column_order if col in df_unordered.columns]]\n",
        "\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    filename = f\"{SEARCH_QUERY.replace(' ', '_')}_{MAX_ARTICLES}.csv\"\n",
        "    df.to_csv(filename, index=False)\n",
        "\n",
        "    # Try to save to Google Drive\n",
        "    try:\n",
        "        drive_filename = f\"/content/drive/MyDrive/{SEARCH_QUERY.replace(' ', '_')}_{MAX_ARTICLES}.csv\"\n",
        "        df.to_csv(drive_filename, index=False)\n",
        "        print(f\"Saved to Google Drive: {drive_filename}\")\n",
        "    except:\n",
        "        print(\"Google Drive not mounted, saved locally only\")\n",
        "\n",
        "    elapsed_total = time.time() - start_time\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Successfully saved {len(df)} articles to: {filename}\")\n",
        "    print(f\"Total time: {elapsed_total/60:.1f} minutes ({elapsed_total/len(df):.2f}s per article)\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n",
        "    # ============================================\n",
        "    # DISPLAY WITH ITABLES (MORE RELIABLE)\n",
        "    # ============================================\n",
        "\n",
        "    from itables import init_notebook_mode, show\n",
        "    init_notebook_mode(all_interactive=True)\n",
        "\n",
        "    print(\"Interactive Data Table\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"How to use:\")\n",
        "    print(\"SEARCH: Use the search box to filter across all columns\")\n",
        "    print(\"SORT: Click column headers to sort (shift+click for multi-column)\")\n",
        "    print(\"COLUMNS: Click 'Column visibility' button to show/hide columns\")\n",
        "    print(\"PAGES: Use dropdown to change rows per page (10/25/50/100)\")\n",
        "    print(\"EXPORT: Click 'CSV' or 'Excel' to download\")\n",
        "    print(\"=\"*60)\n",
        "    print()\n",
        "\n",
        "    # Show interactive table\n",
        "    show(df,\n",
        "        scrollX=True,\n",
        "        scrollY=\"600px\",\n",
        "        paging=True,\n",
        "        lengthMenu=[10, 25, 50, 100],\n",
        "        pageLength=25,\n",
        "        buttons=['copy', 'csv', 'excel', 'colvis'],\n",
        "        order=[[1, 'desc']],  # Sort by \"Last 3 Months Views\" descending by default\n",
        "        columnDefs=[{\"className\": \"dt-left\", \"targets\": \"_all\"}]\n",
        "    )\n",
        "\n",
        "    # Summary statistics\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"Summary Statistics\")\n",
        "    print(f\"{'='*60}\")\n",
        "    numeric_cols = ['Last 3 Months Views', 'Word Count', 'Talk Page Size', 'Citations',\n",
        "                   'Days Since Last Edit', 'Source Quality Score', 'Neutrality Score',\n",
        "                   'Flesch-Kincaid Grade', 'Polarity', 'Subjectivity']\n",
        "    available_cols = [col for col in numeric_cols if col in df.columns]\n",
        "    print(df[available_cols].describe().round(1))\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo data to save\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "EWoXcCZHU-UO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare the articles you have found using easy to digest ratings:"
      ],
      "metadata": {
        "id": "pX6Z8mLWpxcr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2d9Y2nsj8gAS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "pd.set_option('display.width', 1000)\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "# Load the CSV file\n",
        "path = \"/content/feminist_theory_10.csv\" #@param {type:\"string\"}\n",
        "df = pd.read_csv(path)\n",
        "\n",
        "# Function to calculate percentile rank (0-100 scale)\n",
        "def percentile_rank(series):\n",
        "    \"\"\"Convert values to percentile ranks (0-100)\"\"\"\n",
        "    return series.rank(pct=True) * 100\n",
        "\n",
        "# ============================================\n",
        "# CREATE LINKS FIRST (before percentile calculations)\n",
        "# ============================================\n",
        "\n",
        "# Wikipedia link\n",
        "df['Wikipedia Link'] = df['Article'].apply(\n",
        "    lambda x: f\"https://en.wikipedia.org/wiki/{x.replace(' ', '_')}\"\n",
        ")\n",
        "\n",
        "# IIT Library search link\n",
        "df['IIT Library Link'] = df[\"Article\"].str.replace(' ', '%20').apply(\n",
        "    lambda x: f\"https://i-share-iit.primo.exlibrisgroup.com/discovery/search?query=any,contains,{x},AND&tab=Everything&search_scope=MyInst_and_CI&sortby=rank&vid=01CARLI_IIT:CARLI_IIT&mfacet=tlevel,include,peer_reviewed,1&lang=en&mode=advanced&offset=0\"\n",
        ")\n",
        "\n",
        "# ============================================\n",
        "# CALCULATE PERCENTILES FOR ALL METRICS\n",
        "# ============================================\n",
        "\n",
        "# Activity/Collaboration metrics\n",
        "df['Days Since Edit Percentile'] = 100 - percentile_rank(df['Days Since Last Edit'])\n",
        "df['Edits Last Year Percentile'] = percentile_rank(df['Edits Last Year'])\n",
        "df['Total Editors Percentile'] = percentile_rank(df['Total Editors'])\n",
        "df['Talk Page Percentile'] = percentile_rank(df['Talk Page Size'])\n",
        "\n",
        "# Popularity\n",
        "df['Views Percentile'] = percentile_rank(df['Last 3 Months Views'])\n",
        "\n",
        "# Content quality\n",
        "df['Word Count Percentile'] = percentile_rank(df['Word Count'])\n",
        "df['Images Percentile'] = percentile_rank(df['Images'])\n",
        "df['Categories Percentile'] = percentile_rank(df['Categories'])\n",
        "df['Citations Percentile'] = percentile_rank(df['Citations'])\n",
        "df['Citation Needed Percentile'] = 100 - percentile_rank(df['Citations Needed'])\n",
        "df['Citation/Word Ratio Percentile'] = percentile_rank(df['Citations'] / df['Word Count'].replace(0, 1))\n",
        "\n",
        "# Source quality metrics\n",
        "df['Source Quality Percentile'] = percentile_rank(df['Source Quality Score'])\n",
        "df['Journal Sources Percentile'] = percentile_rank(df['Journal Sources'])\n",
        "df['Book Sources Percentile'] = percentile_rank(df['Book Sources'])\n",
        "df['Recent Sources Percentile'] = percentile_rank(df['Recent Sources (5yr)'])\n",
        "df['Source Age Percentile'] = 100 - percentile_rank(df['Avg Source Age'])\n",
        "\n",
        "# Neutrality and bias metrics\n",
        "df['Neutrality Percentile'] = percentile_rank(df['Neutrality Score'])\n",
        "df['Peacock Words Percentile'] = 100 - percentile_rank(df['Peacock Words'])\n",
        "df['Weasel Words Percentile'] = 100 - percentile_rank(df['Weasel Words'])\n",
        "df['Value Judgments Percentile'] = 100 - percentile_rank(df['Value Judgments'])\n",
        "\n",
        "# Readability metrics\n",
        "df['Reading Level Percentile'] = 100 - percentile_rank(df['Flesch-Kincaid Grade'])\n",
        "\n",
        "# Sentiment metrics\n",
        "df['Polarity Neutrality Percentile'] = 100 - percentile_rank(df['Polarity'].abs())\n",
        "df['Objectivity Percentile'] = 100 - percentile_rank(df['Subjectivity'])\n",
        "\n",
        "# ============================================\n",
        "# CREATE COMPOSITE SCORES\n",
        "# ============================================\n",
        "\n",
        "# Collaboration Score\n",
        "df['Collaboration Score'] = (\n",
        "    (df['Total Editors Percentile'] + df['Talk Page Percentile']) / 2\n",
        ").round().astype(int)\n",
        "\n",
        "# Aliveness Score\n",
        "df['Aliveness Score'] = (\n",
        "    (df['Days Since Edit Percentile'] + df['Edits Last Year Percentile']) / 2\n",
        ").round().astype(int)\n",
        "\n",
        "# Popularity Score\n",
        "df['Popularity Score'] = df['Views Percentile'].round().astype(int)\n",
        "\n",
        "# Quality Score\n",
        "df['Quality Score'] = (\n",
        "    df['Citation/Word Ratio Percentile'] * 0.25 +\n",
        "    df['Images Percentile'] * 0.10 +\n",
        "    df['Categories Percentile'] * 0.10 +\n",
        "    df['Citation Needed Percentile'] * 0.05 +\n",
        "    df['Source Quality Percentile'] * 0.25 +\n",
        "    df['Neutrality Percentile'] * 0.15 +\n",
        "    df['Objectivity Percentile'] * 0.10\n",
        ").round().astype(int)\n",
        "\n",
        "# Scholarly Source Score\n",
        "df['Scholarly Source Score'] = (\n",
        "    df['Source Quality Percentile'] * 0.30 +\n",
        "    df['Journal Sources Percentile'] * 0.30 +\n",
        "    df['Book Sources Percentile'] * 0.20 +\n",
        "    df['Recent Sources Percentile'] * 0.10 +\n",
        "    df['Source Age Percentile'] * 0.10\n",
        ").round().astype(int)\n",
        "\n",
        "# NPOV Score\n",
        "df['NPOV Score'] = (\n",
        "    df['Neutrality Percentile'] * 0.40 +\n",
        "    df['Objectivity Percentile'] * 0.30 +\n",
        "    df['Peacock Words Percentile'] * 0.15 +\n",
        "    df['Weasel Words Percentile'] * 0.15\n",
        ").round().astype(int)\n",
        "\n",
        "# Accessibility Score\n",
        "df['Accessibility Score'] = df['Reading Level Percentile'].round().astype(int)\n",
        "\n",
        "# ============================================\n",
        "# DISPLAY RESULTS\n",
        "# ============================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"COMPOSITE SCORES CALCULATED\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nScore Definitions:\")\n",
        "print(\"- Collaboration: Average of (Total Editors + Talk Page Size)\")\n",
        "print(\"- Aliveness: Average of (Days Since Edit [inverted] + Edits Last Year)\")\n",
        "print(\"- Popularity: Page Views\")\n",
        "print(\"- Quality: Weighted average of content metrics, sources, and neutrality\")\n",
        "print(\"- Scholarly Source: Quality and recency of academic sources\")\n",
        "print(\"- NPOV Score: Neutral Point of View compliance\")\n",
        "print(\"- Accessibility: Reading level\")\n",
        "print(\"\\nAll scores are on a 0-100 percentile scale.\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Display core composite scores with links\n",
        "display(df[['Article', 'Collaboration Score',\n",
        "            'Aliveness Score', 'Popularity Score', 'Quality Score', 'Scholarly Source Score',\n",
        "            'NPOV Score', 'Accessibility Score', 'Wikipedia Link', 'IIT Library Link']])\n",
        "\n",
        "# Summary statistics\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Composite Score Statistics:\")\n",
        "print(\"=\"*60)\n",
        "score_cols = ['Collaboration Score', 'Aliveness Score', 'Popularity Score', 'Quality Score',\n",
        "              'Scholarly Source Score', 'NPOV Score', 'Accessibility Score']\n",
        "print(df[score_cols].describe().round(1).T)\n",
        "\n",
        "# ============================================\n",
        "# TOP ARTICLES BY EACH SCORE\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TOP 5 ARTICLES BY EACH SCORE:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\nMost Collaborative:\")\n",
        "print(df.nlargest(5, 'Collaboration Score')[['Article', 'Collaboration Score', 'Total Editors', 'Talk Page Size']])\n",
        "\n",
        "print(\"\\nMost Alive:\")\n",
        "print(df.nlargest(5, 'Aliveness Score')[['Article', 'Aliveness Score', 'Days Since Last Edit', 'Edits Last Year']])\n",
        "\n",
        "print(\"\\nMost Popular:\")\n",
        "print(df.nlargest(5, 'Popularity Score')[['Article', 'Popularity Score', 'Last 3 Months Views']])\n",
        "\n",
        "print(\"\\nHighest Quality:\")\n",
        "print(df.nlargest(5, 'Quality Score')[['Article', 'Quality Score', 'Citations', 'Source Quality Score', 'Neutrality Score']])\n",
        "\n",
        "print(\"\\nBest Scholarly Sources:\")\n",
        "print(df.nlargest(5, 'Scholarly Source Score')[['Article', 'Scholarly Source Score', 'Journal Sources', 'Book Sources']])\n",
        "\n",
        "print(\"\\nMost Neutral (NPOV):\")\n",
        "print(df.nlargest(5, 'NPOV Score')[['Article', 'NPOV Score', 'Neutrality Score', 'Objectivity Percentile']])\n",
        "\n",
        "print(\"\\nMost Accessible:\")\n",
        "print(df.nlargest(5, 'Accessibility Score')[['Article', 'Accessibility Score', 'Reading Level', 'Flesch-Kincaid Grade']])\n",
        "\n",
        "# ============================================\n",
        "# SAVE RESULTS\n",
        "# ============================================\n",
        "\n",
        "# Drop percentile columns (keep only final scores)\n",
        "percentile_cols = [col for col in df.columns if 'Percentile' in col]\n",
        "df_final = df.drop(columns=percentile_cols)\n",
        "\n",
        "# Save to local\n",
        "filename_with_scores = path.split('/')[-1].replace('.csv', '_WITH_SCORES.csv')\n",
        "df_final.to_csv(filename_with_scores, index=False)\n",
        "print(f\"\\nSaved locally to: {filename_with_scores}\")\n",
        "\n",
        "# Save to Google Drive\n",
        "try:\n",
        "    drive_filename = \"/content/drive/MyDrive/\" + filename_with_scores\n",
        "    df_final.to_csv(drive_filename, index=False)\n",
        "    print(f\"Saved to Google Drive: {drive_filename}\")\n",
        "except:\n",
        "    print(\"Google Drive not mounted\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can combine two CSVs by pasting their paths here:"
      ],
      "metadata": {
        "id": "g2O7RaOko4-E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "yd5pD4PkxMYT"
      },
      "outputs": [],
      "source": [
        "#combine dfs\n",
        "path1 = \"/content/wikipedia_psychology_C-Class_FINAL_ajose3@hawk.illinoistech.edu.csv\" #@param {type:\"string\"}\n",
        "path2 = \"/content/wikipedia_psychology_C-Class_FINAL_ajose3@hawk.illinoistech.edu.csv\" #@param {type:\"string\"}\n",
        "# Read CSVs into DataFrame\n",
        "First_Sheet = pd.read_csv(path1)\n",
        "Second_Sheet = pd.read_csv(path2)\n",
        "dfs = [First_Sheet,Second_Sheet]\n",
        "combined_df = pd.concat(dfs, axis=0, ignore_index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "See any given CSVs data:"
      ],
      "metadata": {
        "id": "l3H9Rt_dpLiT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9KZaqPX9jmQI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "path = \"/content/feminist_theory_10.csv\" #@param {type:\"string\"}\n",
        "# Read CSV into DataFrame\n",
        "df = pd.read_csv(path)\n",
        "show(df,\n",
        "        scrollX=True,\n",
        "        scrollY=\"600px\",\n",
        "        paging=True,\n",
        "        lengthMenu=[10, 25, 50, 100],\n",
        "        pageLength=25,\n",
        "        buttons=['copy', 'csv', 'excel', 'colvis'],\n",
        "        order=[[1, 'desc']],  # Sort by \"Last 3 Months Views\" descending by default\n",
        "        columnDefs=[{\"className\": \"dt-left\", \"targets\": \"_all\"}]\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55pz5tiL0nGp"
      },
      "source": [
        "Ignore these:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bi6IBVBwXu7V"
      },
      "source": [
        "Sexology_and_sexuality, psychology, Feminism, Women, etc.\n",
        "List of wiki projects here: https://en.wikipedia.org/wiki/Wikipedia:WikiProject_Directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3iU9TZPB4yhc"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta, timezone\n",
        "from urllib.parse import quote\n",
        "import time\n",
        "import re\n",
        "from IPython.display import display\n",
        "EMAIL = \"amjose05@gmail.com\" #@param {type:\"string\"}\n",
        "import random\n",
        "\n",
        "HEADERS = {\n",
        "    \"User-Agent\": f\"Wikiproject_article_finder/1.0 (Educational research; {EMAIL}; Python/requests)\"\n",
        "}\n",
        "WIKIPROJECT = \"psychology\" #@param {type:\"string\"}\n",
        "CLASS = \"C-Class\" #@param {type:\"string\"}\n",
        "MAX_ARTICLES = \"0\" #@param {type:\"string\"}\n",
        "try:\n",
        "  MAX_ARTICLES = int(MAX_ARTICLES)  # Try conversion\n",
        "except (ValueError, TypeError):\n",
        "  MAX_ARTICLES = None\n",
        "LANG = \"en\"\n",
        "REQUEST_DELAY = 0.03\n",
        "CHECKPOINT_INTERVAL = 100\n",
        "\n",
        "\n",
        "\n",
        "def get_project_articles(project, klass, MAX_ARTICLES=None):\n",
        "    \"\"\"\n",
        "    Fetch article titles for a given WikiProject and class.\n",
        "    \"\"\"\n",
        "    category = f\"Category:{klass}_{project}_articles\"\n",
        "    url = \"https://en.wikipedia.org/w/api.php\"\n",
        "\n",
        "    print(f\"Fetching from: {category}\")\n",
        "\n",
        "    titles = []\n",
        "    cmcontinue = None\n",
        "    page_count = 0\n",
        "\n",
        "    while True:\n",
        "        page_count += 1\n",
        "        params = {\n",
        "            \"action\": \"query\",\n",
        "            \"list\": \"categorymembers\",\n",
        "            \"cmtitle\": category,\n",
        "            \"cmlimit\": 500,\n",
        "            \"format\": \"json\"\n",
        "        }\n",
        "        if cmcontinue:\n",
        "            params[\"cmcontinue\"] = cmcontinue\n",
        "\n",
        "        r = requests.get(url, params=params, headers=HEADERS, timeout=30)\n",
        "\n",
        "        if not r.headers.get(\"Content-Type\", \"\").startswith(\"application/json\"):\n",
        "            print(\"Non-JSON response, retrying...\")\n",
        "            time.sleep(2)\n",
        "            continue\n",
        "\n",
        "        data = r.json()\n",
        "        members = data.get(\"query\", {}).get(\"categorymembers\", [])\n",
        "\n",
        "        print(f\"  Page {page_count}: Found {len(members)} items\")\n",
        "\n",
        "        for p in members:\n",
        "            title = p[\"title\"]\n",
        "            if title.startswith(\"Talk:\"):\n",
        "                article_title = title[5:]\n",
        "                titles.append(article_title)\n",
        "            elif not title.startswith(\"Category:\"):\n",
        "                titles.append(title)\n",
        "\n",
        "            if MAX_ARTICLES and len(titles) >= MAX_ARTICLES:\n",
        "                unique = sorted(set(titles))[:MAX_ARTICLES]\n",
        "                print(f\"Reached limit of {MAX_ARTICLES} articles\")\n",
        "                return unique\n",
        "\n",
        "        cmcontinue = data.get(\"continue\", {}).get(\"cmcontinue\")\n",
        "        if not cmcontinue:\n",
        "            break\n",
        "\n",
        "        time.sleep(REQUEST_DELAY)\n",
        "\n",
        "    unique_titles = sorted(set(titles))\n",
        "    print(f\"Total unique articles found: {len(unique_titles)}\")\n",
        "\n",
        "    return unique_titles\n",
        "\n",
        "def get_article_metadata(title):\n",
        "    \"\"\"\n",
        "    Get basic metadata: days since edit, word count, citation needed count\n",
        "    \"\"\"\n",
        "    url = \"https://en.wikipedia.org/w/api.php\"\n",
        "    params = {\n",
        "        \"action\": \"query\",\n",
        "        \"titles\": title,\n",
        "        \"redirects\": True,\n",
        "        \"prop\": \"revisions|extracts\",\n",
        "        \"rvprop\": \"content|timestamp\",\n",
        "        \"rvslots\": \"main\",\n",
        "        \"explaintext\": True,\n",
        "        \"exlimit\": 1,\n",
        "        \"format\": \"json\"\n",
        "    }\n",
        "    r = requests.get(url, params=params, headers=HEADERS, timeout=30)\n",
        "    if not r.headers.get(\"Content-Type\", \"\").startswith(\"application/json\"):\n",
        "        return 0, 0, 0\n",
        "\n",
        "    data = r.json()\n",
        "    page = next(iter(data[\"query\"][\"pages\"].values()))\n",
        "\n",
        "    # Calculate days since last edit\n",
        "    last_edit_str = page.get(\"revisions\", [{}])[0].get(\"timestamp\", \"\")\n",
        "    days_since_edit = 0\n",
        "    if last_edit_str:\n",
        "        last_edit = datetime.strptime(last_edit_str, \"%Y-%m-%dT%H:%M:%SZ\")\n",
        "        days_since_edit = round((datetime.now(timezone.utc).replace(tzinfo=None) - last_edit).days)\n",
        "\n",
        "    # Word count\n",
        "    extract = page.get(\"extract\", \"\")\n",
        "    word_count = len(extract.split()) if extract else 0\n",
        "\n",
        "    # Count \"citation needed\"\n",
        "    wikitext = page.get(\"revisions\", [{}])[0].get(\"slots\", {}).get(\"main\", {}).get(\"*\", \"\")\n",
        "    citation_needed_count = wikitext.lower().count(\"citation needed\")\n",
        "\n",
        "    return days_since_edit, word_count, citation_needed_count\n",
        "\n",
        "def get_sections(title):\n",
        "    url = \"https://en.wikipedia.org/w/api.php\"\n",
        "    params = {\n",
        "        \"action\": \"parse\",\n",
        "        \"page\": title,\n",
        "        \"prop\": \"sections\",\n",
        "        \"redirects\": 1,\n",
        "        \"format\": \"json\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        r = requests.get(url, params=params, headers=HEADERS, timeout=30)\n",
        "        if not r.headers.get(\"Content-Type\", \"\").startswith(\"application/json\"):\n",
        "            return \"\"\n",
        "        parse_data = r.json().get(\"parse\", {})\n",
        "        if not parse_data:\n",
        "            return \"\"\n",
        "        sections = parse_data.get(\"sections\", [])\n",
        "        section_names = \", \".join(s[\"line\"] for s in sections)\n",
        "        return section_names\n",
        "    except:\n",
        "        return \"\"\n",
        "\n",
        "def get_citation_count(title):\n",
        "    \"\"\"\n",
        "    Universal citation counter that handles ALL citation formats:\n",
        "    - Standard <ref> tags\n",
        "    - {{sfn}}, {{sfnp}}, {{sfnm}} (short footnotes with variants)\n",
        "    - {{harv}}, {{harvnb}}, {{harvp}}, etc. (Harvard citations)\n",
        "    - {{r}}, {{rp}} (reference shortcuts)\n",
        "    - {{efn}} (explanatory footnotes)\n",
        "    - {{citation needed}} tags\n",
        "    - List-defined references\n",
        "    \"\"\"\n",
        "    url = \"https://en.wikipedia.org/w/api.php\"\n",
        "    params = {\n",
        "        \"action\": \"query\",\n",
        "        \"prop\": \"revisions\",\n",
        "        \"rvprop\": \"content\",\n",
        "        \"rvslots\": \"main\",\n",
        "        \"titles\": title,\n",
        "        \"redirects\": 1,\n",
        "        \"format\": \"json\"\n",
        "    }\n",
        "    try:\n",
        "        r = requests.get(url, params=params, headers=HEADERS, timeout=30)\n",
        "        if not r.headers.get(\"Content-Type\", \"\").startswith(\"application/json\"):\n",
        "            return 0\n",
        "\n",
        "        data = r.json()\n",
        "        pages = data.get(\"query\", {}).get(\"pages\", {})\n",
        "        page = next(iter(pages.values()), {})\n",
        "\n",
        "        if \"revisions\" not in page:\n",
        "            return 0\n",
        "\n",
        "        content = page.get(\"revisions\", [{}])[0].get(\"slots\", {}).get(\"main\", {}).get(\"*\", \"\")\n",
        "\n",
        "        # Method 1: Count standard <ref> tags (unique named refs + unnamed refs)\n",
        "        named_refs = set()\n",
        "        unnamed_count = 0\n",
        "\n",
        "        ref_pattern = r'<ref(?:\\s+[^>]*)?>'\n",
        "        all_refs = re.findall(ref_pattern, content, re.IGNORECASE | re.DOTALL)\n",
        "\n",
        "        for ref in all_refs:\n",
        "            if ref.strip().endswith('/>'):\n",
        "                continue\n",
        "\n",
        "            name_match = re.search(r'name\\s*=\\s*[\"\\']([^\"\\']+)[\"\\']', ref, re.IGNORECASE)\n",
        "            if name_match:\n",
        "                named_refs.add(name_match.group(1))\n",
        "            else:\n",
        "                unnamed_count += 1\n",
        "\n",
        "        ref_count = len(named_refs) + unnamed_count\n",
        "\n",
        "        # Method 2: Count ALL sfn variants (sfn, sfnp, sfnm, sfnmp, etc.)\n",
        "        sfn_pattern = r'\\{\\{sfn[a-z]*\\|'\n",
        "        sfn_count = len(re.findall(sfn_pattern, content, re.IGNORECASE))\n",
        "\n",
        "        # Method 3: Count ALL harv variants (harv, harvnb, harvp, harvtxt, etc.)\n",
        "        harv_pattern = r'\\{\\{harv[a-z]*\\|'\n",
        "        harv_count = len(re.findall(harv_pattern, content, re.IGNORECASE))\n",
        "\n",
        "        # Method 4: Count {{r}} and {{rp}} (reference shortcuts)\n",
        "        r_pattern = r'\\{\\{rp?\\|'\n",
        "        r_count = len(re.findall(r_pattern, content, re.IGNORECASE))\n",
        "\n",
        "        # Method 5: Count {{efn}} (explanatory footnotes)\n",
        "        efn_pattern = r'\\{\\{efn[a-z]*\\|'\n",
        "        efn_count = len(re.findall(efn_pattern, content, re.IGNORECASE))\n",
        "\n",
        "        # Method 6: Count list-defined references\n",
        "        ldr_count = 0\n",
        "        ldr_match = re.search(r'\\{\\{reflist\\|refs=(.*?)\\n\\}\\}', content, re.IGNORECASE | re.DOTALL)\n",
        "        if ldr_match:\n",
        "            ldr_content = ldr_match.group(1)\n",
        "            ldr_count = len(re.findall(r'<ref name=', ldr_content, re.IGNORECASE))\n",
        "\n",
        "        # Combine footnote-style citations (sfn + efn count together, as they're often used together)\n",
        "        footnote_count = sfn_count + efn_count\n",
        "\n",
        "        # Use the highest count from all methods\n",
        "        # (articles typically use ONE main citation style)\n",
        "        total_citations = max(ref_count, footnote_count, harv_count, r_count, ldr_count)\n",
        "\n",
        "        return total_citations\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"   ⚠ Citation error: {e}\")\n",
        "        return 0\n",
        "\n",
        "def get_images_and_categories(title):\n",
        "    \"\"\"\n",
        "    Get image count and category count\n",
        "    \"\"\"\n",
        "    url = \"https://en.wikipedia.org/w/api.php\"\n",
        "    params = {\n",
        "        \"action\": \"query\",\n",
        "        \"titles\": title,\n",
        "        \"redirects\": True,\n",
        "        \"prop\": \"images|categories\",\n",
        "        \"imlimit\": 500,\n",
        "        \"cllimit\": 500,\n",
        "        \"format\": \"json\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        r = requests.get(url, params=params, headers=HEADERS, timeout=30)\n",
        "        if not r.headers.get(\"Content-Type\", \"\").startswith(\"application/json\"):\n",
        "            return 0, 0\n",
        "\n",
        "        data = r.json()\n",
        "        page = next(iter(data[\"query\"][\"pages\"].values()))\n",
        "\n",
        "        images = len(page.get(\"images\", []))\n",
        "        categories = len(page.get(\"categories\", []))\n",
        "\n",
        "        return images, categories\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Images/categories error: {e}\")\n",
        "        return 0, 0\n",
        "\n",
        "def get_edit_statistics(title):\n",
        "    \"\"\"\n",
        "    Get edit statistics: total editors and edits in last year\n",
        "    \"\"\"\n",
        "    url = \"https://en.wikipedia.org/w/api.php\"\n",
        "\n",
        "    params = {\n",
        "        \"action\": \"query\",\n",
        "        \"titles\": title,\n",
        "        \"redirects\": True,\n",
        "        \"prop\": \"revisions\",\n",
        "        \"rvprop\": \"timestamp|user\",\n",
        "        \"rvlimit\": 500,\n",
        "        \"format\": \"json\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        r = requests.get(url, params=params, headers=HEADERS, timeout=30)\n",
        "        if not r.headers.get(\"Content-Type\", \"\").startswith(\"application/json\"):\n",
        "            return 0, 0\n",
        "\n",
        "        data = r.json()\n",
        "        pages = data.get(\"query\", {}).get(\"pages\", {})\n",
        "        page = next(iter(pages.values()), {})\n",
        "\n",
        "        revisions = page.get(\"revisions\", [])\n",
        "\n",
        "        unique_editors = set()\n",
        "        recent_edits = 0\n",
        "        one_year_ago = datetime.now(timezone.utc) - timedelta(days=365)\n",
        "\n",
        "        for rev in revisions:\n",
        "            user = rev.get(\"user\", \"\")\n",
        "            if user:\n",
        "                unique_editors.add(user)\n",
        "\n",
        "            # Count recent edits (last year)\n",
        "            timestamp_str = rev.get(\"timestamp\", \"\")\n",
        "            if timestamp_str:\n",
        "                timestamp = datetime.strptime(timestamp_str, \"%Y-%m-%dT%H:%M:%SZ\").replace(tzinfo=timezone.utc)\n",
        "                if timestamp >= one_year_ago:\n",
        "                    recent_edits += 1\n",
        "\n",
        "        return len(unique_editors), recent_edits\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Edit statistics error: {e}\")\n",
        "        return 0, 0\n",
        "\n",
        "def get_talk_page_size(title):\n",
        "    \"\"\"\n",
        "    Get the size of the talk page in bytes\n",
        "    \"\"\"\n",
        "    url = \"https://en.wikipedia.org/w/api.php\"\n",
        "    params = {\n",
        "        \"action\": \"query\",\n",
        "        \"titles\": f\"Talk:{title}\",\n",
        "        \"redirects\": True,\n",
        "        \"prop\": \"revisions\",\n",
        "        \"rvprop\": \"size\",\n",
        "        \"rvlimit\": 1,\n",
        "        \"format\": \"json\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        r = requests.get(url, params=params, headers=HEADERS, timeout=30)\n",
        "        if not r.headers.get(\"Content-Type\", \"\").startswith(\"application/json\"):\n",
        "            return 0\n",
        "\n",
        "        data = r.json()\n",
        "        pages = data.get(\"query\", {}).get(\"pages\", {})\n",
        "        page = next(iter(pages.values()), {})\n",
        "\n",
        "        # Check if page exists (missing pages have negative IDs)\n",
        "        if int(page.get(\"pageid\", -1)) < 0:\n",
        "            return 0\n",
        "\n",
        "        size = page.get(\"revisions\", [{}])[0].get(\"size\", 0)\n",
        "        return size\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Talk page error: {e}\")\n",
        "        return 0\n",
        "\n",
        "def get_pageviews_3mo(title):\n",
        "    end = datetime.now(timezone.utc).replace(tzinfo=None)\n",
        "    start = end - timedelta(days=90)\n",
        "    encoded_title = quote(title.replace(' ', '_'))\n",
        "    url = (\n",
        "        f\"https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/\"\n",
        "        f\"en.wikipedia/all-access/user/\"\n",
        "        f\"{encoded_title}/daily/\"\n",
        "        f\"{start:%Y%m%d}/{end:%Y%m%d}\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        r = requests.get(url, headers=HEADERS, timeout=30)\n",
        "        if not r.headers.get(\"Content-Type\", \"\").startswith(\"application/json\"):\n",
        "            return 0\n",
        "        data = r.json()\n",
        "        return sum(d[\"views\"] for d in data.get(\"items\", []))\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "# Main execution\n",
        "print(\"=\"*60)\n",
        "print(f\"Fetching {CLASS} {WIKIPROJECT} articles from Wikipedia\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "articles = get_project_articles(WIKIPROJECT, CLASS, MAX_ARTICLES)  # Change to None for all\n",
        "\n",
        "if not articles:\n",
        "    print(\"\\nNo articles found.\")\n",
        "    exit()\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Processing {len(articles)} articles...\")\n",
        "print(f\"Estimated time: {len(articles) * 6 * REQUEST_DELAY / 60:.1f} - {len(articles) * 6 * 0.5 / 60:.1f} minutes\")\n",
        "print(f\"{'='*60}\\n\")\n",
        "\n",
        "rows = []\n",
        "start_time = time.time()\n",
        "\n",
        "for i, title in enumerate(articles, 1):\n",
        "    print(f\"[{i}/{len(articles)}] {title}\")\n",
        "    try:\n",
        "        days_since_edit, word_count, citation_needed = get_article_metadata(title)\n",
        "        sections = get_sections(title)\n",
        "        citations = get_citation_count(title)\n",
        "        views = get_pageviews_3mo(title)\n",
        "        images, categories = get_images_and_categories(title)\n",
        "        num_editors, recent_edits = get_edit_statistics(title)\n",
        "        talk_page_size = get_talk_page_size(title)\n",
        "\n",
        "        rows.append({\n",
        "            \"Article\": title,\n",
        "            \"Days Since Last Edit\": days_since_edit,\n",
        "            \"Word Count\": word_count,\n",
        "            \"Section Names\": sections,\n",
        "            \"Citations\": citations,\n",
        "            \"Citation Needed Count\": citation_needed,\n",
        "            \"Images\": images,\n",
        "            \"Categories\": categories,\n",
        "            \"Total Editors\": num_editors,\n",
        "            \"Edits Last Year\": recent_edits,\n",
        "            \"Talk Page Size (bytes)\": talk_page_size,\n",
        "            \"Last 3 Months Views\": views\n",
        "        })\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        rows.append({\n",
        "            \"Article\": title,\n",
        "            \"Days Since Last Edit\": 0,\n",
        "            \"Word Count\": 0,\n",
        "            \"Section Names\": \"\",\n",
        "            \"Citations\": 0,\n",
        "            \"Citation Needed Count\": 0,\n",
        "            \"Images\": 0,\n",
        "            \"Categories\": 0,\n",
        "            \"Total Editors\": 0,\n",
        "            \"Edits Last Year\": 0,\n",
        "            \"Talk Page Size (bytes)\": 0,\n",
        "            \"Last 3 Months Views\": 0\n",
        "        })\n",
        "\n",
        "    # Checkpoint saves\n",
        "    if i % CHECKPOINT_INTERVAL == 0:\n",
        "        df_checkpoint = pd.DataFrame(rows)\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        checkpoint_file = f\"{WIKIPROJECT}_{CLASS}_{EMAIL}_{MAX_ARTICLES}.csv\"\n",
        "        df_checkpoint.to_csv(checkpoint_file, index=False)\n",
        "        elapsed = time.time() - start_time\n",
        "        remaining = (elapsed / i) * (len(articles) - i)\n",
        "        print(f\"Checkpoint saved: {checkpoint_file}\")\n",
        "        print(f\"Elapsed: {elapsed/60:.1f}min | Estimated remaining: {remaining/60:.1f}min\")\n",
        "\n",
        "    time.sleep(REQUEST_DELAY)\n",
        "\n",
        "# Save final CSV and display dataframe\n",
        "if rows:\n",
        "    df = pd.DataFrame(rows)\n",
        "    #timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    filename = f\"{WIKIPROJECT}_{CLASS}_{EMAIL}_{MAX_ARTICLES}.csv\"\n",
        "    df.to_csv(filename, index=False)\n",
        "    filename = f\"/content/drive/MyDrive/{WIKIPROJECT}_{CLASS}_{EMAIL}_{MAX_ARTICLES}.csv\"\n",
        "    df.to_csv(filename, index=False)\n",
        "\n",
        "    elapsed_total = time.time() - start_time\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Successfully saved {len(df)} articles to: {filename}\")\n",
        "    print(f\"Total time: {elapsed_total/60:.1f} minutes\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n",
        "    # Display editable dataframe\n",
        "    print(\"Editable DataFrame:\")\n",
        "    display(df)\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"Summary statistics:\")\n",
        "    print(f\"{'='*60}\")\n",
        "    numeric_cols = ['Days Since Last Edit', 'Word Count', 'Citations', 'Citation Needed Count',\n",
        "                   'Images', 'Categories', 'Total Editors', 'Edits Last Year',\n",
        "                   'Talk Page Size (bytes)', 'Last 3 Months Views']\n",
        "    print(round(df[numeric_cols].describe(), 1))\n",
        "else:\n",
        "    print(\"\\nNo data to save\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1pPWf-puxWKAhrKYk3VVPRoa43gyAVqVN",
      "authorship_tag": "ABX9TyOEL4MGcJeKoqrUAcXHCSUl",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}