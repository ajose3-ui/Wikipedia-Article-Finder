{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajose3-ui/Wikipedia-Article-Finder/blob/main/WIKIPROJECT_THING.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "General tips for finding articles:\n",
        "\n",
        "\n",
        "*   [Template for choosing an article](https://en.wikipedia.org/wiki/Template:Dashboard.wikiedu.org_choose_article).\n",
        "*   [Template for evaluating an article](https://en.wikipedia.org/wiki/Template:Dashboard.wikiedu.org_evaluate_article).\n",
        "*   Chose a topic that you are intrested in learning more about or are already passionate about. **Strive to pick an article about something that is meaningful to you**.\n",
        "*   An article can only ever be as good as its sources. **Make sure you can find 3-5 high-quality sources** (literature reviews, textbooks, publications, books created by experts and published by a reputable institution) for any article you are seriously considering working on. You want to find secondary sources that focus on your topic and that are published by reputable institutions that do not have a vested interest in reporting information in a biased way.\n",
        "*   **Talk pages may give you an idea of what changes need to be made to an article you are interested in**.\n",
        "*   **You can create a [watchlist](https://youtu.be/Pa8Htsj3Gxg) to keep track of all the pages you are interested in**.\n",
        "*   Keep in mind the fact that articles on the histories of LGBTQ+ individuals and of Black figures in STEM are relatively lacking, while articles on warfare and sports tend to be among the highest quality articles Wikipedia has to offer. This does not have to be intentional to be harmful.\n",
        "*   Try to avoid editing “Good,” “A-Class,” or “Featured” articles (it is harder to improve them). **Try to pick articles that are of lower qualities but are of great importance to Wikiprojects or get a low of views**.\n",
        "*  **Signs that articles that need improvement**: **imbalanced sections** (fringe aspects are emphasized while more important one have little information about them), **writen with little neutrality**, **few refrences are listed**, **the structure has no flow**, **the intro is hard to follow and lacks detail or accuracy**.\n",
        "*   **Avoid controversial topics**. A list of them are provided at the bottom of [this page](https://en.wikipedia.org/wiki/Wikipedia:Contentious_topics).\n",
        "*   **Avoid conflicts of interest**. Do not write about your program or the professor or heavily rely on their works for your project.\n",
        "*   **Scroll to the bottom of an article you are interested in to check out its categories** ([this](https://en.wikipedia.org/wiki/Category:Academic_disciplines) might be a good place to start). Clicking on these categories takes you to related articles and subcategories.\n",
        "*   When you go to the talk page of an article you will often see a colorful that tell you the [Wikiprojects](https://en.wikipedia.org/wiki/Wikipedia:WikiProject_Council/Directory) it is associated with and how important it is to those Wikiproject. **Try to work on articles of mid or higher importance**.\n",
        "*   **You can use [article finder](https://dashboard.wikiedu.org/article_finder) to find articles to edit. If you click on “Show Options,” you can pick the minimum views and maximum article quality you want for your results**. You can also decide if you want to search by category or key word. Additionally, if you scroll down to the bottom of the page, you can load more than just 50 results. Try to look for low quality articles that  get a lot of views.\n",
        "*   [This is a super helpful page](https://en.wikipedia.org/wiki/Wikipedia:Student_assignments) if you want to learn more about how to successfully complete this project in general. They have a good section on choosing an article that is relatively concise. Please read the section if you are able to.\n",
        "*   [This is a great article](https://en.wikipedia.org/wiki/Wikipedia:Manual_of_Style/Words_to_watch) to keep in mind when looking for issues with the language used in articles. Avoid making these mistakes.\n",
        "*   [This page](https://en.wikipedia.org/wiki/Wikipedia:Task_Center) guides you to articles that need fact-checking, categorization, and copy editing. This might be a good place to go to find an article to edit.\n",
        "\n",
        "\n",
        "Other, even more general stuff:\n",
        "*   This offers a list of [extra trainings](https://en.wikipedia.org/wiki/Help:Introduction) if you are still confused about some topic covered in your trainings.\n",
        "*   Scroll down to the bottom of [this page](https://en.wikipedia.org/wiki/Help:Contents) to search for answers to [frequently asked questions](https://en.wikipedia.org/wiki/Wikipedia:FAQ) about Wikipedia and look at the help desk archives. There are also a lot of how to guides located here.\n",
        "*   If you have questions about using and editing wikipedia you can go [here](https://en.wikipedia.org/wiki/Wikipedia:Teahouse) or [here](https://en.wikipedia.org/wiki/Wikipedia:Help_desk). You can also search for help pages and archived questions here. On [this page](https://en.wikipedia.org/wiki/Wikipedia:Teahouse/Suggestions), you can get personalized suggestions for articles delivered right to your user talk page.\n",
        "*   [This is another good resource](https://en.wikipedia.org/wiki/Wikipedia:The_Wikipedia_Adventure) for learning how to edit a page.\n",
        "*   [This is the wikipedia help menu](https://en.wikipedia.org/wiki/Help:Menu), and [this is the help directory](https://en.wikipedia.org/wiki/Help:Directory).\n",
        "*   [What to strive for when editing an article](https://en.wikipedia.org/wiki/Wikipedia:The_perfect_article).\n",
        "*   [This short page](https://en.wikipedia.org/wiki/Help:Your_first_article) walks you through everything you need to know when editing your first article.\n",
        "*   [This instruction manual](https://en.wikipedia.org/wiki/Wikipedia:Instructional_material) has a list of videos, books, and short tutorials that might help you with any given thing you are struggling with. Whether you want to do a deep dive or brush up on something in a few minutes, this is a good resource.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-yVnXE8kOAAf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install these so that everything works properly:"
      ],
      "metadata": {
        "id": "75LFKT9tplhq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "!pip install textblob vaderSentiment textstat\n",
        "!pip install itables"
      ],
      "metadata": {
        "cellView": "form",
        "id": "5vChMDPsXFAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Search for topics that intrest you (for detailed table, scroll all the way to the right and click the top button to do an advanced search):"
      ],
      "metadata": {
        "id": "CU8MR-qFptNi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta, timezone\n",
        "from urllib.parse import quote\n",
        "import time\n",
        "import re\n",
        "from IPython.display import display\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from textblob import TextBlob\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "import textstat\n",
        "\n",
        "DETAILED_TABLE = True #@param {type:\"boolean\"}\n",
        "DETAILED_DATA = True #@param {type:\"boolean\"}\n",
        "\n",
        "EMAIL = \"ajose3@hawk.illinoistech.edu\" #@param {type:\"string\"}\n",
        "HEADERS = {\"User-Agent\": f\"Wikiproject_article_finder/1.0 (Educational research; {EMAIL}; Python/requests)\"}\n",
        "SEARCH_QUERY = \"surfing\" #@param {type:\"string\"}\n",
        "MAX_ARTICLES = \"30\" #@param {type:\"string\"}\n",
        "\n",
        "try:\n",
        "    MAX_ARTICLES = int(MAX_ARTICLES)\n",
        "except (ValueError, TypeError):\n",
        "    MAX_ARTICLES = None\n",
        "\n",
        "LANG = \"en\"\n",
        "REQUEST_DELAY = 0.03\n",
        "CHECKPOINT_INTERVAL = 100\n",
        "MAX_WORKERS = 3\n",
        "\n",
        "def search_wikipedia_articles(query, max_articles=None):\n",
        "    url = \"https://en.wikipedia.org/w/api.php\"\n",
        "    titles = []\n",
        "    offset = 0\n",
        "\n",
        "    while True:\n",
        "        params = {\"action\": \"query\", \"list\": \"search\", \"srsearch\": query, \"srlimit\": 50, \"sroffset\": offset, \"format\": \"json\"}\n",
        "        r = requests.get(url, params=params, headers=HEADERS, timeout=30)\n",
        "\n",
        "        if not r.headers.get(\"Content-Type\", \"\").startswith(\"application/json\"):\n",
        "            print(\"Non-JSON response, retrying...\")\n",
        "            time.sleep(2)\n",
        "            continue\n",
        "\n",
        "        data = r.json()\n",
        "        results = data.get(\"query\", {}).get(\"search\", [])\n",
        "        if not results:\n",
        "            break\n",
        "\n",
        "        for result in results:\n",
        "            title = result[\"title\"]\n",
        "            titles.append(title)\n",
        "            if max_articles and len(titles) >= max_articles:\n",
        "                return titles[:max_articles]\n",
        "\n",
        "        if \"continue\" in data:\n",
        "            offset = data[\"continue\"][\"sroffset\"]\n",
        "            time.sleep(REQUEST_DELAY)\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    print(f\"Total articles found: {len(titles)}\")\n",
        "    return titles\n",
        "\n",
        "def get_article_class(title):\n",
        "    url = \"https://en.wikipedia.org/w/api.php\"\n",
        "\n",
        "    try:\n",
        "        params = {\"action\": \"query\", \"titles\": f\"Talk:{title}\", \"prop\": \"revisions\", \"rvprop\": \"content\", \"rvslots\": \"main\", \"format\": \"json\"}\n",
        "        r = requests.get(url, params=params, headers=HEADERS, timeout=30)\n",
        "\n",
        "        if not r.headers.get(\"Content-Type\", \"\").startswith(\"application/json\"):\n",
        "            return \"Unassessed\"\n",
        "\n",
        "        data = r.json()\n",
        "\n",
        "        if \"query\" not in data or \"pages\" not in data[\"query\"]:\n",
        "            return \"Unassessed\"\n",
        "\n",
        "        page = next(iter(data.get(\"query\", {}).get(\"pages\", {}).values()))\n",
        "\n",
        "        if int(page.get(\"pageid\", -1)) <= 0:\n",
        "            return \"Unassessed\"\n",
        "\n",
        "        revisions = page.get(\"revisions\", [])\n",
        "        if not revisions:\n",
        "            return \"Unassessed\"\n",
        "\n",
        "        talk_text = revisions[0].get(\"slots\", {}).get(\"main\", {}).get(\"*\", \"\")\n",
        "\n",
        "        if not talk_text or len(talk_text.strip()) == 0:\n",
        "            return \"Unassessed\"\n",
        "\n",
        "        talk_lower = talk_text.lower()\n",
        "\n",
        "        class_mapping = {\n",
        "            'fa': 'FA', 'featured': 'FA', 'fa-class': 'FA', 'featured article': 'FA',\n",
        "            'fl': 'FL', 'fl-class': 'FL', 'featured list': 'FL',\n",
        "            'a': 'A', 'a-class': 'A',\n",
        "            'ga': 'GA', 'good': 'GA', 'ga-class': 'GA', 'good article': 'GA',\n",
        "            'b': 'B', 'b-class': 'B',\n",
        "            'c': 'C', 'c-class': 'C',\n",
        "            'start': 'START', 'start-class': 'START',\n",
        "            'stub': 'STUB', 'stub-class': 'STUB',\n",
        "            'list': 'LIST', 'list-class': 'LIST'\n",
        "        }\n",
        "\n",
        "        class_patterns = [\n",
        "            r'\\|\\s*class\\s*=\\s*([A-Za-z\\-]+)',\n",
        "            r'\\|\\s*currentstatus\\s*=\\s*([A-Za-z\\-]+)',\n",
        "            r'{{[Cc]lass\\|([A-Za-z\\-]+)',\n",
        "            r'{{WikiProject[^}]*\\|\\s*class\\s*=\\s*([A-Za-z\\-]+)',\n",
        "            r'{{[Aa]rticle\\s*history[^}]*\\|currentstatus\\s*=\\s*([A-Za-z\\-]+)',\n",
        "            r'{{[Rr]ated\\s+as\\s+([A-Za-z\\-]+)',\n",
        "            r'\\|\\s*1\\s*=\\s*([A-Za-z\\-]+)(?:\\-class)?',\n",
        "            r'class\\s*[:=]\\s*([A-Za-z\\-]+)',\n",
        "            r'quality\\s*[:=]\\s*([A-Za-z\\-]+)',\n",
        "        ]\n",
        "\n",
        "        found_classes = []\n",
        "        for pattern in class_patterns:\n",
        "            matches = re.findall(pattern, talk_text, re.IGNORECASE)\n",
        "            for match in matches:\n",
        "                clean = match.strip().lower().replace('-class', '').replace('_class', '').strip()\n",
        "                if clean in class_mapping:\n",
        "                    found_classes.append(class_mapping[clean])\n",
        "\n",
        "        if found_classes:\n",
        "            priority = ['FA', 'FL', 'GA', 'A', 'B', 'C', 'START', 'STUB', 'LIST']\n",
        "            for cls in priority:\n",
        "                if cls in found_classes:\n",
        "                    return cls\n",
        "            return found_classes[0]\n",
        "\n",
        "        specific_markers = {\n",
        "            'this is a featured article': 'FA',\n",
        "            'featured article star': 'FA',\n",
        "            'this is a good article': 'GA',\n",
        "            'good article': 'GA',\n",
        "            'ga-icon': 'GA'\n",
        "        }\n",
        "\n",
        "        for marker, cls in specific_markers.items():\n",
        "            if marker in talk_lower:\n",
        "                return cls\n",
        "\n",
        "        badge_patterns = [\n",
        "            (r'{{featured article}}', 'FA'),\n",
        "            (r'{{fa}}', 'FA'),\n",
        "            (r'{{good article}}', 'GA'),\n",
        "            (r'{{ga}}', 'GA'),\n",
        "        ]\n",
        "\n",
        "        for pattern, cls in badge_patterns:\n",
        "            if re.search(pattern, talk_text, re.IGNORECASE):\n",
        "                return cls\n",
        "\n",
        "        if 'wikiproject' not in talk_lower:\n",
        "            return \"Unassessed\"\n",
        "\n",
        "        quality_indicators = {\n",
        "            'stub': 'STUB',\n",
        "            'start': 'START',\n",
        "            'b-class': 'B',\n",
        "            'c-class': 'C'\n",
        "        }\n",
        "\n",
        "        for indicator, cls in quality_indicators.items():\n",
        "            if indicator in talk_lower:\n",
        "                return cls\n",
        "\n",
        "        return \"Unassessed\"\n",
        "\n",
        "    except requests.exceptions.Timeout:\n",
        "        return \"Unassessed\"\n",
        "    except requests.exceptions.RequestException:\n",
        "        return \"Unassessed\"\n",
        "    except Exception as e:\n",
        "        return \"Unassessed\"\n",
        "\n",
        "def get_all_article_data(title):\n",
        "    url = \"https://en.wikipedia.org/w/api.php\"\n",
        "    params = {\"action\": \"query\", \"titles\": title, \"redirects\": True, \"prop\": \"revisions|extracts|images|categories\", \"rvprop\": \"content|timestamp\", \"rvslots\": \"main\", \"explaintext\": True, \"exlimit\": 1, \"imlimit\": 500, \"cllimit\": 500, \"format\": \"json\"}\n",
        "\n",
        "    try:\n",
        "        r = requests.get(url, params=params, headers=HEADERS, timeout=30)\n",
        "        if not r.headers.get(\"Content-Type\", \"\").startswith(\"application/json\"):\n",
        "            return None\n",
        "\n",
        "        data = r.json()\n",
        "        page = next(iter(data[\"query\"][\"pages\"].values()))\n",
        "\n",
        "        wikitext = page.get(\"revisions\", [{}])[0].get(\"slots\", {}).get(\"main\", {}).get(\"*\", \"\")\n",
        "        plaintext = page.get(\"extract\", \"\")\n",
        "        timestamp = page.get(\"revisions\", [{}])[0].get(\"timestamp\", \"\")\n",
        "        images = len(page.get(\"images\", []))\n",
        "        categories = len(page.get(\"categories\", []))\n",
        "\n",
        "        return {\"wikitext\": wikitext, \"plaintext\": plaintext, \"timestamp\": timestamp, \"images\": images, \"categories\": categories}\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching data for {title}: {e}\")\n",
        "        return None\n",
        "\n",
        "def analyze_from_cached_data(title, cached_data):\n",
        "    if not cached_data:\n",
        "        return get_empty_metrics()\n",
        "\n",
        "    wikitext = cached_data[\"wikitext\"]\n",
        "    plaintext = cached_data[\"plaintext\"]\n",
        "    timestamp = cached_data[\"timestamp\"]\n",
        "    metrics = {}\n",
        "\n",
        "    days_since_edit = 0\n",
        "    if timestamp:\n",
        "        last_edit = datetime.strptime(timestamp, \"%Y-%m-%dT%H:%M:%SZ\")\n",
        "        days_since_edit = round((datetime.now(timezone.utc).replace(tzinfo=None) - last_edit).days)\n",
        "\n",
        "    word_count = len(plaintext.split()) if plaintext else 0\n",
        "    citation_needed_count = wikitext.lower().count(\"citation needed\")\n",
        "\n",
        "    metrics[\"Days Since Last Edit\"] = days_since_edit\n",
        "    metrics[\"Word Count\"] = word_count\n",
        "    metrics[\"Citations Needed\"] = citation_needed_count\n",
        "    metrics[\"Images\"] = cached_data[\"images\"]\n",
        "    metrics[\"Categories\"] = cached_data[\"categories\"]\n",
        "\n",
        "    source_metrics = analyze_source_quality_from_text(wikitext)\n",
        "    metrics.update(source_metrics)\n",
        "    neutrality_metrics = analyze_neutrality_from_text(plaintext)\n",
        "    metrics.update(neutrality_metrics)\n",
        "    readability_metrics = analyze_readability_from_text(plaintext)\n",
        "    metrics.update(readability_metrics)\n",
        "    sentiment_metrics = detect_sentiment_bias_from_text(plaintext)\n",
        "    metrics.update(sentiment_metrics)\n",
        "    metrics[\"Citations\"] = get_citation_count_from_text(wikitext)\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def analyze_source_quality_from_text(wikitext):\n",
        "    cite_patterns = {\"journal\": r'{{cite journal', \"book\": r'{{cite book', \"web\": r'{{cite web', \"news\": r'{{cite news'}\n",
        "    source_types = {k: len(re.findall(v, wikitext, re.IGNORECASE)) for k, v in cite_patterns.items()}\n",
        "    total_typed_sources = sum(source_types.values())\n",
        "    years = []\n",
        "    current_year = datetime.now().year\n",
        "\n",
        "    citation_blocks = re.findall(r'{{cite[^}]+}}', wikitext, re.IGNORECASE | re.DOTALL)\n",
        "    for block in citation_blocks:\n",
        "        year_matches = re.findall(r'\\|(?:year|date|publication-date|access-date)\\s*=\\s*[^\\d]*(\\d{4})', block, re.IGNORECASE)\n",
        "        for year_str in year_matches:\n",
        "            year = int(year_str)\n",
        "            if 1800 <= year <= current_year:\n",
        "                years.append(year)\n",
        "                break\n",
        "\n",
        "    ref_blocks = re.findall(r'<ref[^>]*>(.*?)</ref>', wikitext, re.IGNORECASE | re.DOTALL)\n",
        "    for ref in ref_blocks:\n",
        "        year_patterns = [r'\\((\\d{4})\\)', r'[,\\s](\\d{4})[,\\.\\s]', r'\"(\\d{4})\"', r'(\\d{4})-\\d{2}-\\d{2}']\n",
        "        for pattern in year_patterns:\n",
        "            year_matches = re.findall(pattern, ref)\n",
        "            if year_matches:\n",
        "                year = int(year_matches[0])\n",
        "                if 1800 <= year <= current_year:\n",
        "                    years.append(year)\n",
        "                    break\n",
        "\n",
        "    short_footnotes = re.findall(r'{{(?:sfn|harvnb|harv)[^}]*\\|[^}]*?(\\d{4})', wikitext, re.IGNORECASE)\n",
        "    for year_str in short_footnotes:\n",
        "        year = int(year_str)\n",
        "        if 1800 <= year <= current_year:\n",
        "            years.append(year)\n",
        "\n",
        "    citation_templates = re.findall(r'{{citation[^}]+}}', wikitext, re.IGNORECASE | re.DOTALL)\n",
        "    for block in citation_templates:\n",
        "        year_matches = re.findall(r'\\|(?:year|date|publication-date)\\s*=\\s*[^\\d]*(\\d{4})', block, re.IGNORECASE)\n",
        "        for year_str in year_matches:\n",
        "            year = int(year_str)\n",
        "            if 1800 <= year <= current_year:\n",
        "                years.append(year)\n",
        "                break\n",
        "\n",
        "    recent_sources = len([y for y in years if current_year - y <= 5]) if years else 0\n",
        "    avg_source_age = (current_year - sum(years) / len(years)) if years else 0\n",
        "    quality_score = 0\n",
        "\n",
        "    if total_typed_sources > 0:\n",
        "        scholarly_ratio = (source_types[\"journal\"] + source_types[\"book\"]) / total_typed_sources\n",
        "        quality_score += scholarly_ratio * 50\n",
        "\n",
        "    source_diversity = len([v for v in source_types.values() if v > 0])\n",
        "    quality_score += source_diversity * 10\n",
        "\n",
        "    if avg_source_age < 10:\n",
        "        quality_score += 25\n",
        "    elif avg_source_age < 20:\n",
        "        quality_score += 15\n",
        "\n",
        "    return {\"Journal Sources\": source_types[\"journal\"], \"Book Sources\": source_types[\"book\"], \"Web Sources\": source_types[\"web\"], \"News Sources\": source_types[\"news\"], \"Avg Source Age\": round(avg_source_age, 1), \"Recent Sources (5yr)\": recent_sources, \"Source Quality Score\": round(min(quality_score, 100), 1)}\n",
        "\n",
        "def analyze_neutrality_from_text(text):\n",
        "    if not text:\n",
        "        return {\"Hedging Words\": 0, \"Peacock Words\": 0, \"Weasel Words\": 0, \"Value Judgments\": 0, \"Neutrality Score\": 100}\n",
        "\n",
        "    text_lower = text.lower()\n",
        "    peacock_terms = [\"legendary\", \"iconic\", \"acclaimed\", \"prestigious\", \"renowned\", \"celebrated\", \"world-class\", \"premier\", \"leading\", \"foremost\", \"groundbreaking\", \"seminal\", \"pivotal\", \"revolutionary\", \"extraordinary\", \"remarkable\", \"outstanding\", \"exceptional\", \"unparalleled\", \"best\"]\n",
        "    peacock_count = sum(text_lower.count(term) for term in peacock_terms)\n",
        "\n",
        "    weasel_patterns = [r'\\bsome (people|experts|scholars|critics|observers|say|believe)', r'\\bmany (believe|argue|claim|suggest|think|feel)', r'\\bit (is said|has been said|is believed|is widely|is commonly)', r'\\bmost (people|experts|scholars)', r'\\bwidely (regarded|considered|accepted|believed)', r'\\boften (considered|regarded|viewed)', r'\\bgenerally (accepted|believed|considered)']\n",
        "    weasel_count = sum(len(re.findall(pattern, text_lower)) for pattern in weasel_patterns)\n",
        "\n",
        "    hedging_words = [\"perhaps\", \"possibly\", \"maybe\", \"might\", \"could\", \"may\", \"seemingly\"]\n",
        "    hedging_count = sum(text_lower.count(word) for word in hedging_words)\n",
        "\n",
        "    value_words = [\"unfortunately\", \"fortunately\", \"clearly\", \"obviously\", \"naturally\", \"of course\", \"undoubtedly\", \"certainly\", \"arguably\", \"notably\", \"importantly\", \"surprisingly\", \"interestingly\", \"regrettably\"]\n",
        "    value_count = sum(text_lower.count(word) for word in value_words)\n",
        "\n",
        "    word_count = len(text.split())\n",
        "    neutrality_score = 100\n",
        "\n",
        "    if word_count > 0:\n",
        "        neutrality_score -= (peacock_count / word_count * 1000) * 10\n",
        "        neutrality_score -= (weasel_count / word_count * 1000) * 15\n",
        "        neutrality_score -= (value_count / word_count * 1000) * 8\n",
        "\n",
        "    neutrality_score = max(0, min(100, neutrality_score))\n",
        "\n",
        "    return {\"Hedging Words\": hedging_count, \"Peacock Words\": peacock_count, \"Weasel Words\": weasel_count, \"Value Judgments\": value_count, \"Neutrality Score\": round(neutrality_score, 1)}\n",
        "\n",
        "def analyze_readability_from_text(text):\n",
        "    if not text or len(text) < 100:\n",
        "        return {\"Flesch-Kincaid Grade\": 0, \"Reading Level\": \"Unknown\"}\n",
        "\n",
        "    try:\n",
        "        flesch_grade = textstat.flesch_kincaid_grade(text)\n",
        "        flesch_ease = textstat.flesch_reading_ease(text)\n",
        "\n",
        "        if flesch_ease >= 90:\n",
        "            level = \"Elementary (5th grade)\"\n",
        "        elif flesch_ease >= 80:\n",
        "            level = \"Middle School (6-7th)\"\n",
        "        elif flesch_ease >= 70:\n",
        "            level = \"High School (8-9th)\"\n",
        "        elif flesch_ease >= 60:\n",
        "            level = \"High School (10-12th)\"\n",
        "        elif flesch_ease >= 50:\n",
        "            level = \"College\"\n",
        "        elif flesch_ease >= 30:\n",
        "            level = \"College Graduate\"\n",
        "        else:\n",
        "            level = \"Professional/Academic\"\n",
        "\n",
        "        return {\"Flesch-Kincaid Grade\": round(flesch_grade, 1), \"Reading Level\": level}\n",
        "    except:\n",
        "        return {\"Flesch-Kincaid Grade\": 0, \"Reading Level\": \"Error\"}\n",
        "\n",
        "def detect_sentiment_bias_from_text(text):\n",
        "    if not text:\n",
        "        return {\"Polarity\": 0, \"Subjectivity\": 0, \"VADER Compound\": 0, \"Sentiment\": \"Neutral\"}\n",
        "\n",
        "    try:\n",
        "        blob = TextBlob(text)\n",
        "        polarity = blob.sentiment.polarity\n",
        "        subjectivity = blob.sentiment.subjectivity\n",
        "        analyzer = SentimentIntensityAnalyzer()\n",
        "        vader_scores = analyzer.polarity_scores(text)\n",
        "\n",
        "        if abs(polarity) < 0.1 and subjectivity < 0.3:\n",
        "            sentiment = \"Neutral & Objective\"\n",
        "        elif abs(polarity) < 0.1:\n",
        "            sentiment = \"Neutral but Subjective\"\n",
        "        elif polarity > 0.2:\n",
        "            sentiment = \"Positive Bias Detected\"\n",
        "        elif polarity < -0.2:\n",
        "            sentiment = \"Negative Bias Detected\"\n",
        "        else:\n",
        "            sentiment = \"Slight Bias\"\n",
        "\n",
        "        return {\"Polarity\": round(polarity, 3), \"Subjectivity\": round(subjectivity, 3), \"VADER Compound\": round(vader_scores['compound'], 3), \"Sentiment\": sentiment}\n",
        "    except:\n",
        "        return {\"Polarity\": 0, \"Subjectivity\": 0, \"VADER Compound\": 0, \"Sentiment\": \"Error\"}\n",
        "\n",
        "def get_citation_count_from_text(wikitext):\n",
        "    if not wikitext:\n",
        "        return 0\n",
        "\n",
        "    named_refs = set()\n",
        "    unnamed_count = 0\n",
        "    ref_pattern = r'<ref(?:\\s+[^>]*)?>'\n",
        "    all_refs = re.findall(ref_pattern, wikitext, re.IGNORECASE | re.DOTALL)\n",
        "\n",
        "    for ref in all_refs:\n",
        "        if ref.strip().endswith('/>'):\n",
        "            continue\n",
        "        name_match = re.search(r'name\\s*=\\s*[\"\\']([^\"\\']+)[\"\\']', ref, re.IGNORECASE)\n",
        "        if name_match:\n",
        "            named_refs.add(name_match.group(1))\n",
        "        else:\n",
        "            unnamed_count += 1\n",
        "\n",
        "    ref_count = len(named_refs) + unnamed_count\n",
        "    sfn_count = len(re.findall(r'\\{\\{sfn[a-z]*\\|', wikitext, re.IGNORECASE))\n",
        "    harv_count = len(re.findall(r'\\{\\{harv[a-z]*\\|', wikitext, re.IGNORECASE))\n",
        "    r_count = len(re.findall(r'\\{\\{rp?\\|', wikitext, re.IGNORECASE))\n",
        "    efn_count = len(re.findall(r'\\{\\{efn[a-z]*\\|', wikitext, re.IGNORECASE))\n",
        "    footnote_count = sfn_count + efn_count\n",
        "    return max(ref_count, footnote_count, harv_count, r_count)\n",
        "\n",
        "def get_remaining_data(title):\n",
        "    url = \"https://en.wikipedia.org/w/api.php\"\n",
        "    sections = \"\"\n",
        "    try:\n",
        "        params = {\"action\": \"parse\", \"page\": title, \"prop\": \"sections\", \"redirects\": 1, \"format\": \"json\"}\n",
        "        r = requests.get(url, params=params, headers=HEADERS, timeout=30)\n",
        "        if r.headers.get(\"Content-Type\", \"\").startswith(\"application/json\"):\n",
        "            parse_data = r.json().get(\"parse\", {})\n",
        "            if parse_data:\n",
        "                section_list = parse_data.get(\"sections\", [])\n",
        "                sections = \", \".join(s[\"line\"] for s in section_list)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    talk_page_size = 0\n",
        "    try:\n",
        "        params = {\"action\": \"query\", \"titles\": f\"Talk:{title}\", \"redirects\": True, \"prop\": \"revisions\", \"rvprop\": \"size\", \"rvlimit\": 1, \"format\": \"json\"}\n",
        "        r = requests.get(url, params=params, headers=HEADERS, timeout=30)\n",
        "        if r.headers.get(\"Content-Type\", \"\").startswith(\"application/json\"):\n",
        "            data = r.json()\n",
        "            pages = data.get(\"query\", {}).get(\"pages\", {})\n",
        "            page = next(iter(pages.values()), {})\n",
        "            if int(page.get(\"pageid\", -1)) > 0:\n",
        "                talk_page_size = page.get(\"revisions\", [{}])[0].get(\"size\", 0)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    num_editors, recent_edits = 0, 0\n",
        "    try:\n",
        "        params = {\"action\": \"query\", \"titles\": title, \"redirects\": True, \"prop\": \"revisions\", \"rvprop\": \"timestamp|user\", \"rvlimit\": 500, \"format\": \"json\"}\n",
        "        r = requests.get(url, params=params, headers=HEADERS, timeout=30)\n",
        "        if r.headers.get(\"Content-Type\", \"\").startswith(\"application/json\"):\n",
        "            data = r.json()\n",
        "            pages = data.get(\"query\", {}).get(\"pages\", {})\n",
        "            page = next(iter(pages.values()), {})\n",
        "            revisions = page.get(\"revisions\", [])\n",
        "            unique_editors = set()\n",
        "            one_year_ago = datetime.now(timezone.utc) - timedelta(days=365)\n",
        "\n",
        "            for rev in revisions:\n",
        "                user = rev.get(\"user\", \"\")\n",
        "                if user:\n",
        "                    unique_editors.add(user)\n",
        "                timestamp_str = rev.get(\"timestamp\", \"\")\n",
        "                if timestamp_str:\n",
        "                    timestamp = datetime.strptime(timestamp_str, \"%Y-%m-%dT%H:%M:%SZ\").replace(tzinfo=timezone.utc)\n",
        "                    if timestamp >= one_year_ago:\n",
        "                        recent_edits += 1\n",
        "            num_editors = len(unique_editors)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    views = 0\n",
        "    try:\n",
        "        end = datetime.now(timezone.utc).replace(tzinfo=None)\n",
        "        start = end - timedelta(days=90)\n",
        "        encoded_title = quote(title.replace(' ', '_'))\n",
        "        pv_url = f\"https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia/all-access/user/{encoded_title}/daily/{start:%Y%m%d}/{end:%Y%m%d}\"\n",
        "        r = requests.get(pv_url, headers=HEADERS, timeout=30)\n",
        "        if r.headers.get(\"Content-Type\", \"\").startswith(\"application/json\"):\n",
        "            data = r.json()\n",
        "            views = sum(d[\"views\"] for d in data.get(\"items\", []))\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    return {\"Section Names\": sections, \"Talk Page Size\": talk_page_size, \"Total Editors\": num_editors, \"Edits Last Year\": recent_edits, \"Last 3 Months Views\": views}\n",
        "\n",
        "def get_empty_metrics():\n",
        "    return {\"Days Since Last Edit\": 0, \"Word Count\": 0, \"Section Names\": \"\", \"Citations\": 0, \"Citations Needed\": 0, \"Images\": 0, \"Categories\": 0, \"Total Editors\": 0, \"Edits Last Year\": 0, \"Talk Page Size\": 0, \"Last 3 Months Views\": 0, \"Journal Sources\": 0, \"Book Sources\": 0, \"Web Sources\": 0, \"News Sources\": 0, \"Avg Source Age\": 0, \"Recent Sources (5yr)\": 0, \"Source Quality Score\": 0, \"Hedging Words\": 0, \"Peacock Words\": 0, \"Weasel Words\": 0, \"Value Judgments\": 0, \"Neutrality Score\": 0, \"Flesch-Kincaid Grade\": 0, \"Reading Level\": \"Unknown\", \"Polarity\": 0, \"Subjectivity\": 0, \"VADER Compound\": 0, \"Sentiment\": \"Unknown\", \"Article Class\": \"Unknown\"}\n",
        "\n",
        "def process_single_article(title):\n",
        "    try:\n",
        "        cached_data = get_all_article_data(title)\n",
        "        metrics = analyze_from_cached_data(title, cached_data)\n",
        "        remaining = get_remaining_data(title)\n",
        "        metrics.update(remaining)\n",
        "        metrics[\"Article\"] = title\n",
        "        article_class = get_article_class(title)\n",
        "        metrics[\"Article Class\"] = article_class\n",
        "        time.sleep(REQUEST_DELAY)\n",
        "        return metrics\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {title}: {e}\")\n",
        "        empty = get_empty_metrics()\n",
        "        empty[\"Article\"] = title\n",
        "        return empty\n",
        "\n",
        "def percentile_rank(series):\n",
        "    return series.rank(pct=True) * 100\n",
        "\n",
        "def calculate_scores(df):\n",
        "    pd.set_option('display.width', 1000)\n",
        "    pd.set_option('display.max_columns', None)\n",
        "    pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "    df['Days Since Edit Percentile'] = 100 - percentile_rank(df['Days Since Last Edit'])\n",
        "    df['Edits Last Year Percentile'] = percentile_rank(df['Edits Last Year'])\n",
        "    df['Total Editors Percentile'] = percentile_rank(df['Total Editors'])\n",
        "    df['Talk Page Percentile'] = percentile_rank(df['Talk Page Size'])\n",
        "    df['Views Percentile'] = percentile_rank(df['Last 3 Months Views'])\n",
        "    df['Word Count Percentile'] = percentile_rank(df['Word Count'])\n",
        "    df['Images Percentile'] = percentile_rank(df['Images'])\n",
        "    df['Categories Percentile'] = percentile_rank(df['Categories'])\n",
        "    df['Citations Percentile'] = percentile_rank(df['Citations'])\n",
        "    df['Citation Needed Percentile'] = 100 - percentile_rank(df['Citations Needed'])\n",
        "    df['Citation/Word Ratio Percentile'] = percentile_rank(df['Citations'] / df['Word Count'].replace(0, 1))\n",
        "    df['Source Quality Percentile'] = percentile_rank(df['Source Quality Score'])\n",
        "    df['Journal Sources Percentile'] = percentile_rank(df['Journal Sources'])\n",
        "    df['Book Sources Percentile'] = percentile_rank(df['Book Sources'])\n",
        "    df['Recent Sources Percentile'] = percentile_rank(df['Recent Sources (5yr)'])\n",
        "    df['Source Age Percentile'] = 100 - percentile_rank(df['Avg Source Age'])\n",
        "    df['Neutrality Percentile'] = percentile_rank(df['Neutrality Score'])\n",
        "    df['Hedging Words Percentile'] = 100 - percentile_rank(df['Hedging Words'])\n",
        "    df['Peacock Words Percentile'] = 100 - percentile_rank(df['Peacock Words'])\n",
        "    df['Weasel Words Percentile'] = 100 - percentile_rank(df['Weasel Words'])\n",
        "    df['Value Judgments Percentile'] = 100 - percentile_rank(df['Value Judgments'])\n",
        "    df['Reading Level Percentile'] = 100 - percentile_rank(df['Flesch-Kincaid Grade'])\n",
        "    df['Polarity Neutrality Percentile'] = 100 - percentile_rank(df['Polarity'].abs())\n",
        "    df['Objectivity Percentile'] = 100 - percentile_rank(df['Subjectivity'])\n",
        "\n",
        "    df['Collaboration Score'] = ((df['Total Editors Percentile'] + df['Talk Page Percentile']) / 2).round().astype(int)\n",
        "    df['Aliveness Score'] = ((df['Days Since Edit Percentile'] + df['Edits Last Year Percentile']) / 2).round().astype(int)\n",
        "    df['Popularity Score'] = df['Views Percentile'].round().astype(int)\n",
        "    df['Quality Score'] = (df['Citation/Word Ratio Percentile'] * 0.25 + df['Images Percentile'] * 0.10 + df['Categories Percentile'] * 0.10 + df['Citation Needed Percentile'] * 0.05 + df['Source Quality Percentile'] * 0.25 + df['Neutrality Percentile'] * 0.15 + df['Objectivity Percentile'] * 0.10).round().astype(int)\n",
        "    df['Scholarly Source Score'] = (df['Source Quality Percentile'] * 0.30 + df['Journal Sources Percentile'] * 0.30 + df['Book Sources Percentile'] * 0.20 + df['Recent Sources Percentile'] * 0.10 + df['Source Age Percentile'] * 0.10).round().astype(int)\n",
        "    df['NPOV Score'] = (df['Value Judgments Percentile'] * 0.10 + df['Hedging Words Percentile'] * 0.05 + df['Neutrality Percentile'] * 0.35 + df['Objectivity Percentile'] * 0.30 + df['Peacock Words Percentile'] * 0.10 + df['Weasel Words Percentile'] * 0.10).round().astype(int)\n",
        "    df['Accessibility Score'] = df['Reading Level Percentile'].round().astype(int)\n",
        "\n",
        "    df['Wikipedia Link'] = df['Article'].apply(lambda x: f\"https://en.wikipedia.org/wiki/{x.replace(' ', '_')}\")\n",
        "    df['IIT Library Link'] = df[\"Article\"].str.replace(' ', '%20').apply(lambda x: f\"https://i-share-iit.primo.exlibrisgroup.com/discovery/search?query=any,contains,{x},AND&tab=Everything&search_scope=MyInst_and_CI&sortby=rank&vid=01CARLI_IIT:CARLI_IIT&mfacet=tlevel,include,peer_reviewed,1&lang=en&mode=advanced&offset=0\")\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "articles = search_wikipedia_articles(SEARCH_QUERY, MAX_ARTICLES)\n",
        "\n",
        "if not articles:\n",
        "    print(\"\\nNo articles found.\")\n",
        "else:\n",
        "\n",
        "    rows = []\n",
        "    start_time = time.time()\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
        "        future_to_article = {executor.submit(process_single_article, title): title for title in articles}\n",
        "\n",
        "        for i, future in enumerate(as_completed(future_to_article), 1):\n",
        "            article = future_to_article[future]\n",
        "            try:\n",
        "                result = future.result()\n",
        "                rows.append(result)\n",
        "                print(f\"[{i}/{len(articles)}] Completed: {article}\")\n",
        "\n",
        "                if i % CHECKPOINT_INTERVAL == 0:\n",
        "                    df_checkpoint = pd.DataFrame(rows)\n",
        "                    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "                    checkpoint_file = f\"{SEARCH_QUERY.replace(' ', '_')}_{timestamp}_checkpoint.csv\"\n",
        "                    df_checkpoint.to_csv(checkpoint_file, index=False)\n",
        "                    elapsed = time.time() - start_time\n",
        "                    remaining_time = (elapsed / i) * (len(articles) - i)\n",
        "                    print(f\"Checkpoint saved: {checkpoint_file}\")\n",
        "                    print(f\"Elapsed: {elapsed/60:.1f}min | Estimated remaining: {remaining_time/60:.1f}min\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Failed: {article} - {e}\")\n",
        "\n",
        "    if rows:\n",
        "        df_unordered = pd.DataFrame(rows)\n",
        "\n",
        "        column_order = [\"Article\", \"Article Class\", \"Last 3 Months Views\", \"Word Count\", \"Talk Page Size\", \"Days Since Last Edit\", \"Edits Last Year\", \"Total Editors\", \"Images\", \"Citations\", \"Citations Needed\", \"Journal Sources\", \"Book Sources\", \"Web Sources\", \"News Sources\", \"Avg Source Age\", \"Recent Sources (5yr)\", \"Source Quality Score\", \"Hedging Words\", \"Peacock Words\", \"Weasel Words\", \"Value Judgments\", \"Neutrality Score\", \"Flesch-Kincaid Grade\", \"Reading Level\", \"Polarity\", \"Subjectivity\", \"VADER Compound\", \"Sentiment\", \"Categories\", \"Section Names\"]\n",
        "\n",
        "        df = df_unordered[[col for col in column_order if col in df_unordered.columns]]\n",
        "\n",
        "        df_with_scores = calculate_scores(df.copy())\n",
        "\n",
        "        percentile_cols = [col for col in df_with_scores.columns if 'Percentile' in col]\n",
        "        df_final = df_with_scores.drop(columns=percentile_cols)\n",
        "\n",
        "        final_column_order = [\n",
        "            \"Article\",\n",
        "            \"Article Class\",\n",
        "            \"Last 3 Months Views\",\n",
        "            \"Word Count\",\n",
        "            \"Edits Last Year\",\n",
        "            \"Total Editors\",\n",
        "            \"Citations\",\n",
        "            \"Citations Needed\",\n",
        "            \"Collaboration Score\",\n",
        "            \"Aliveness Score\",\n",
        "            \"Popularity Score\",\n",
        "            \"Quality Score\",\n",
        "            \"Scholarly Source Score\",\n",
        "            \"NPOV Score\",\n",
        "            \"Neutrality Score\",\n",
        "            \"Flesch-Kincaid Grade\",\n",
        "            \"Journal Sources\",\n",
        "            \"Book Sources\",\n",
        "            \"Avg Source Age\",\n",
        "            \"Recent Sources (5yr)\",\n",
        "            \"Web Sources\",\n",
        "            \"News Sources\",\n",
        "            \"Reading Level\",\n",
        "            \"Accessibility Score\",\n",
        "            \"Source Quality Score\",\n",
        "            \"Sentiment\",\n",
        "            \"Polarity\",\n",
        "            \"Subjectivity\",\n",
        "            \"VADER Compound\",\n",
        "            \"Images\",\n",
        "            \"Days Since Last Edit\",\n",
        "            \"Talk Page Size\",\n",
        "            \"Hedging Words\",\n",
        "            \"Peacock Words\",\n",
        "            \"Weasel Words\",\n",
        "            \"Value Judgments\",\n",
        "            \"Categories\",\n",
        "            \"Section Names\",\n",
        "            \"Wikipedia Link\",\n",
        "            \"IIT Library Link\",\n",
        "        ]\n",
        "\n",
        "        df_final = df_final[[col for col in final_column_order if col in df_final.columns]]\n",
        "\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        filename = f\"{SEARCH_QUERY.replace(' ', '_')}_{MAX_ARTICLES}_WITH_SCORES.csv\"\n",
        "        df_final.to_csv(filename, index=False)\n",
        "\n",
        "        elapsed_total = time.time() - start_time\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Successfully saved {len(df_final)} articles to: {filename}\")\n",
        "        print(f\"Total time: {elapsed_total/60:.1f} minutes ({elapsed_total/len(df_final):.2f}s per article)\")\n",
        "        print(f\"{'='*60}\\n\")\n",
        "\n",
        "        from itables import init_notebook_mode, show\n",
        "        init_notebook_mode(all_interactive=True)\n",
        "\n",
        "        print(\"=\"*60)\n",
        "        print(\"COMPOSITE SCORES CALCULATED\")\n",
        "        print(\"=\"*60)\n",
        "        print(\"\\nScore Definitions:\")\n",
        "        print(\"- Collaboration: Average of (Total Editors + Talk Page Size)\")\n",
        "        print(\"- Aliveness: Average of (Days Since Edit [inverted] + Edits Last Year)\")\n",
        "        print(\"- Popularity: Page Views\")\n",
        "        print(\"- Quality: Weighted average of content metrics, sources, and neutrality\")\n",
        "        print(\"- Scholarly Source: Quality and recency of academic sources\")\n",
        "        print(\"- NPOV Score: Neutral Point of View compliance\")\n",
        "        print(\"- Accessibility: Reading level\")\n",
        "        print(\"\\nAll scores are on a 0-100 percentile scale.\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        print(\"\\nInteractive Data Table\")\n",
        "        print(\"=\"*60)\n",
        "        print(\"How to use:\")\n",
        "        print(\"SEARCH: Use the search box to filter across all columns\")\n",
        "        print(\"SORT: Click column headers to sort (shift+click for multi-column)\")\n",
        "        print(\"COLUMNS: Click 'Column visibility' button to show/hide columns\")\n",
        "        print(\"PAGES: Use dropdown to change rows per page (10/25/50/100)\")\n",
        "        print(\"EXPORT: Click 'CSV' or 'Excel' to download\")\n",
        "        print(\"=\"*60)\n",
        "        print()\n",
        "\n",
        "        df_print = df_final[[\"Article\",\n",
        "            \"Article Class\",\n",
        "            \"Last 3 Months Views\",\n",
        "            \"Word Count\",\n",
        "            \"Edits Last Year\",\n",
        "            \"Citations\",\n",
        "            \"Recent Sources (5yr)\",\n",
        "            \"Reading Level\",\n",
        "            \"Sentiment\",\n",
        "            \"Collaboration Score\",\n",
        "            \"Aliveness Score\",\n",
        "            \"Popularity Score\",\n",
        "            \"Quality Score\",\n",
        "            \"Scholarly Source Score\",\n",
        "            \"NPOV Score\",\n",
        "            \"Wikipedia Link\",\n",
        "            \"IIT Library Link\"]].copy()\n",
        "\n",
        "        if DETAILED_TABLE == True:\n",
        "          display(df_final if DETAILED_DATA == True else df_print)\n",
        "        else:\n",
        "          show(df_final if DETAILED_DATA == True else df_print, scrollX=True, scrollY=\"600px\", paging=True, lengthMenu=[10, 25, 50, 100], pageLength=25, buttons=['copy', 'csv', 'excel', 'colvis'], order=[[4, 'desc']], columnDefs=[{\"className\": \"dt-left\", \"targets\": \"_all\"}])\n",
        "\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(\"Summary Statistics\")\n",
        "        print(f\"{'='*60}\")\n",
        "        numeric_cols = ['Last 3 Months Views', 'Word Count', 'Talk Page Size', 'Citations', 'Days Since Last Edit', 'Source Quality Score', 'Neutrality Score', 'Flesch-Kincaid Grade', 'Polarity', 'Subjectivity', 'Collaboration Score', 'Aliveness Score', 'Popularity Score', 'Quality Score', 'Scholarly Source Score', 'NPOV Score', 'Accessibility Score']\n",
        "        available_cols = [col for col in numeric_cols if col in df_final.columns]\n",
        "        print(df_final[available_cols].describe().round(1))\n",
        "\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(\"Article Class Distribution\")\n",
        "        print(f\"{'='*60}\")\n",
        "        print(df_final['Article Class'].value_counts())\n",
        "\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(\"TOP 5 ARTICLES BY EACH SCORE:\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        print(\"\\nMost Collaborative:\")\n",
        "        print(df_final.nlargest(5, 'Collaboration Score')[['Article', 'Collaboration Score', 'Total Editors', 'Talk Page Size']])\n",
        "\n",
        "        print(\"\\nMost Alive:\")\n",
        "        print(df_final.nlargest(5, 'Aliveness Score')[['Article', 'Aliveness Score', 'Days Since Last Edit', 'Edits Last Year']])\n",
        "\n",
        "        print(\"\\nMost Popular:\")\n",
        "        print(df_final.nlargest(5, 'Popularity Score')[['Article', 'Popularity Score', 'Last 3 Months Views']])\n",
        "\n",
        "        print(\"\\nHighest Quality:\")\n",
        "        print(df_final.nlargest(5, 'Quality Score')[['Article', 'Quality Score', 'Citations', 'Source Quality Score', 'Neutrality Score']])\n",
        "\n",
        "        print(\"\\nBest Scholarly Sources:\")\n",
        "        print(df_final.nlargest(5, 'Scholarly Source Score')[['Article', 'Scholarly Source Score', 'Journal Sources', 'Book Sources']])\n",
        "\n",
        "        print(\"\\nMost Neutral (NPOV):\")\n",
        "        print(df_final.nlargest(5, 'NPOV Score')[['Article', 'NPOV Score', 'Neutrality Score']])\n",
        "\n",
        "        print(\"\\nMost Accessible:\")\n",
        "        print(df_final.nlargest(5, 'Accessibility Score')[['Article', 'Accessibility Score', 'Reading Level', 'Flesch-Kincaid Grade']])\n",
        "\n",
        "    else:\n",
        "        print(\"\\nNo data to save\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2NccjllsJjej",
        "outputId": "df940ab5-4341-4bbb-8a76-cb61ab4c89c4"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1/30] Completed: Surfing (disambiguation)\n",
            "[2/30] Completed: Surf\n",
            "[3/30] Completed: Couch surfing\n",
            "[4/30] Completed: Surfing\n",
            "[5/30] Completed: Surf music\n",
            "[6/30] Completed: Surf culture\n",
            "[7/30] Completed: Surfing at the Summer Olympics\n",
            "[8/30] Completed: Powder surfing\n",
            "[9/30] Completed: World Wide Web\n",
            "[10/30] Completed: History of surfing\n",
            "[11/30] Completed: Big wave surfing\n",
            "[12/30] Completed: Car surfing\n",
            "[13/30] Completed: Train surfing\n",
            "[14/30] Completed: Snowboarding\n",
            "[15/30] Completed: Surfing with the Alien\n",
            "[16/30] Completed: Tanker surfing\n",
            "[17/30] Completed: Crowd surfing\n",
            "[18/30] Completed: Glossary of surfing\n",
            "[19/30] Completed: Elevator surfing\n",
            "[20/30] Completed: List of train-surfing injuries and deaths\n",
            "[21/30] Completed: Surfing in Madeira\n",
            "[22/30] Completed: Shoulder surfing\n",
            "[23/30] Completed: Shoulder surfing (computer security)\n",
            "[24/30] Completed: Surfing on a Rocket\n",
            "[25/30] Completed: Surfing on Sine Waves\n",
            "[26/30] Completed: World Surf League\n",
            "[27/30] Completed: Bethany Hamilton\n",
            "[28/30] Completed: Tia Blanco\n",
            "[29/30] Completed: Para surfing\n",
            "[30/30] Completed: Trestles (surfing)\n",
            "\n",
            "============================================================\n",
            "Successfully saved 30 articles to: surfing_30_WITH_SCORES.csv\n",
            "Total time: 0.2 minutes (0.42s per article)\n",
            "============================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<script type=\"module\">\n",
              "    import { set_or_remove_dark_class } from 'https://www.unpkg.com/dt_for_itables/dt_bundle.js';\n",
              "    set_or_remove_dark_class();\n",
              "</script>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "COMPOSITE SCORES CALCULATED\n",
            "============================================================\n",
            "\n",
            "Score Definitions:\n",
            "- Collaboration: Average of (Total Editors + Talk Page Size)\n",
            "- Aliveness: Average of (Days Since Edit [inverted] + Edits Last Year)\n",
            "- Popularity: Page Views\n",
            "- Quality: Weighted average of content metrics, sources, and neutrality\n",
            "- Scholarly Source: Quality and recency of academic sources\n",
            "- NPOV Score: Neutral Point of View compliance\n",
            "- Accessibility: Reading level\n",
            "\n",
            "All scores are on a 0-100 percentile scale.\n",
            "============================================================\n",
            "\n",
            "Interactive Data Table\n",
            "============================================================\n",
            "How to use:\n",
            "SEARCH: Use the search box to filter across all columns\n",
            "SORT: Click column headers to sort (shift+click for multi-column)\n",
            "COLUMNS: Click 'Column visibility' button to show/hide columns\n",
            "PAGES: Use dropdown to change rows per page (10/25/50/100)\n",
            "EXPORT: Click 'CSV' or 'Excel' to download\n",
            "============================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                      Article Article Class  Last 3 Months Views  Word Count  Edits Last Year  Total Editors  Citations  Citations Needed  Collaboration Score  Aliveness Score  Popularity Score  Quality Score  Scholarly Source Score  NPOV Score  Neutrality Score  Flesch-Kincaid Grade  Journal Sources  Book Sources  Avg Source Age  Recent Sources (5yr)  Web Sources  News Sources          Reading Level  Accessibility Score  Source Quality Score               Sentiment  Polarity  Subjectivity  VADER Compound  Images  Days Since Last Edit  Talk Page Size  Hedging Words  Peacock Words  Weasel Words  Value Judgments  Categories  \\\n",
              "0                    Surfing (disambiguation)    Unassessed                  246         193                4             39          0                 0                   12               28                10             30                      31          65             100.0                  28.3                0             0             0.0                     0            0             0  Professional/Academic                    3                  25.0  Neutral but Subjective     0.017         0.361          -0.153       2                   127              61              1              0             0                0           4   \n",
              "1                                        Surf    Unassessed                 1549         342                5            157          0                 0                   49               30                30             31                      31          67             100.0                  68.4                0             0             0.0                     0            0             0  Professional/Academic                    0                  25.0  Neutral but Subjective     0.047         0.335           0.946       2                   142            1806              1              0             0                0           5   \n",
              "2                               Couch surfing             C                 3546         600                4             42         10                 0                   25               25                37             57                      75          51             100.0                  11.6                6             0             7.6                     5            3             1       College Graduate                   62                  85.0  Neutral but Subjective     0.071         0.455          -0.942       0                   141             692              3              0             0                0           8   \n",
              "3                                     Surfing             B                44032        8725               72            279         82                 4                   77               89                90             51                      88          18              84.0                  10.7               12            15            13.6                    20           42             8                College                   73                  72.5             Slight Bias     0.107         0.456           1.000      34                     5            5967             15             11             2                0          17   \n",
              "4                                  Surf music         START                23035        2913               23            216         45                 2                   87               53                83             56                      76          42              96.6                  10.6                3            33            24.6                     3           16             2                College                   77                  73.3             Slight Bias     0.142         0.420           0.999       6                    82           32287              2              1             0                0          19   \n",
              "5                                Surf culture             B                 9336        7399               36            158         96                 3                   69               78                70             53                      77          23              87.4                  12.2                5            51            20.7                     9           46            12       College Graduate                   37                  64.6             Slight Bias     0.106         0.415           1.000      24                     9            6911             12              7             1                1          20   \n",
              "6              Surfing at the Summer Olympics    Unassessed                 5153         279                4             30          5                 0                   12               61                53             54                      36          67             100.0                  14.5                0             0             8.0                     2            5             0       College Graduate                   10                  35.0  Neutral but Subjective     0.070         0.352           0.802      10                     3             220              0              0             0                0           8   \n",
              "7                              Powder surfing          STUB                  297         345                1             35          4                 0                   18               10                13             32                      36          49             100.0                  12.1                0             0             0.0                     0            4             0                College                   40                  35.0             Slight Bias     0.176         0.608           0.994       0                   346             402              0              0             0                0           1   \n",
              "8                              World Wide Web             C               364836        7184              208            310        143                 2                   77               96               100             67                      87          41              97.5                  12.7               13            16            14.3                    42           82            25       College Graduate                   25                  65.7  Neutral but Subjective     0.054         0.383           1.000      21                     3            3528             47              1             0                1          37   \n",
              "9                          History of surfing             C                 8019        5175               24            259         39                 1                   83               62                63             43                      67          17              77.4                  12.3                3             9            48.4                     3           12             2       College Graduate                   33                  63.1             Slight Bias     0.139         0.427           1.000       8                    61           12773              8              7             1                4          13   \n",
              "10                           Big wave surfing         START                 8411        1453               19            273         61                 1                   82               64                67             56                      44          39              51.8                  11.1                0             0             8.7                    13           52             6                College                   70                  45.0             Slight Bias     0.155         0.305           0.997       8                    34            7939              3              7             0                0          13   \n",
              "11                                Car surfing         START                 1908         238                4            158          2                 0                   42               20                33             38                      27          75             100.0                  11.8                0             0            12.0                     0            1             0       College Graduate                   48                  25.0     Neutral & Objective     0.039         0.250          -0.788       2                   216             604              0              0             0                0           7   \n",
              "12                              Train surfing             C                 9492        2591               59            239         44                 3                   83               65                73             55                      59          46              96.1                  11.6                5             0            12.4                     6            4            34                College                   62                  50.8  Neutral but Subjective     0.034         0.369          -0.999      10                    78           13019              4              1             0                0          23   \n",
              "13                               Snowboarding             C                90498        4765               39            315         78                 0                   77               82                93             63                      83          33              89.9                  11.8               10             7            13.0                    13           49             3       College Graduate                   48                  67.3             Slight Bias     0.115         0.387           0.999      28                     7            3017              6              4             0                1          24   \n",
              "14                     Surfing with the Alien             C                10518        1401                4            190         30                 0                   63               27                77             54                      46          25              71.4                  13.3                0             2            18.6                     0            6             4       College Graduate                   17                  53.3             Slight Bias     0.117         0.437           0.998       6                   133            5092              2              4             0                0          28   \n",
              "15                             Tanker surfing         START                   60         213                1             12          4                 1                    5               33                 3             35                      27          50             100.0                  10.1                0             0            10.6                     0            0             4  High School (10-12th)                   87                  25.0  Positive Bias Detected     0.275         0.581           0.848       1                    58             185              0              0             0                0           5   \n",
              "16                              Crowd surfing    Unassessed                 4136         324                8            292          8                 0                   88               52                47             56                      36          74             100.0                  12.0                0             1            25.4                     0            2             1                College                   43                  42.5     Neutral & Objective     0.010         0.261           0.847       3                    51           11512              0              0             0                0           5   \n",
              "17                        Glossary of surfing          LIST                 3824        2132                4            112         16                 0                   45               38                40             40                      50          32              74.2                  21.7                0             6            19.9                     6           20             0       College Graduate                    7                  46.5  Neutral but Subjective     0.081         0.365           0.991       9                    63            2384              1              4             1                0          10   \n",
              "18                           Elevator surfing    Unassessed                 1548         820               15            157         21                 0                   44               50                27             45                      40          48             100.0                  13.4                1             0            22.1                     6            7            12       College Graduate                   13                  32.5  Neutral but Subjective     0.028         0.458          -0.990       2                    64            1591              5              0             0                0           6   \n",
              "19  List of train-surfing injuries and deaths          LIST                 5106          30              133             75        685                 0                   42               83                50             79                      70          78             100.0                   9.6                6             0             7.1                   268          206           468       College Graduate                   93                  55.4     Neutral & Objective     0.000         0.000           0.000       6                    23            2002              0              0             0                0          20   \n",
              "20                         Surfing in Madeira         START                  354         115                0             45          1                 0                   33                4                17             38                      31          69             100.0                  12.5                0             0             0.0                     0            0             0       College Graduate                   30                  25.0             Slight Bias     0.147         0.327           0.807       2                   552            1774              0              0             0                0           9   \n",
              "21                           Shoulder surfing    Unassessed                  107          12                0             26          0                 0                   13                2                 7             32                      31          77             100.0                   0.0                0             0             0.0                     0            0             0                Unknown                   97                  25.0     Neutral & Objective     0.000         0.000           0.340       1                  2241             401              1              0             0                0           4   \n",
              "22       Shoulder surfing (computer security)             C                 5886         553               17            149         17                 0                   61               72                57             63                      72          52             100.0                  12.8                3             3            13.0                     2            5             0       College Graduate                   20                  72.3  Neutral but Subjective     0.005         0.437          -0.797       0                     5            6673             10              0             0                0           6   \n",
              "23                        Surfing on a Rocket          STUB                  664         461                1             63         20                 0                   33               23                23             58                      26          76             100.0                  10.2                0             1            22.0                     0           11             0                College                   83                  24.2     Neutral & Objective     0.009         0.224           0.983       4                    87            1771              0              0             0                0          15   \n",
              "24                      Surfing on Sine Waves         START                 7588         472               19            149         21                 0                   59               48                60             63                      64          22              36.4                  10.5                1             2            16.8                     6           15             1                College                   80                  62.9  Positive Bias Detected     0.268         0.453           0.984       6                    92            6251              2              3             0                0          26   \n",
              "25                          World Surf League         START                12265        2486               35            191         39                 1                   67               75                80             47                      42          33              79.9                  11.7                0             0             9.1                     8           31             4                College                   55                  45.0             Slight Bias     0.159         0.392           1.000      24                    12            5487              5              5             0                0          20   \n",
              "26                           Bethany Hamilton             C                95605        1715               57            283         48                 2                   92               92                97             60                      45          44              76.7                  11.5                0             0             8.2                    17           42             4       College Graduate                   67                  45.0             Slight Bias     0.135         0.318           0.999      14                     2           15983              0              4             0                0          38   \n",
              "27                                 Tia Blanco         START                30851         454               12             74         15                 0                   22               57                87             64                      44          69             100.0                  10.0                0             0             7.2                    10           13             2                College                   90                  45.0             Slight Bias     0.126         0.313           0.998       4                    42             220              2              0             0                0          10   \n",
              "28                               Para surfing             C                  520        1022                4             21         27                 0                   18               48                20             56                      63          56              90.2                  12.7                1             0             5.1                    15           11            15       College Graduate                   25                  56.9  Neutral but Subjective     0.082         0.302           0.998       2                    33            1116              0              1             0                0           4   \n",
              "29                         Trestles (surfing)         START                 4091        1317               11            146         57                 1                   70               32                43             56                      37          45              92.4                  11.7                0             0            15.1                    15           38            14                College                   55                  35.0             Slight Bias     0.130         0.378           0.990       5                   147           21287              2              1             0                0          22   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Section Names                                                           Wikipedia Link  \\\n",
              "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Activities, Music, Other uses, See also                   https://en.wikipedia.org/wiki/Surfing_(disambiguation)   \n",
              "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Commercial products, Computers and software, Education, Music, Places, Popular culture, Ships, Sports, Other uses, See also                                       https://en.wikipedia.org/wiki/Surf   \n",
              "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Couch surfing in travel, Couch surfing as homelessness, See also, References                              https://en.wikipedia.org/wiki/Couch_surfing   \n",
              "3                                                                                                   Origins and history, Peru, Polynesia, West Africa, California, Surf waves, Tube shape and speed, Wave intensity, Artificial reefs, Artificial waves, Maneuvers, Terms, Learning, Equipment, The physics of surfing, Wave formation, Wave conditions for surfing, Surf breaks, Headland (point break), Beach break, River or estuary entrance bar, Reef break, Ledge break, Rip currents, On the surfboard, Dangers, Drowning, Collisions, Marine life, Rip currents, Seabed, Microorganisms, Ear damage, Surf rash, Spinal cord, Surfers and surf culture, See also, References, Further reading, External links                                    https://en.wikipedia.org/wiki/Surfing   \n",
              "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Instrumental surf, Form, History, Vocal surf, Distinctions, Hot rod rock, Popularity, Decline, Influence and revival, Surf punk, Indie surf, Production, See also, Notes, References, Bibliography, Further reading                                 https://en.wikipedia.org/wiki/Surf_music   \n",
              "5   History, Spirituality, Women in surfing, Beach bunnies, Diversity, Big wave culture, Localism, Surf Nazi, Surf gangs, Wolfpak, Bra Boys, Surf terminology, Issues affecting surfers, Surfing and environmentalism, Surf tourism, Surfing art, Surf visual art, Surf graphics, Surf music, Fashion, Surfwear, Bikini, Events, Surfing contests, Surfing organizations, Spin-offs &amp; influences, Boardsports, Surfing in multimedia, Films about surfing, TV documentary series about surfing, Print media, Surfing magazines, Surfing in fiction, Surfing in non-fiction, Conceptual metaphor, Popular, Natural science, Philosophical, Graphic art, See also, Notes, References, Bibliography, External links                               https://en.wikipedia.org/wiki/Surf_culture   \n",
              "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Bid for inclusion, Events, Venues, Medalists, Men's shortboard, Women's shortboard, Medal table, References, External links             https://en.wikipedia.org/wiki/Surfing_at_the_Summer_Olympics   \n",
              "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         References                             https://en.wikipedia.org/wiki/Powder_surfing   \n",
              "8                                                                                                                                                                                                                                                                                                                                                        History, Competition with Gopher, Nomenclature, Function, HTML, Linking, www prefix, Scheme specifiers, Pages, Static page, Dynamic pages, Website, Browser, Server, Optical Networking, Cookie, Search engine, Deep web, Caching, Security, Privacy, Standards, Accessibility, Internationalisation, See also, References, Further reading, External links                             https://en.wikipedia.org/wiki/World_Wide_Web   \n",
              "9                                                                                                                                                                                                                                                                                                Peru, West Africa, Polynesia, Ancient Hawaii, Post-contact Hawaii, North America, Australian surfing, Great Britain, Modern surfing, Professional surfing, Technological innovations in surfing, The Short Board Revolution, Big wave surfing, Style versus performance, Surfing's impact on popular culture, Alternatives to wind-generated waves, References, Citations, Sources, External links, Surfing museums                         https://en.wikipedia.org/wiki/History_of_surfing   \n",
              "10                                                                                                                                                                                                                                                                                                        Hazards of big wave surfing, Paddle-in surfing, Big wave surfing world records, Male, Female, Big Wave Surfing Contest, Big Wave Surfing Awards, WSL Big Wave Championship Tour champions, Notable big wave surfing spots, Australia, United States (Mainland), Oceania, Europe, Latin America, Caribbean, Africa, Notable big wave surfers, Big wave surfing movies, See also, References, External links                           https://en.wikipedia.org/wiki/Big_wave_surfing   \n",
              "11                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              History, Risks, See also, References, External links                                https://en.wikipedia.org/wiki/Car_surfing   \n",
              "12                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Overcrowding, Travelling without a ticket, For entertainment, Hazards, Injuries and deaths, Prevention and punishments, Deterrents, See also, References, Notes, Bibliography, External links                              https://en.wikipedia.org/wiki/Train_surfing   \n",
              "13                                                                                                                                                                                                                                                                                                                                                                History, Styles, Jibbing, Freeriding, Freestyle, Alpine snowboarding, Slopestyle, Big air, Half-pipe, Snowboard cross, Snowboard racing, Competitions, Subculture, Common injuries, Safety and precautions, Terminology, Stances, Parts of a snowboard, Rotations, Flips, Off-axis rotations, Notable people, See also, References, External links                               https://en.wikipedia.org/wiki/Snowboarding   \n",
              "14                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Recording, Composition, Artwork, Release and reception, Reissues, Track listing, 2007 remastered edition bonus DVD, Personnel, Charts, Weekly charts, Year-end charts, Certifications, References, External links                     https://en.wikipedia.org/wiki/Surfing_with_the_Alien   \n",
              "15                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        References                             https://en.wikipedia.org/wiki/Tanker_surfing   \n",
              "16                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Origins, See also, References, External links                              https://en.wikipedia.org/wiki/Crowd_surfing   \n",
              "17                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         About the water, Techniques and maneuvers, Accidental, About people and behavior, About the board, Clothing, Further reading, See also, Notes, References, External links                        https://en.wikipedia.org/wiki/Glossary_of_surfing   \n",
              "18                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Origins, Hazards, Injuries and deaths, See also, References                           https://en.wikipedia.org/wiki/Elevator_surfing   \n",
              "19                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Data of train-surfing injuries and deaths, Train-surfing injuries and deaths, See also, References  https://en.wikipedia.org/wiki/List_of_train-surfing_injuries_and_deaths   \n",
              "20                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        References, External links                         https://en.wikipedia.org/wiki/Surfing_in_Madeira   \n",
              "21                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             https://en.wikipedia.org/wiki/Shoulder_surfing   \n",
              "22                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Overview, Countermeasures, Graphical passwords, PIN entry, Biometrics, Eye tracking, Virtual reality, See also, References       https://en.wikipedia.org/wiki/Shoulder_surfing_(computer_security)   \n",
              "23                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Music video, Track listings, Charts, Release history, References                        https://en.wikipedia.org/wiki/Surfing_on_a_Rocket   \n",
              "24                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Background, Reissues, Reception, Track listing, Personnel, Charts, References, External links                      https://en.wikipedia.org/wiki/Surfing_on_Sine_Waves   \n",
              "25                                                                                                                                                                                             History, Predecessors, Equal pay for athletes in 2019, COVID-19 impact, WSL sanctioned tours, WSL Championship tour, WSL Qualifying Series events, WSL world ranking, Promotion and relegation, 2012 tours, 2013-2018 tours, Rules, Judging, Rules, WSL Championship Tour champions, Surfers with the most World Tour wins (Men), WSL Longboard Championship Tour champions, WSL World Junior champions, WSL Big Wave Tour champions, Men's Triple Crown Champions, Top Nations, See also, References, External links                          https://en.wikipedia.org/wiki/World_Surf_League   \n",
              "26                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Early life, Shark attack and recovery, Media, Personal life, Books, Outreach programs, Surfing career, World Surf League boycott, Filmography, Film, Television, References, External links                           https://en.wikipedia.org/wiki/Bethany_Hamilton   \n",
              "27                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Early life, Professional career, Personal life, Filmography, References, External links                                 https://en.wikipedia.org/wiki/Tia_Blanco   \n",
              "28                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         History, Classification, Events, ISA, AASP, PSL, Other, Paralympics, See also, References                               https://en.wikipedia.org/wiki/Para_surfing   \n",
              "29                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Access and amenities, Plant and animal life, Seasonal stream, Toll road controversy, In popular culture, References, External links                         https://en.wikipedia.org/wiki/Trestles_(surfing)   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                IIT Library Link  \n",
              "0                            https://i-share-iit.primo.exlibrisgroup.com/discovery/search?query=any,contains,Surfing%20(disambiguation),AND&tab=Everything&search_scope=MyInst_and_CI&sortby=rank&vid=01CARLI_IIT:CARLI_IIT&mfacet=tlevel,include,peer_reviewed,1&lang=en&mode=advanced&offset=0  \n",
              "1                                                  https://i-share-iit.primo.exlibrisgroup.com/discovery/search?query=any,contains,Surf,AND&tab=Everything&search_scope=MyInst_and_CI&sortby=rank&vid=01CARLI_IIT:CARLI_IIT&mfacet=tlevel,include,peer_reviewed,1&lang=en&mode=advanced&offset=0  \n",
              "2                                       https://i-share-iit.primo.exlibrisgroup.com/discovery/search?query=any,contains,Couch%20surfing,AND&tab=Everything&search_scope=MyInst_and_CI&sortby=rank&vid=01CARLI_IIT:CARLI_IIT&mfacet=tlevel,include,peer_reviewed,1&lang=en&mode=advanced&offset=0  \n",
              "3                                               https://i-share-iit.primo.exlibrisgroup.com/discovery/search?query=any,contains,Surfing,AND&tab=Everything&search_scope=MyInst_and_CI&sortby=rank&vid=01CARLI_IIT:CARLI_IIT&mfacet=tlevel,include,peer_reviewed,1&lang=en&mode=advanced&offset=0  \n",
              "4                                          https://i-share-iit.primo.exlibrisgroup.com/discovery/search?query=any,contains,Surf%20music,AND&tab=Everything&search_scope=MyInst_and_CI&sortby=rank&vid=01CARLI_IIT:CARLI_IIT&mfacet=tlevel,include,peer_reviewed,1&lang=en&mode=advanced&offset=0  \n",
              "5                                        https://i-share-iit.primo.exlibrisgroup.com/discovery/search?query=any,contains,Surf%20culture,AND&tab=Everything&search_scope=MyInst_and_CI&sortby=rank&vid=01CARLI_IIT:CARLI_IIT&mfacet=tlevel,include,peer_reviewed,1&lang=en&mode=advanced&offset=0  \n",
              "6                https://i-share-iit.primo.exlibrisgroup.com/discovery/search?query=any,contains,Surfing%20at%20the%20Summer%20Olympics,AND&tab=Everything&search_scope=MyInst_and_CI&sortby=rank&vid=01CARLI_IIT:CARLI_IIT&mfacet=tlevel,include,peer_reviewed,1&lang=en&mode=advanced&offset=0  \n",
              "7                                      https://i-share-iit.primo.exlibrisgroup.com/discovery/search?query=any,contains,Powder%20surfing,AND&tab=Everything&search_scope=MyInst_and_CI&sortby=rank&vid=01CARLI_IIT:CARLI_IIT&mfacet=tlevel,include,peer_reviewed,1&lang=en&mode=advanced&offset=0  \n",
              "8                                    https://i-share-iit.primo.exlibrisgroup.com/discovery/search?query=any,contains,World%20Wide%20Web,AND&tab=Everything&search_scope=MyInst_and_CI&sortby=rank&vid=01CARLI_IIT:CARLI_IIT&mfacet=tlevel,include,peer_reviewed,1&lang=en&mode=advanced&offset=0  \n",
              "9                                https://i-share-iit.primo.exlibrisgroup.com/discovery/search?query=any,contains,History%20of%20surfing,AND&tab=Everything&search_scope=MyInst_and_CI&sortby=rank&vid=01CARLI_IIT:CARLI_IIT&mfacet=tlevel,include,peer_reviewed,1&lang=en&mode=advanced&offset=0  \n",
              "10                                 https://i-share-iit.primo.exlibrisgroup.com/discovery/search?query=any,contains,Big%20wave%20surfing,AND&tab=Everything&search_scope=MyInst_and_CI&sortby=rank&vid=01CARLI_IIT:CARLI_IIT&mfacet=tlevel,include,peer_reviewed,1&lang=en&mode=advanced&offset=0  \n",
              "11                                        https://i-share-iit.primo.exlibrisgroup.com/discovery/search?query=any,contains,Car%20surfing,AND&tab=Everything&search_scope=MyInst_and_CI&sortby=rank&vid=01CARLI_IIT:CARLI_IIT&mfacet=tlevel,include,peer_reviewed,1&lang=en&mode=advanced&offset=0  \n",
              "12                                      https://i-share-iit.primo.exlibrisgroup.com/discovery/search?query=any,contains,Train%20surfing,AND&tab=Everything&search_scope=MyInst_and_CI&sortby=rank&vid=01CARLI_IIT:CARLI_IIT&mfacet=tlevel,include,peer_reviewed,1&lang=en&mode=advanced&offset=0  \n",
              "13                                         https://i-share-iit.primo.exlibrisgroup.com/discovery/search?query=any,contains,Snowboarding,AND&tab=Everything&search_scope=MyInst_and_CI&sortby=rank&vid=01CARLI_IIT:CARLI_IIT&mfacet=tlevel,include,peer_reviewed,1&lang=en&mode=advanced&offset=0  \n",
              "14                         https://i-share-iit.primo.exlibrisgroup.com/discovery/search?query=any,contains,Surfing%20with%20the%20Alien,AND&tab=Everything&search_scope=MyInst_and_CI&sortby=rank&vid=01CARLI_IIT:CARLI_IIT&mfacet=tlevel,include,peer_reviewed,1&lang=en&mode=advanced&offset=0  \n",
              "15                                     https://i-share-iit.primo.exlibrisgroup.com/discovery/search?query=any,contains,Tanker%20surfing,AND&tab=Everything&search_scope=MyInst_and_CI&sortby=rank&vid=01CARLI_IIT:CARLI_IIT&mfacet=tlevel,include,peer_reviewed,1&lang=en&mode=advanced&offset=0  \n",
              "16                                      https://i-share-iit.primo.exlibrisgroup.com/discovery/search?query=any,contains,Crowd%20surfing,AND&tab=Everything&search_scope=MyInst_and_CI&sortby=rank&vid=01CARLI_IIT:CARLI_IIT&mfacet=tlevel,include,peer_reviewed,1&lang=en&mode=advanced&offset=0  \n",
              "17                              https://i-share-iit.primo.exlibrisgroup.com/discovery/search?query=any,contains,Glossary%20of%20surfing,AND&tab=Everything&search_scope=MyInst_and_CI&sortby=rank&vid=01CARLI_IIT:CARLI_IIT&mfacet=tlevel,include,peer_reviewed,1&lang=en&mode=advanced&offset=0  \n",
              "18                                   https://i-share-iit.primo.exlibrisgroup.com/discovery/search?query=any,contains,Elevator%20surfing,AND&tab=Everything&search_scope=MyInst_and_CI&sortby=rank&vid=01CARLI_IIT:CARLI_IIT&mfacet=tlevel,include,peer_reviewed,1&lang=en&mode=advanced&offset=0  \n",
              "19  https://i-share-iit.primo.exlibrisgroup.com/discovery/search?query=any,contains,List%20of%20train-surfing%20injuries%20and%20deaths,AND&tab=Everything&search_scope=MyInst_and_CI&sortby=rank&vid=01CARLI_IIT:CARLI_IIT&mfacet=tlevel,include,peer_reviewed,1&lang=en&mode=advanced&offset=0  \n",
              "20                               https://i-share-iit.primo.exlibrisgroup.com/discovery/search?query=any,contains,Surfing%20in%20Madeira,AND&tab=Everything&search_scope=MyInst_and_CI&sortby=rank&vid=01CARLI_IIT:CARLI_IIT&mfacet=tlevel,include,peer_reviewed,1&lang=en&mode=advanced&offset=0  \n",
              "21                                   https://i-share-iit.primo.exlibrisgroup.com/discovery/search?query=any,contains,Shoulder%20surfing,AND&tab=Everything&search_scope=MyInst_and_CI&sortby=rank&vid=01CARLI_IIT:CARLI_IIT&mfacet=tlevel,include,peer_reviewed,1&lang=en&mode=advanced&offset=0  \n",
              "22           https://i-share-iit.primo.exlibrisgroup.com/discovery/search?query=any,contains,Shoulder%20surfing%20(computer%20security),AND&tab=Everything&search_scope=MyInst_and_CI&sortby=rank&vid=01CARLI_IIT:CARLI_IIT&mfacet=tlevel,include,peer_reviewed,1&lang=en&mode=advanced&offset=0  \n",
              "23                            https://i-share-iit.primo.exlibrisgroup.com/discovery/search?query=any,contains,Surfing%20on%20a%20Rocket,AND&tab=Everything&search_scope=MyInst_and_CI&sortby=rank&vid=01CARLI_IIT:CARLI_IIT&mfacet=tlevel,include,peer_reviewed,1&lang=en&mode=advanced&offset=0  \n",
              "24                          https://i-share-iit.primo.exlibrisgroup.com/discovery/search?query=any,contains,Surfing%20on%20Sine%20Waves,AND&tab=Everything&search_scope=MyInst_and_CI&sortby=rank&vid=01CARLI_IIT:CARLI_IIT&mfacet=tlevel,include,peer_reviewed,1&lang=en&mode=advanced&offset=0  \n",
              "25                                https://i-share-iit.primo.exlibrisgroup.com/discovery/search?query=any,contains,World%20Surf%20League,AND&tab=Everything&search_scope=MyInst_and_CI&sortby=rank&vid=01CARLI_IIT:CARLI_IIT&mfacet=tlevel,include,peer_reviewed,1&lang=en&mode=advanced&offset=0  \n",
              "26                                   https://i-share-iit.primo.exlibrisgroup.com/discovery/search?query=any,contains,Bethany%20Hamilton,AND&tab=Everything&search_scope=MyInst_and_CI&sortby=rank&vid=01CARLI_IIT:CARLI_IIT&mfacet=tlevel,include,peer_reviewed,1&lang=en&mode=advanced&offset=0  \n",
              "27                                         https://i-share-iit.primo.exlibrisgroup.com/discovery/search?query=any,contains,Tia%20Blanco,AND&tab=Everything&search_scope=MyInst_and_CI&sortby=rank&vid=01CARLI_IIT:CARLI_IIT&mfacet=tlevel,include,peer_reviewed,1&lang=en&mode=advanced&offset=0  \n",
              "28                                       https://i-share-iit.primo.exlibrisgroup.com/discovery/search?query=any,contains,Para%20surfing,AND&tab=Everything&search_scope=MyInst_and_CI&sortby=rank&vid=01CARLI_IIT:CARLI_IIT&mfacet=tlevel,include,peer_reviewed,1&lang=en&mode=advanced&offset=0  \n",
              "29                                 https://i-share-iit.primo.exlibrisgroup.com/discovery/search?query=any,contains,Trestles%20(surfing),AND&tab=Everything&search_scope=MyInst_and_CI&sortby=rank&vid=01CARLI_IIT:CARLI_IIT&mfacet=tlevel,include,peer_reviewed,1&lang=en&mode=advanced&offset=0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b3bc9661-5022-4f9f-8647-fb6a3b48631c\" class=\"colab-df-container\">\n",
              "    <!--| quarto-html-table-processing: none -->\n",
              "<table id=\"itables_3bb4a74e_0ce1_4ccc_b587_8bf2f7fce601\"><tbody><tr>\n",
              "    <td style=\"vertical-align:middle; text-align:left\">\n",
              "    <a href=https://mwouts.github.io/itables/><svg class=\"main-svg\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\n",
              "width=\"64\" viewBox=\"0 0 500 400\" style=\"font-family: 'Droid Sans', sans-serif;\">\n",
              "    <g style=\"fill:#d9d7fc\">\n",
              "        <path d=\"M100,400H500V357H100Z\" />\n",
              "        <path d=\"M100,300H400V257H100Z\" />\n",
              "        <path d=\"M0,200H400V157H0Z\" />\n",
              "        <path d=\"M100,100H500V57H100Z\" />\n",
              "        <path d=\"M100,350H500V307H100Z\" />\n",
              "        <path d=\"M100,250H400V207H100Z\" />\n",
              "        <path d=\"M0,150H400V107H0Z\" />\n",
              "        <path d=\"M100,50H500V7H100Z\" />\n",
              "    </g>\n",
              "    <g style=\"fill:#1a1366;stroke:#1a1366;\">\n",
              "   <rect x=\"100\" y=\"7\" width=\"400\" height=\"43\">\n",
              "    <animate\n",
              "      attributeName=\"width\"\n",
              "      values=\"0;400;0\"\n",
              "      dur=\"5s\"\n",
              "      repeatCount=\"indefinite\" />\n",
              "      <animate\n",
              "      attributeName=\"x\"\n",
              "      values=\"100;100;500\"\n",
              "      dur=\"5s\"\n",
              "      repeatCount=\"indefinite\" />\n",
              "  </rect>\n",
              "        <rect x=\"0\" y=\"107\" width=\"400\" height=\"43\">\n",
              "    <animate\n",
              "      attributeName=\"width\"\n",
              "      values=\"0;400;0\"\n",
              "      dur=\"3.5s\"\n",
              "      repeatCount=\"indefinite\" />\n",
              "    <animate\n",
              "      attributeName=\"x\"\n",
              "      values=\"0;0;400\"\n",
              "      dur=\"3.5s\"\n",
              "      repeatCount=\"indefinite\" />\n",
              "  </rect>\n",
              "        <rect x=\"100\" y=\"207\" width=\"300\" height=\"43\">\n",
              "    <animate\n",
              "      attributeName=\"width\"\n",
              "      values=\"0;300;0\"\n",
              "      dur=\"3s\"\n",
              "      repeatCount=\"indefinite\" />\n",
              "    <animate\n",
              "      attributeName=\"x\"\n",
              "      values=\"100;100;400\"\n",
              "      dur=\"3s\"\n",
              "      repeatCount=\"indefinite\" />\n",
              "  </rect>\n",
              "        <rect x=\"100\" y=\"307\" width=\"400\" height=\"43\">\n",
              "    <animate\n",
              "      attributeName=\"width\"\n",
              "      values=\"0;400;0\"\n",
              "      dur=\"4s\"\n",
              "      repeatCount=\"indefinite\" />\n",
              "      <animate\n",
              "      attributeName=\"x\"\n",
              "      values=\"100;100;500\"\n",
              "      dur=\"4s\"\n",
              "      repeatCount=\"indefinite\" />\n",
              "  </rect>\n",
              "        <g style=\"fill:transparent;stroke-width:8; stroke-linejoin:round\" rx=\"5\">\n",
              "            <g transform=\"translate(45 50) rotate(-45)\">\n",
              "                <circle r=\"33\" cx=\"0\" cy=\"0\" />\n",
              "                <rect x=\"-8\" y=\"32\" width=\"16\" height=\"30\" />\n",
              "            </g>\n",
              "\n",
              "            <g transform=\"translate(450 152)\">\n",
              "                <polyline points=\"-15,-20 -35,-20 -35,40 25,40 25,20\" />\n",
              "                <rect x=\"-15\" y=\"-40\" width=\"60\" height=\"60\" />\n",
              "            </g>\n",
              "\n",
              "            <g transform=\"translate(50 352)\">\n",
              "                <polygon points=\"-35,-5 0,-40 35,-5\" />\n",
              "                <polygon points=\"-35,10 0,45 35,10\" />\n",
              "            </g>\n",
              "\n",
              "            <g transform=\"translate(75 250)\">\n",
              "                <polyline points=\"-30,30 -60,0 -30,-30\" />\n",
              "                <polyline points=\"0,30 -30,0 0,-30\" />\n",
              "            </g>\n",
              "\n",
              "            <g transform=\"translate(425 250) rotate(180)\">\n",
              "                <polyline points=\"-30,30 -60,0 -30,-30\" />\n",
              "                <polyline points=\"0,30 -30,0 0,-30\" />\n",
              "            </g>\n",
              "        </g>\n",
              "    </g>\n",
              "</svg>\n",
              "</a>\n",
              "    Loading ITables v2.7.0 from the internet...\n",
              "    (need <a href=https://mwouts.github.io/itables/troubleshooting.html>help</a>?)</td>\n",
              "    </tr></tbody></table>\n",
              "<link href=\"https://www.unpkg.com/dt_for_itables@2.5.0/dt_bundle.css\" rel=\"stylesheet\">\n",
              "<script type=\"module\">\n",
              "    import { ITable, jQuery as $ } from 'https://www.unpkg.com/dt_for_itables@2.5.0/dt_bundle.js';\n",
              "\n",
              "    document.querySelectorAll(\"#itables_3bb4a74e_0ce1_4ccc_b587_8bf2f7fce601:not(.dataTable)\").forEach(table => {\n",
              "        if (!(table instanceof HTMLTableElement))\n",
              "            return;\n",
              "\n",
              "        let dt_args = {\"layout\": {\"topStart\": \"pageLength\", \"topEnd\": \"search\", \"bottomStart\": \"info\", \"bottomEnd\": \"paging\"}, \"style\": {\"table-layout\": \"auto\", \"width\": \"auto\", \"margin\": \"auto\", \"caption-side\": \"bottom\"}, \"classes\": [\"display\", \"nowrap\", \"compact\"], \"text_in_header_can_be_selected\": true, \"order\": [], \"table_html\": \"<table><thead>\\n    <tr style=\\\"text-align: right;\\\">\\n      \\n      <th>Article</th>\\n      <th>Article Class</th>\\n      <th>Last 3 Months Views</th>\\n      <th>Word Count</th>\\n      <th>Edits Last Year</th>\\n      <th>Total Editors</th>\\n      <th>Citations</th>\\n      <th>Citations Needed</th>\\n      <th>Collaboration Score</th>\\n      <th>Aliveness Score</th>\\n      <th>Popularity Score</th>\\n      <th>Quality Score</th>\\n      <th>Scholarly Source Score</th>\\n      <th>NPOV Score</th>\\n      <th>Neutrality Score</th>\\n      <th>Flesch-Kincaid Grade</th>\\n      <th>Journal Sources</th>\\n      <th>Book Sources</th>\\n      <th>Avg Source Age</th>\\n      <th>Recent Sources (5yr)</th>\\n      <th>Web Sources</th>\\n      <th>News Sources</th>\\n      <th>Reading Level</th>\\n      <th>Accessibility Score</th>\\n      <th>Source Quality Score</th>\\n      <th>Sentiment</th>\\n      <th>Polarity</th>\\n      <th>Subjectivity</th>\\n      <th>VADER Compound</th>\\n      <th>Images</th>\\n      <th>Days Since Last Edit</th>\\n      <th>Talk Page Size</th>\\n      <th>Hedging Words</th>\\n      <th>Peacock Words</th>\\n      <th>Weasel Words</th>\\n      <th>Value Judgments</th>\\n      <th>Categories</th>\\n      <th>Section Names</th>\\n      <th>Wikipedia Link</th>\\n      <th>IIT Library Link</th>\\n    </tr>\\n  </thead></table>\", \"data_json\": \"[[\\\"Surfing (disambiguation)\\\", \\\"Unassessed\\\", 246, 193, 4, 39, 0, 0, 12, 28, 10, 30, 31, 65, [\\\"100.0\\\", 16], [\\\"28.3\\\", 25], 0, 0, [\\\"0.0\\\", 1], 0, 0, 0, \\\"Professional/Academic\\\", 3, [\\\"25.0\\\", 2], \\\"Neutral but Subjective\\\", [\\\"0.017\\\", 5], [\\\"0.361\\\", 12], [\\\"-0.153\\\", 6], 2, 127, 61, 1, 0, 0, 0, 4, \\\"Activities, Music, Other uses, See also\\\", \\\"https://en.wikipedia.org/wiki/Surfing_(disambiguation)\\\", \\\"https://i-share-iit.primo.exlibrisgroup.com/discovery/search?query=any,contains,Surfing%20(disambiguation),AND&amp;tab=Everything&amp;search_scope=MyInst_and_CI&amp;sortby=rank&amp;vid=01CARLI_IIT:CARLI_IIT&amp;mfacet=tlevel,include,peer_reviewed,1&amp;lang=en&amp;mode=advanced&amp;offset=0\\\"], [\\\"Surf\\\", \\\"Unassessed\\\", 1549, 342, 5, 157, 0, 0, 49, 30, 30, 31, 31, 67, [\\\"100.0\\\", 16], [\\\"68.4\\\", 26], 0, 0, [\\\"0.0\\\", 1], 0, 0, 0, \\\"Professional/Academic\\\", 0, [\\\"25.0\\\", 2], \\\"Neutral but Subjective\\\", [\\\"0.047\\\", 9], [\\\"0.335\\\", 10], [\\\"0.946\\\", 13], 2, 142, 1806, 1, 0, 0, 0, 5, \\\"Commercial products, Computers and software, Education, Music, Places, Popular culture, Ships, Sports, Other uses, See also\\\", \\\"https://en.wikipedia.org/wiki/Surf\\\", \\\"https://i-share-iit.primo.exlibrisgroup.com/discovery/search?query=any,contains,Surf,AND&amp;tab=Everything&amp;search_scope=MyInst_and_CI&amp;sortby=rank&amp;vid=01CARLI_IIT:CARLI_IIT&amp;mfacet=tlevel,include,peer_reviewed,1&amp;lang=en&amp;mode=advanced&amp;offset=0\\\"], [\\\"Couch surfing\\\", \\\"C\\\", 3546, 600, 4, 42, 10, 0, 25, 25, 37, 57, 75, 51, [\\\"100.0\\\", 16], [\\\"11.6\\\", 11], 6, 0, [\\\"7.6\\\", 5], 5, 3, 1, \\\"College Graduate\\\", 62, [\\\"85.0\\\", 20], \\\"Neutral but Subjective\\\", [\\\"0.071\\\", 12], [\\\"0.455\\\", 24], [\\\"-0.942\\\", 3], 0, 141, 692, 3, 0, 0, 0, 8, \\\"Couch surfing in travel, Couch surfing as homelessness, See also, References\\\", \\\"https://en.wikipedia.org/wiki/Couch_surfing\\\", \\\"https://i-share-iit.primo.exlibrisgroup.com/discovery/search?query=any,contains,Couch%20surfing,AND&amp;tab=Everything&amp;search_scope=MyInst_and_CI&amp;sortby=rank&amp;vid=01CARLI_IIT:CARLI_IIT&amp;mfacet=tlevel,include,peer_reviewed,1&amp;lang=en&amp;mode=advanced&amp;offset=0\\\"], [\\\"Surfing\\\", \\\"B\\\", 44032, 8725, 72, 279, 82, 4, 77, 89, 90, 51, 88, 18, [\\\"84.0\\\", 8], [\\\"10.7\\\", 8], 12, 15, [\\\"13.6\\\", 14], 20, 42, 8, \\\"College\\\", 73, [\\\"72.5\\\", 18], \\\"Slight Bias\\\", [\\\"0.107\\\", 16], [\\\"0.456\\\", 25], [\\\"1.000\\\", 22], 34, 5, 5967, 15, 11, 2, 0, 17, \\\"Origins and history, Peru, Polynesia, West Africa, California, Surf waves, Tube shape and speed, Wave intensity, Artificial reefs, Artificial waves, Maneuvers, Terms, Learning, Equipment, The physics of surfing, Wave formation, Wave conditions for surfing, Surf breaks, Headland (point break), Beach break, River or estuary entrance bar, Reef break, Ledge break, Rip currents, On the surfboard, Dangers, Drowning, Collisions, Marine life, Rip currents, Seabed, Microorganisms, Ear damage, Surf rash, Spinal cord, Surfers and surf culture, See also, References, Further reading, External links\\\", \\\"https://en.wikipedia.org/wiki/Surfing\\\", \\\"https://i-share-iit.primo.exlibrisgroup.com/discovery/search?query=any,contains,Surfing,AND&amp;tab=Everything&amp;search_scope=MyInst_and_CI&amp;sortby=rank&amp;vid=01CARLI_IIT:CARLI_IIT&amp;mfacet=tlevel,include,peer_reviewed,1&amp;lang=en&amp;mode=advanced&amp;offset=0\\\"], [\\\"Surf music\\\", \\\"START\\\", 23035, 2913, 23, 216, 45, 2, 87, 53, 83, 56, 76, 42, [\\\"96.6\\\", 14], [\\\"10.6\\\", 7], 3, 33, [\\\"24.6\\\", 23], 3, 16, 2, \\\"College\\\", 77, [\\\"73.3\\\", 19], \\\"Slight Bias\\\", [\\\"0.142\\\", 23], [\\\"0.420\\\", 20], [\\\"0.999\\\", 21], 6, 82, 32287, 2, 1, 0, 0, 19, \\\"Instrumental surf, Form, History, Vocal surf, Distinctions, Hot rod rock, Popularity, Decline, Influence and revival, Surf punk, Indie surf, Production, See also, Notes, References, Bibliography, Further reading\\\", \\\"https://en.wikipedia.org/wiki/Surf_music\\\", \\\"https://i-share-iit.primo.exlibrisgroup.com/discovery/search?query=any,contains,Surf%20music,AND&amp;tab=Everything&amp;search_scope=MyInst_and_CI&amp;sortby=rank&amp;vid=01CARLI_IIT:CARLI_IIT&amp;mfacet=tlevel,include,peer_reviewed,1&amp;lang=en&amp;mode=advanced&amp;offset=0\\\"], [\\\"Surf culture\\\", \\\"B\\\", 9336, 7399, 36, 158, 96, 3, 69, 78, 70, 53, 77, 23, [\\\"87.4\\\", 9], [\\\"12.2\\\", 16], 5, 51, [\\\"20.7\\\", 20], 9, 46, 12, \\\"College Graduate\\\", 37, [\\\"64.6\\\", 14], \\\"Slight Bias\\\", [\\\"0.106\\\", 15], [\\\"0.415\\\", 19], [\\\"1.000\\\", 22], 24, 9, 6911, 12, 7, 1, 1, 20, \\\"History, Spirituality, Women in surfing, Beach bunnies, Diversity, Big wave culture, Localism, Surf Nazi, Surf gangs, Wolfpak, Bra Boys, Surf terminology, Issues affecting surfers, Surfing and environmentalism, Surf tourism, Surfing art, Surf visual art, Surf graphics, Surf music, Fashion, Surfwear, Bikini, Events, Surfing contests, Surfing organizations, Spin-offs &amp;amp; influences, Boardsports, Surfing in multimedia, Films about surfing, TV documentary series about surfing, Print media, Surfing magazines, Surfing in fiction, Surfing in non-fiction, Conceptual metaphor, Popular, Natural science, Philosophical, Graphic art, See also, Notes, References, Bibliography, External links\\\", \\\"https://en.wikipedia.org/wiki/Surf_culture\\\", \\\"https://i-share-iit.primo.exlibrisgroup.com/discovery/search?query=any,contains,Surf%20culture,AND&amp;tab=Everything&amp;search_scope=MyInst_and_CI&amp;sortby=rank&amp;vid=01CARLI_IIT:CARLI_IIT&amp;mfacet=tlevel,include,peer_reviewed,1&amp;lang=en&amp;mode=advanced&amp;offset=0\\\"], [\\\"Surfing at the Summer Olympics\\\", \\\"Unassessed\\\", 5153, 279, 4, 30, 5, 0, 12, 61, 53, 54, 36, 67, [\\\"100.0\\\", 16], [\\\"14.5\\\", 23], 0, 0, [\\\"8.0\\\", 6], 2, 5, 0, \\\"College Graduate\\\", 10, [\\\"35.0\\\", 4], \\\"Neutral but Subjective\\\", [\\\"0.070\\\", 11], [\\\"0.352\\\", 11], [\\\"0.802\\\", 9], 10, 3, 220, 0, 0, 0, 0, 8, \\\"Bid for inclusion, Events, Venues, Medalists, Men's shortboard, Women's shortboard, Medal table, References, External links\\\", \\\"https://en.wikipedia.org/wiki/Surfing_at_the_Summer_Olympics\\\", \\\"https://i-share-iit.primo.exlibrisgroup.com/discovery/search?query=any,contains,Surfing%20at%20the%20Summer%20Olympics,AND&amp;tab=Everything&amp;search_scope=MyInst_and_CI&amp;sortby=rank&amp;vid=01CARLI_IIT:CARLI_IIT&amp;mfacet=tlevel,include,peer_reviewed,1&amp;lang=en&amp;mode=advanced&amp;offset=0\\\"], [\\\"Powder surfing\\\", \\\"STUB\\\", 297, 345, 1, 35, 4, 0, 18, 10, 13, 32, 36, 49, [\\\"100.0\\\", 16], [\\\"12.1\\\", 15], 0, 0, [\\\"0.0\\\", 1], 0, 4, 0, \\\"College\\\", 40, [\\\"35.0\\\", 4], \\\"Slight Bias\\\", [\\\"0.176\\\", 27], [\\\"0.608\\\", 28], [\\\"0.994\\\", 18], 0, 346, 402, 0, 0, 0, 0, 1, \\\"References\\\", \\\"https://en.wikipedia.org/wiki/Powder_surfing\\\", \\\"https://i-share-iit.primo.exlibrisgroup.com/discovery/search?query=any,contains,Powder%20surfing,AND&amp;tab=Everything&amp;search_scope=MyInst_and_CI&amp;sortby=rank&amp;vid=01CARLI_IIT:CARLI_IIT&amp;mfacet=tlevel,include,peer_reviewed,1&amp;lang=en&amp;mode=advanced&amp;offset=0\\\"], [\\\"World Wide Web\\\", \\\"C\\\", 364836, 7184, 208, 310, 143, 2, 77, 96, 100, 67, 87, 41, [\\\"97.5\\\", 15], [\\\"12.7\\\", 19], 13, 16, [\\\"14.3\\\", 15], 42, 82, 25, \\\"College Graduate\\\", 25, [\\\"65.7\\\", 15], \\\"Neutral but Subjective\\\", [\\\"0.054\\\", 10], [\\\"0.383\\\", 16], [\\\"1.000\\\", 22], 21, 3, 3528, 47, 1, 0, 1, 37, \\\"History, Competition with Gopher, Nomenclature, Function, HTML, Linking, www prefix, Scheme specifiers, Pages, Static page, Dynamic pages, Website, Browser, Server, Optical Networking, Cookie, Search engine, Deep web, Caching, Security, Privacy, Standards, Accessibility, Internationalisation, See also, References, Further reading, External links\\\", \\\"https://en.wikipedia.org/wiki/World_Wide_Web\\\", \\\"https://i-share-iit.primo.exlibrisgroup.com/discovery/search?query=any,contains,World%20Wide%20Web,AND&amp;tab=Everything&amp;search_scope=MyInst_and_CI&amp;sortby=rank&amp;vid=01CARLI_IIT:CARLI_IIT&amp;mfacet=tlevel,include,peer_reviewed,1&amp;lang=en&amp;mode=advanced&amp;offset=0\\\"], [\\\"History of surfing\\\", \\\"C\\\", 8019, 5175, 24, 259, 39, 1, 83, 62, 63, 43, 67, 17, [\\\"77.4\\\", 6], [\\\"12.3\\\", 17], 3, 9, [\\\"48.4\\\", 25], 3, 12, 2, \\\"College Graduate\\\", 33, [\\\"63.1\\\", 13], \\\"Slight Bias\\\", [\\\"0.139\\\", 22], [\\\"0.427\\\", 21], [\\\"1.000\\\", 22], 8, 61, 12773, 8, 7, 1, 4, 13, \\\"Peru, West Africa, Polynesia, Ancient Hawaii, Post-contact Hawaii, North America, Australian surfing, Great Britain, Modern surfing, Professional surfing, Technological innovations in surfing, The Short Board Revolution, Big wave surfing, Style versus performance, Surfing's impact on popular culture, Alternatives to wind-generated waves, References, Citations, Sources, External links, Surfing museums\\\", \\\"https://en.wikipedia.org/wiki/History_of_surfing\\\", \\\"https://i-share-iit.primo.exlibrisgroup.com/discovery/search?query=any,contains,History%20of%20surfing,AND&amp;tab=Everything&amp;search_scope=MyInst_and_CI&amp;sortby=rank&amp;vid=01CARLI_IIT:CARLI_IIT&amp;mfacet=tlevel,include,peer_reviewed,1&amp;lang=en&amp;mode=advanced&amp;offset=0\\\"], [\\\"Big wave surfing\\\", \\\"START\\\", 8411, 1453, 19, 273, 61, 1, 82, 64, 67, 56, 44, 39, [\\\"51.8\\\", 2], [\\\"11.1\\\", 9], 0, 0, [\\\"8.7\\\", 8], 13, 52, 6, \\\"College\\\", 70, [\\\"45.0\\\", 6], \\\"Slight Bias\\\", [\\\"0.155\\\", 25], [\\\"0.305\\\", 6], [\\\"0.997\\\", 19], 8, 34, 7939, 3, 7, 0, 0, 13, \\\"Hazards of big wave surfing, Paddle-in surfing, Big wave surfing world records, Male, Female, Big Wave Surfing Contest, Big Wave Surfing Awards, WSL Big Wave Championship Tour champions, Notable big wave surfing spots, Australia, United States (Mainland), Oceania, Europe, Latin America, Caribbean, Africa, Notable big wave surfers, Big wave surfing movies, See also, References, External links\\\", \\\"https://en.wikipedia.org/wiki/Big_wave_surfing\\\", \\\"https://i-share-iit.primo.exlibrisgroup.com/discovery/search?query=any,contains,Big%20wave%20surfing,AND&amp;tab=Everything&amp;search_scope=MyInst_and_CI&amp;sortby=rank&amp;vid=01CARLI_IIT:CARLI_IIT&amp;mfacet=tlevel,include,peer_reviewed,1&amp;lang=en&amp;mode=advanced&amp;offset=0\\\"], [\\\"Car surfing\\\", \\\"START\\\", 1908, 238, 4, 158, 2, 0, 42, 20, 33, 38, 27, 75, [\\\"100.0\\\", 16], [\\\"11.8\\\", 13], 0, 0, [\\\"12.0\\\", 11], 0, 1, 0, \\\"College Graduate\\\", 48, [\\\"25.0\\\", 2], \\\"Neutral &amp; Objective\\\", [\\\"0.039\\\", 8], [\\\"0.250\\\", 3], [\\\"-0.788\\\", 5], 2, 216, 604, 0, 0, 0, 0, 7, \\\"History, Risks, See also, References, External links\\\", \\\"https://en.wikipedia.org/wiki/Car_surfing\\\", \\\"https://i-share-iit.primo.exlibrisgroup.com/discovery/search?query=any,contains,Car%20surfing,AND&amp;tab=Everything&amp;search_scope=MyInst_and_CI&amp;sortby=rank&amp;vid=01CARLI_IIT:CARLI_IIT&amp;mfacet=tlevel,include,peer_reviewed,1&amp;lang=en&amp;mode=advanced&amp;offset=0\\\"], [\\\"Train surfing\\\", \\\"C\\\", 9492, 2591, 59, 239, 44, 3, 83, 65, 73, 55, 59, 46, [\\\"96.1\\\", 13], [\\\"11.6\\\", 11], 5, 0, [\\\"12.4\\\", 12], 6, 4, 34, \\\"College\\\", 62, [\\\"50.8\\\", 8], \\\"Neutral but Subjective\\\", [\\\"0.034\\\", 7], [\\\"0.369\\\", 14], [\\\"-0.999\\\", 1], 10, 78, 13019, 4, 1, 0, 0, 23, \\\"Overcrowding, Travelling without a ticket, For entertainment, Hazards, Injuries and deaths, Prevention and punishments, Deterrents, See also, References, Notes, Bibliography, External links\\\", \\\"https://en.wikipedia.org/wiki/Train_surfing\\\", \\\"https://i-share-iit.primo.exlibrisgroup.com/discovery/search?query=any,contains,Train%20surfing,AND&amp;tab=Everything&amp;search_scope=MyInst_and_CI&amp;sortby=rank&amp;vid=01CARLI_IIT:CARLI_IIT&amp;mfacet=tlevel,include,peer_reviewed,1&amp;lang=en&amp;mode=advanced&amp;offset=0\\\"], [\\\"Snowboarding\\\", \\\"C\\\", 90498, 4765, 39, 315, 78, 0, 77, 82, 93, 63, 83, 33, [\\\"89.9\\\", 10], [\\\"11.8\\\", 13], 10, 7, [\\\"13.0\\\", 13], 13, 49, 3, \\\"College Graduate\\\", 48, [\\\"67.3\\\", 16], \\\"Slight Bias\\\", [\\\"0.115\\\", 17], [\\\"0.387\\\", 17], [\\\"0.999\\\", 21], 28, 7, 3017, 6, 4, 0, 1, 24, \\\"History, Styles, Jibbing, Freeriding, Freestyle, Alpine snowboarding, Slopestyle, Big air, Half-pipe, Snowboard cross, Snowboard racing, Competitions, Subculture, Common injuries, Safety and precautions, Terminology, Stances, Parts of a snowboard, Rotations, Flips, Off-axis rotations, Notable people, See also, References, External links\\\", \\\"https://en.wikipedia.org/wiki/Snowboarding\\\", \\\"https://i-share-iit.primo.exlibrisgroup.com/discovery/search?query=any,contains,Snowboarding,AND&amp;tab=Everything&amp;search_scope=MyInst_and_CI&amp;sortby=rank&amp;vid=01CARLI_IIT:CARLI_IIT&amp;mfacet=tlevel,include,peer_reviewed,1&amp;lang=en&amp;mode=advanced&amp;offset=0\\\"], [\\\"Surfing with the Alien\\\", \\\"C\\\", 10518, 1401, 4, 190, 30, 0, 63, 27, 77, 54, 46, 25, [\\\"71.4\\\", 3], [\\\"13.3\\\", 21], 0, 2, [\\\"18.6\\\", 18], 0, 6, 4, \\\"College Graduate\\\", 17, [\\\"53.3\\\", 9], \\\"Slight Bias\\\", [\\\"0.117\\\", 18], [\\\"0.437\\\", 22], [\\\"0.998\\\", 20], 6, 133, 5092, 2, 4, 0, 0, 28, \\\"Recording, Composition, Artwork, Release and reception, Reissues, Track listing, 2007 remastered edition bonus DVD, Personnel, Charts, Weekly charts, Year-end charts, Certifications, References, External links\\\", \\\"https://en.wikipedia.org/wiki/Surfing_with_the_Alien\\\", \\\"https://i-share-iit.primo.exlibrisgroup.com/discovery/search?query=any,contains,Surfing%20with%20the%20Alien,AND&amp;tab=Everything&amp;search_scope=MyInst_and_CI&amp;sortby=rank&amp;vid=01CARLI_IIT:CARLI_IIT&amp;mfacet=tlevel,include,peer_reviewed,1&amp;lang=en&amp;mode=advanced&amp;offset=0\\\"], [\\\"Tanker surfing\\\", \\\"START\\\", 60, 213, 1, 12, 4, 1, 5, 33, 3, 35, 27, 50, [\\\"100.0\\\", 16], [\\\"10.1\\\", 4], 0, 0, [\\\"10.6\\\", 10], 0, 0, 4, \\\"High School (10-12th)\\\", 87, [\\\"25.0\\\", 2], \\\"Positive Bias Detected\\\", [\\\"0.275\\\", 29], [\\\"0.581\\\", 27], [\\\"0.848\\\", 12], 1, 58, 185, 0, 0, 0, 0, 5, \\\"References\\\", \\\"https://en.wikipedia.org/wiki/Tanker_surfing\\\", \\\"https://i-share-iit.primo.exlibrisgroup.com/discovery/search?query=any,contains,Tanker%20surfing,AND&amp;tab=Everything&amp;search_scope=MyInst_and_CI&amp;sortby=rank&amp;vid=01CARLI_IIT:CARLI_IIT&amp;mfacet=tlevel,include,peer_reviewed,1&amp;lang=en&amp;mode=advanced&amp;offset=0\\\"], [\\\"Crowd surfing\\\", \\\"Unassessed\\\", 4136, 324, 8, 292, 8, 0, 88, 52, 47, 56, 36, 74, [\\\"100.0\\\", 16], [\\\"12.0\\\", 14], 0, 1, [\\\"25.4\\\", 24], 0, 2, 1, \\\"College\\\", 43, [\\\"42.5\\\", 5], \\\"Neutral &amp; Objective\\\", [\\\"0.010\\\", 4], [\\\"0.261\\\", 4], [\\\"0.847\\\", 11], 3, 51, 11512, 0, 0, 0, 0, 5, \\\"Origins, See also, References, External links\\\", \\\"https://en.wikipedia.org/wiki/Crowd_surfing\\\", \\\"https://i-share-iit.primo.exlibrisgroup.com/discovery/search?query=any,contains,Crowd%20surfing,AND&amp;tab=Everything&amp;search_scope=MyInst_and_CI&amp;sortby=rank&amp;vid=01CARLI_IIT:CARLI_IIT&amp;mfacet=tlevel,include,peer_reviewed,1&amp;lang=en&amp;mode=advanced&amp;offset=0\\\"], [\\\"Glossary of surfing\\\", \\\"LIST\\\", 3824, 2132, 4, 112, 16, 0, 45, 38, 40, 40, 50, 32, [\\\"74.2\\\", 4], [\\\"21.7\\\", 24], 0, 6, [\\\"19.9\\\", 19], 6, 20, 0, \\\"College Graduate\\\", 7, [\\\"46.5\\\", 7], \\\"Neutral but Subjective\\\", [\\\"0.081\\\", 13], [\\\"0.365\\\", 13], [\\\"0.991\\\", 17], 9, 63, 2384, 1, 4, 1, 0, 10, \\\"About the water, Techniques and maneuvers, Accidental, About people and behavior, About the board, Clothing, Further reading, See also, Notes, References, External links\\\", \\\"https://en.wikipedia.org/wiki/Glossary_of_surfing\\\", \\\"https://i-share-iit.primo.exlibrisgroup.com/discovery/search?query=any,contains,Glossary%20of%20surfing,AND&amp;tab=Everything&amp;search_scope=MyInst_and_CI&amp;sortby=rank&amp;vid=01CARLI_IIT:CARLI_IIT&amp;mfacet=tlevel,include,peer_reviewed,1&amp;lang=en&amp;mode=advanced&amp;offset=0\\\"], [\\\"Elevator surfing\\\", \\\"Unassessed\\\", 1548, 820, 15, 157, 21, 0, 44, 50, 27, 45, 40, 48, [\\\"100.0\\\", 16], [\\\"13.4\\\", 22], 1, 0, [\\\"22.1\\\", 22], 6, 7, 12, \\\"College Graduate\\\", 13, [\\\"32.5\\\", 3], \\\"Neutral but Subjective\\\", [\\\"0.028\\\", 6], [\\\"0.458\\\", 26], [\\\"-0.990\\\", 2], 2, 64, 1591, 5, 0, 0, 0, 6, \\\"Origins, Hazards, Injuries and deaths, See also, References\\\", \\\"https://en.wikipedia.org/wiki/Elevator_surfing\\\", \\\"https://i-share-iit.primo.exlibrisgroup.com/discovery/search?query=any,contains,Elevator%20surfing,AND&amp;tab=Everything&amp;search_scope=MyInst_and_CI&amp;sortby=rank&amp;vid=01CARLI_IIT:CARLI_IIT&amp;mfacet=tlevel,include,peer_reviewed,1&amp;lang=en&amp;mode=advanced&amp;offset=0\\\"], [\\\"List of train-surfing injuries and deaths\\\", \\\"LIST\\\", 5106, 30, 133, 75, 685, 0, 42, 83, 50, 79, 70, 78, [\\\"100.0\\\", 16], [\\\"9.6\\\", 2], 6, 0, [\\\"7.1\\\", 3], 268, 206, 468, \\\"College Graduate\\\", 93, [\\\"55.4\\\", 10], \\\"Neutral &amp; Objective\\\", [\\\"0.000\\\", 1], [\\\"0.000\\\", 1], [\\\"0.000\\\", 7], 6, 23, 2002, 0, 0, 0, 0, 20, \\\"Data of train-surfing injuries and deaths, Train-surfing injuries and deaths, See also, References\\\", \\\"https://en.wikipedia.org/wiki/List_of_train-surfing_injuries_and_deaths\\\", \\\"https://i-share-iit.primo.exlibrisgroup.com/discovery/search?query=any,contains,List%20of%20train-surfing%20injuries%20and%20deaths,AND&amp;tab=Everything&amp;search_scope=MyInst_and_CI&amp;sortby=rank&amp;vid=01CARLI_IIT:CARLI_IIT&amp;mfacet=tlevel,include,peer_reviewed,1&amp;lang=en&amp;mode=advanced&amp;offset=0\\\"], [\\\"Surfing in Madeira\\\", \\\"START\\\", 354, 115, 0, 45, 1, 0, 33, 4, 17, 38, 31, 69, [\\\"100.0\\\", 16], [\\\"12.5\\\", 18], 0, 0, [\\\"0.0\\\", 1], 0, 0, 0, \\\"College Graduate\\\", 30, [\\\"25.0\\\", 2], \\\"Slight Bias\\\", [\\\"0.147\\\", 24], [\\\"0.327\\\", 9], [\\\"0.807\\\", 10], 2, 552, 1774, 0, 0, 0, 0, 9, \\\"References, External links\\\", \\\"https://en.wikipedia.org/wiki/Surfing_in_Madeira\\\", \\\"https://i-share-iit.primo.exlibrisgroup.com/discovery/search?query=any,contains,Surfing%20in%20Madeira,AND&amp;tab=Everything&amp;search_scope=MyInst_and_CI&amp;sortby=rank&amp;vid=01CARLI_IIT:CARLI_IIT&amp;mfacet=tlevel,include,peer_reviewed,1&amp;lang=en&amp;mode=advanced&amp;offset=0\\\"], [\\\"Shoulder surfing\\\", \\\"Unassessed\\\", 107, 12, 0, 26, 0, 0, 13, 2, 7, 32, 31, 77, [\\\"100.0\\\", 16], [\\\"0.0\\\", 1], 0, 0, [\\\"0.0\\\", 1], 0, 0, 0, \\\"Unknown\\\", 97, [\\\"25.0\\\", 2], \\\"Neutral &amp; Objective\\\", [\\\"0.000\\\", 1], [\\\"0.000\\\", 1], [\\\"0.340\\\", 8], 1, 2241, 401, 1, 0, 0, 0, 4, \\\"\\\", \\\"https://en.wikipedia.org/wiki/Shoulder_surfing\\\", \\\"https://i-share-iit.primo.exlibrisgroup.com/discovery/search?query=any,contains,Shoulder%20surfing,AND&amp;tab=Everything&amp;search_scope=MyInst_and_CI&amp;sortby=rank&amp;vid=01CARLI_IIT:CARLI_IIT&amp;mfacet=tlevel,include,peer_reviewed,1&amp;lang=en&amp;mode=advanced&amp;offset=0\\\"], [\\\"Shoulder surfing (computer security)\\\", \\\"C\\\", 5886, 553, 17, 149, 17, 0, 61, 72, 57, 63, 72, 52, [\\\"100.0\\\", 16], [\\\"12.8\\\", 20], 3, 3, [\\\"13.0\\\", 13], 2, 5, 0, \\\"College Graduate\\\", 20, [\\\"72.3\\\", 17], \\\"Neutral but Subjective\\\", [\\\"0.005\\\", 2], [\\\"0.437\\\", 22], [\\\"-0.797\\\", 4], 0, 5, 6673, 10, 0, 0, 0, 6, \\\"Overview, Countermeasures, Graphical passwords, PIN entry, Biometrics, Eye tracking, Virtual reality, See also, References\\\", \\\"https://en.wikipedia.org/wiki/Shoulder_surfing_(computer_security)\\\", \\\"https://i-share-iit.primo.exlibrisgroup.com/discovery/search?query=any,contains,Shoulder%20surfing%20(computer%20security),AND&amp;tab=Everything&amp;search_scope=MyInst_and_CI&amp;sortby=rank&amp;vid=01CARLI_IIT:CARLI_IIT&amp;mfacet=tlevel,include,peer_reviewed,1&amp;lang=en&amp;mode=advanced&amp;offset=0\\\"], [\\\"Surfing on a Rocket\\\", \\\"STUB\\\", 664, 461, 1, 63, 20, 0, 33, 23, 23, 58, 26, 76, [\\\"100.0\\\", 16], [\\\"10.2\\\", 5], 0, 1, [\\\"22.0\\\", 21], 0, 11, 0, \\\"College\\\", 83, [\\\"24.2\\\", 1], \\\"Neutral &amp; Objective\\\", [\\\"0.009\\\", 3], [\\\"0.224\\\", 2], [\\\"0.983\\\", 14], 4, 87, 1771, 0, 0, 0, 0, 15, \\\"Music video, Track listings, Charts, Release history, References\\\", \\\"https://en.wikipedia.org/wiki/Surfing_on_a_Rocket\\\", \\\"https://i-share-iit.primo.exlibrisgroup.com/discovery/search?query=any,contains,Surfing%20on%20a%20Rocket,AND&amp;tab=Everything&amp;search_scope=MyInst_and_CI&amp;sortby=rank&amp;vid=01CARLI_IIT:CARLI_IIT&amp;mfacet=tlevel,include,peer_reviewed,1&amp;lang=en&amp;mode=advanced&amp;offset=0\\\"], [\\\"Surfing on Sine Waves\\\", \\\"START\\\", 7588, 472, 19, 149, 21, 0, 59, 48, 60, 63, 64, 22, [\\\"36.4\\\", 1], [\\\"10.5\\\", 6], 1, 2, [\\\"16.8\\\", 17], 6, 15, 1, \\\"College\\\", 80, [\\\"62.9\\\", 12], \\\"Positive Bias Detected\\\", [\\\"0.268\\\", 28], [\\\"0.453\\\", 23], [\\\"0.984\\\", 15], 6, 92, 6251, 2, 3, 0, 0, 26, \\\"Background, Reissues, Reception, Track listing, Personnel, Charts, References, External links\\\", \\\"https://en.wikipedia.org/wiki/Surfing_on_Sine_Waves\\\", \\\"https://i-share-iit.primo.exlibrisgroup.com/discovery/search?query=any,contains,Surfing%20on%20Sine%20Waves,AND&amp;tab=Everything&amp;search_scope=MyInst_and_CI&amp;sortby=rank&amp;vid=01CARLI_IIT:CARLI_IIT&amp;mfacet=tlevel,include,peer_reviewed,1&amp;lang=en&amp;mode=advanced&amp;offset=0\\\"], [\\\"World Surf League\\\", \\\"START\\\", 12265, 2486, 35, 191, 39, 1, 67, 75, 80, 47, 42, 33, [\\\"79.9\\\", 7], [\\\"11.7\\\", 12], 0, 0, [\\\"9.1\\\", 9], 8, 31, 4, \\\"College\\\", 55, [\\\"45.0\\\", 6], \\\"Slight Bias\\\", [\\\"0.159\\\", 26], [\\\"0.392\\\", 18], [\\\"1.000\\\", 22], 24, 12, 5487, 5, 5, 0, 0, 20, \\\"History, Predecessors, Equal pay for athletes in 2019, COVID-19 impact, WSL sanctioned tours, WSL Championship tour, WSL Qualifying Series events, WSL world ranking, Promotion and relegation, 2012 tours, 2013-2018 tours, Rules, Judging, Rules, WSL Championship Tour champions, Surfers with the most World Tour wins (Men), WSL Longboard Championship Tour champions, WSL World Junior champions, WSL Big Wave Tour champions, Men's Triple Crown Champions, Top Nations, See also, References, External links\\\", \\\"https://en.wikipedia.org/wiki/World_Surf_League\\\", \\\"https://i-share-iit.primo.exlibrisgroup.com/discovery/search?query=any,contains,World%20Surf%20League,AND&amp;tab=Everything&amp;search_scope=MyInst_and_CI&amp;sortby=rank&amp;vid=01CARLI_IIT:CARLI_IIT&amp;mfacet=tlevel,include,peer_reviewed,1&amp;lang=en&amp;mode=advanced&amp;offset=0\\\"], [\\\"Bethany Hamilton\\\", \\\"C\\\", 95605, 1715, 57, 283, 48, 2, 92, 92, 97, 60, 45, 44, [\\\"76.7\\\", 5], [\\\"11.5\\\", 10], 0, 0, [\\\"8.2\\\", 7], 17, 42, 4, \\\"College Graduate\\\", 67, [\\\"45.0\\\", 6], \\\"Slight Bias\\\", [\\\"0.135\\\", 21], [\\\"0.318\\\", 8], [\\\"0.999\\\", 21], 14, 2, 15983, 0, 4, 0, 0, 38, \\\"Early life, Shark attack and recovery, Media, Personal life, Books, Outreach programs, Surfing career, World Surf League boycott, Filmography, Film, Television, References, External links\\\", \\\"https://en.wikipedia.org/wiki/Bethany_Hamilton\\\", \\\"https://i-share-iit.primo.exlibrisgroup.com/discovery/search?query=any,contains,Bethany%20Hamilton,AND&amp;tab=Everything&amp;search_scope=MyInst_and_CI&amp;sortby=rank&amp;vid=01CARLI_IIT:CARLI_IIT&amp;mfacet=tlevel,include,peer_reviewed,1&amp;lang=en&amp;mode=advanced&amp;offset=0\\\"], [\\\"Tia Blanco\\\", \\\"START\\\", 30851, 454, 12, 74, 15, 0, 22, 57, 87, 64, 44, 69, [\\\"100.0\\\", 16], [\\\"10.0\\\", 3], 0, 0, [\\\"7.2\\\", 4], 10, 13, 2, \\\"College\\\", 90, [\\\"45.0\\\", 6], \\\"Slight Bias\\\", [\\\"0.126\\\", 19], [\\\"0.313\\\", 7], [\\\"0.998\\\", 20], 4, 42, 220, 2, 0, 0, 0, 10, \\\"Early life, Professional career, Personal life, Filmography, References, External links\\\", \\\"https://en.wikipedia.org/wiki/Tia_Blanco\\\", \\\"https://i-share-iit.primo.exlibrisgroup.com/discovery/search?query=any,contains,Tia%20Blanco,AND&amp;tab=Everything&amp;search_scope=MyInst_and_CI&amp;sortby=rank&amp;vid=01CARLI_IIT:CARLI_IIT&amp;mfacet=tlevel,include,peer_reviewed,1&amp;lang=en&amp;mode=advanced&amp;offset=0\\\"], [\\\"Para surfing\\\", \\\"C\\\", 520, 1022, 4, 21, 27, 0, 18, 48, 20, 56, 63, 56, [\\\"90.2\\\", 11], [\\\"12.7\\\", 19], 1, 0, [\\\"5.1\\\", 2], 15, 11, 15, \\\"College Graduate\\\", 25, [\\\"56.9\\\", 11], \\\"Neutral but Subjective\\\", [\\\"0.082\\\", 14], [\\\"0.302\\\", 5], [\\\"0.998\\\", 20], 2, 33, 1116, 0, 1, 0, 0, 4, \\\"History, Classification, Events, ISA, AASP, PSL, Other, Paralympics, See also, References\\\", \\\"https://en.wikipedia.org/wiki/Para_surfing\\\", \\\"https://i-share-iit.primo.exlibrisgroup.com/discovery/search?query=any,contains,Para%20surfing,AND&amp;tab=Everything&amp;search_scope=MyInst_and_CI&amp;sortby=rank&amp;vid=01CARLI_IIT:CARLI_IIT&amp;mfacet=tlevel,include,peer_reviewed,1&amp;lang=en&amp;mode=advanced&amp;offset=0\\\"], [\\\"Trestles (surfing)\\\", \\\"START\\\", 4091, 1317, 11, 146, 57, 1, 70, 32, 43, 56, 37, 45, [\\\"92.4\\\", 12], [\\\"11.7\\\", 12], 0, 0, [\\\"15.1\\\", 16], 15, 38, 14, \\\"College\\\", 55, [\\\"35.0\\\", 4], \\\"Slight Bias\\\", [\\\"0.130\\\", 20], [\\\"0.378\\\", 15], [\\\"0.990\\\", 16], 5, 147, 21287, 2, 1, 0, 0, 22, \\\"Access and amenities, Plant and animal life, Seasonal stream, Toll road controversy, In popular culture, References, External links\\\", \\\"https://en.wikipedia.org/wiki/Trestles_(surfing)\\\", \\\"https://i-share-iit.primo.exlibrisgroup.com/discovery/search?query=any,contains,Trestles%20(surfing),AND&amp;tab=Everything&amp;search_scope=MyInst_and_CI&amp;sortby=rank&amp;vid=01CARLI_IIT:CARLI_IIT&amp;mfacet=tlevel,include,peer_reviewed,1&amp;lang=en&amp;mode=advanced&amp;offset=0\\\"]]\", \"columnDefs\": [{\"targets\": [14, 15, 18, 24, 26, 27, 28], \"render\": \"\\n                        function (data, type, row, meta) {\\n                            return type === 'sort' ? data[1] : data[0];\\n                        }\\n                    \"}], \"keys_to_be_evaluated\": [[\"columnDefs\", 0, \"render\"]]};\n",
              "        new ITable(table, dt_args);\n",
              "    });\n",
              "</script>\n",
              "\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b3bc9661-5022-4f9f-8647-fb6a3b48631c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b3bc9661-5022-4f9f-8647-fb6a3b48631c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b3bc9661-5022-4f9f-8647-fb6a3b48631c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_78abf052-05fb-4f15-9c3b-c8786c8de808\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_final')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_78abf052-05fb-4f15-9c3b-c8786c8de808 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_final');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_final"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Summary Statistics\n",
            "============================================================\n",
            "       Last 3 Months Views  Word Count  Talk Page Size  Citations  Days Since Last Edit  Source Quality Score  Neutrality Score  Flesch-Kincaid Grade  Polarity  Subjectivity  Collaboration Score  Aliveness Score  Popularity Score  Quality Score  Scholarly Source Score  NPOV Score  Accessibility Score\n",
            "count                 30.0        30.0            30.0       30.0                  30.0                  30.0              30.0                  30.0      30.0          30.0                 30.0             30.0              30.0           30.0                    30.0        30.0                 30.0\n",
            "mean               25116.0      1857.6          5765.2       53.9                 162.0                  47.5              90.1                  14.1       0.1           0.4                 51.6             50.0              51.7           51.1                    51.4        49.4                 48.3\n",
            "std                68410.8      2397.7          7313.2      123.8                 409.1                  17.9              15.6                  11.1       0.1           0.1                 27.4             26.7              29.4           12.3                    20.0        19.0                 29.4\n",
            "min                   60.0        12.0            61.0        0.0                   2.0                  24.2              36.4                   0.0       0.0           0.0                  5.0              2.0               3.0           30.0                    26.0        17.0                  0.0\n",
            "25%                 1548.2       328.5           798.0        5.8                  14.8                  33.1              84.8                  10.8       0.0           0.3                 27.0             28.5              27.8           40.8                    36.0        34.5                 25.0\n",
            "50%                 5129.5       710.0          2700.5       21.0                  62.0                  45.0              98.8                  11.8       0.1           0.4                 54.0             51.0              51.5           54.5                    44.5        48.5                 48.0\n",
            "75%                10261.5      2397.5          6851.5       47.2                 131.5                  63.0             100.0                  12.7       0.1           0.4                 77.0             70.2              76.0           57.8                    69.2        67.0                 72.2\n",
            "max               364836.0      8725.0         32287.0      685.0                2241.0                  85.0             100.0                  68.4       0.3           0.6                 92.0             96.0             100.0           79.0                    88.0        78.0                 97.0\n",
            "\n",
            "============================================================\n",
            "Article Class Distribution\n",
            "============================================================\n",
            "Article Class\n",
            "C             9\n",
            "START         9\n",
            "Unassessed    6\n",
            "B             2\n",
            "STUB          2\n",
            "LIST          2\n",
            "Name: count, dtype: int64\n",
            "\n",
            "============================================================\n",
            "TOP 5 ARTICLES BY EACH SCORE:\n",
            "============================================================\n",
            "\n",
            "Most Collaborative:\n",
            "               Article  Collaboration Score  Total Editors  Talk Page Size\n",
            "26    Bethany Hamilton                   92            283           15983\n",
            "16       Crowd surfing                   88            292           11512\n",
            "4           Surf music                   87            216           32287\n",
            "9   History of surfing                   83            259           12773\n",
            "12       Train surfing                   83            239           13019\n",
            "\n",
            "Most Alive:\n",
            "                                      Article  Aliveness Score  Days Since Last Edit  Edits Last Year\n",
            "8                              World Wide Web               96                     3              208\n",
            "26                           Bethany Hamilton               92                     2               57\n",
            "3                                     Surfing               89                     5               72\n",
            "19  List of train-surfing injuries and deaths               83                    23              133\n",
            "13                               Snowboarding               82                     7               39\n",
            "\n",
            "Most Popular:\n",
            "             Article  Popularity Score  Last 3 Months Views\n",
            "8     World Wide Web               100               364836\n",
            "26  Bethany Hamilton                97                95605\n",
            "13      Snowboarding                93                90498\n",
            "3            Surfing                90                44032\n",
            "27        Tia Blanco                87                30851\n",
            "\n",
            "Highest Quality:\n",
            "                                      Article  Quality Score  Citations  Source Quality Score  Neutrality Score\n",
            "19  List of train-surfing injuries and deaths             79        685                  55.4             100.0\n",
            "8                              World Wide Web             67        143                  65.7              97.5\n",
            "27                                 Tia Blanco             64         15                  45.0             100.0\n",
            "13                               Snowboarding             63         78                  67.3              89.9\n",
            "22       Shoulder surfing (computer security)             63         17                  72.3             100.0\n",
            "\n",
            "Best Scholarly Sources:\n",
            "           Article  Scholarly Source Score  Journal Sources  Book Sources\n",
            "3          Surfing                      88               12            15\n",
            "8   World Wide Web                      87               13            16\n",
            "13    Snowboarding                      83               10             7\n",
            "5     Surf culture                      77                5            51\n",
            "4       Surf music                      76                3            33\n",
            "\n",
            "Most Neutral (NPOV):\n",
            "                                      Article  NPOV Score  Neutrality Score\n",
            "19  List of train-surfing injuries and deaths          78             100.0\n",
            "21                           Shoulder surfing          77             100.0\n",
            "23                        Surfing on a Rocket          76             100.0\n",
            "11                                Car surfing          75             100.0\n",
            "16                              Crowd surfing          74             100.0\n",
            "\n",
            "Most Accessible:\n",
            "                                      Article  Accessibility Score          Reading Level  Flesch-Kincaid Grade\n",
            "21                           Shoulder surfing                   97                Unknown                   0.0\n",
            "19  List of train-surfing injuries and deaths                   93       College Graduate                   9.6\n",
            "27                                 Tia Blanco                   90                College                  10.0\n",
            "15                             Tanker surfing                   87  High School (10-12th)                  10.1\n",
            "23                        Surfing on a Rocket                   83                College                  10.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55pz5tiL0nGp"
      },
      "source": [
        "# Ignore these:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta, timezone\n",
        "from urllib.parse import quote\n",
        "import time\n",
        "import re\n",
        "from IPython.display import display\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from textblob import TextBlob\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "import textstat\n",
        "\n",
        "EMAIL = \"ajose3@hawk.illinoistech.edu\" #@param {type:\"string\"}\n",
        "\n",
        "HEADERS = {\n",
        "    \"User-Agent\": f\"Wikiproject_article_finder/1.0 (Educational research; {EMAIL}; Python/requests)\"\n",
        "}\n",
        "\n",
        "SEARCH_QUERY = \"feminist theory\" #@param {type:\"string\"}\n",
        "MAX_ARTICLES = \"10\" #@param {type:\"string\"}\n",
        "\n",
        "try:\n",
        "  MAX_ARTICLES = int(MAX_ARTICLES)\n",
        "except (ValueError, TypeError):\n",
        "  MAX_ARTICLES = None\n",
        "\n",
        "LANG = \"en\"\n",
        "REQUEST_DELAY = 0.03\n",
        "CHECKPOINT_INTERVAL = 100\n",
        "MAX_WORKERS = 3\n",
        "\n",
        "def search_wikipedia_articles(query, max_articles=None):\n",
        "    \"\"\"Search Wikipedia for articles containing specific keywords.\"\"\"\n",
        "    url = \"https://en.wikipedia.org/w/api.php\"\n",
        "    print(f\"Searching for articles related to: '{query}'\")\n",
        "\n",
        "    titles = []\n",
        "    offset = 0\n",
        "\n",
        "    while True:\n",
        "        params = {\n",
        "            \"action\": \"query\",\n",
        "            \"list\": \"search\",\n",
        "            \"srsearch\": query,\n",
        "            \"srlimit\": 50,\n",
        "            \"sroffset\": offset,\n",
        "            \"format\": \"json\"\n",
        "        }\n",
        "\n",
        "        r = requests.get(url, params=params, headers=HEADERS, timeout=30)\n",
        "\n",
        "        if not r.headers.get(\"Content-Type\", \"\").startswith(\"application/json\"):\n",
        "            print(\"Non-JSON response, retrying...\")\n",
        "            time.sleep(2)\n",
        "            continue\n",
        "\n",
        "        data = r.json()\n",
        "        results = data.get(\"query\", {}).get(\"search\", [])\n",
        "\n",
        "        if not results:\n",
        "            break\n",
        "\n",
        "        print(f\"  Found {len(results)} results at offset {offset}\")\n",
        "\n",
        "        for result in results:\n",
        "            title = result[\"title\"]\n",
        "            titles.append(title)\n",
        "\n",
        "            if max_articles and len(titles) >= max_articles:\n",
        "                print(f\"✓ Reached limit of {max_articles} articles\")\n",
        "                return titles[:max_articles]\n",
        "\n",
        "        if \"continue\" in data:\n",
        "            offset = data[\"continue\"][\"sroffset\"]\n",
        "            time.sleep(REQUEST_DELAY)\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    print(f\"✓ Total articles found: {len(titles)}\")\n",
        "    return titles\n",
        "\n",
        "def get_article_class(title):\n",
        "    \"\"\"Get Wikipedia article quality class (FA, GA, B, C, Start, Stub, etc.)\"\"\"\n",
        "    url = \"https://en.wikipedia.org/w/api.php\"\n",
        "\n",
        "    try:\n",
        "        # Get talk page content\n",
        "        params = {\n",
        "            \"action\": \"query\",\n",
        "            \"titles\": f\"Talk:{title}\",\n",
        "            \"prop\": \"revisions\",\n",
        "            \"rvprop\": \"content\",\n",
        "            \"rvslots\": \"main\",\n",
        "            \"format\": \"json\"\n",
        "        }\n",
        "\n",
        "        r = requests.get(url, params=params, headers=HEADERS, timeout=30)\n",
        "        if not r.headers.get(\"Content-Type\", \"\").startswith(\"application/json\"):\n",
        "            return \"Unknown\"\n",
        "\n",
        "        data = r.json()\n",
        "        page = next(iter(data.get(\"query\", {}).get(\"pages\", {}).values()))\n",
        "\n",
        "        # Check if talk page exists\n",
        "        if int(page.get(\"pageid\", -1)) <= 0:\n",
        "            return \"Unassessed\"\n",
        "\n",
        "        # Get talk page wikitext\n",
        "        talk_text = page.get(\"revisions\", [{}])[0].get(\"slots\", {}).get(\"main\", {}).get(\"*\", \"\")\n",
        "\n",
        "        if not talk_text:\n",
        "            return \"Unassessed\"\n",
        "\n",
        "        # Look for WikiProject quality assessments\n",
        "        # Common patterns: {{WikiProject|class=FA}}, {{class|FA}}, |class=FA\n",
        "        class_patterns = [\n",
        "            r'\\|\\s*class\\s*=\\s*([A-Z][A-Za-z]*)',  # |class=FA or |class=Start\n",
        "            r'{{[Cc]lass\\|([A-Z][A-Za-z]*)',        # {{class|FA}}\n",
        "            r'{{WikiProject[^}]*\\|\\s*class\\s*=\\s*([A-Z][A-Za-z]*)',  # {{WikiProject|class=FA}}\n",
        "        ]\n",
        "\n",
        "        for pattern in class_patterns:\n",
        "            matches = re.findall(pattern, talk_text, re.IGNORECASE)\n",
        "            if matches:\n",
        "                article_class = matches[0].upper()\n",
        "\n",
        "                # Normalize to standard classes\n",
        "                valid_classes = ['FA', 'FL', 'A', 'GA', 'B', 'C', 'START', 'STUB', 'LIST']\n",
        "\n",
        "                if article_class in valid_classes:\n",
        "                    return article_class\n",
        "                elif article_class in ['FEATURED', 'FA-CLASS']:\n",
        "                    return 'FA'\n",
        "                elif article_class in ['GOOD', 'GA-CLASS']:\n",
        "                    return 'GA'\n",
        "                elif article_class in ['B-CLASS']:\n",
        "                    return 'B'\n",
        "                elif article_class in ['C-CLASS']:\n",
        "                    return 'C'\n",
        "                elif article_class in ['START-CLASS']:\n",
        "                    return 'START'\n",
        "                elif article_class in ['STUB-CLASS']:\n",
        "                    return 'STUB'\n",
        "\n",
        "        return \"Unassessed\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting class for {title}: {e}\")\n",
        "        return \"Unknown\"\n",
        "\n",
        "def get_all_article_data(title):\n",
        "    \"\"\"Fetch ALL data for an article in one go to minimize API calls.\"\"\"\n",
        "    url = \"https://en.wikipedia.org/w/api.php\"\n",
        "\n",
        "    params = {\n",
        "        \"action\": \"query\",\n",
        "        \"titles\": title,\n",
        "        \"redirects\": True,\n",
        "        \"prop\": \"revisions|extracts|images|categories\",\n",
        "        \"rvprop\": \"content|timestamp\",\n",
        "        \"rvslots\": \"main\",\n",
        "        \"explaintext\": True,\n",
        "        \"exlimit\": 1,\n",
        "        \"imlimit\": 500,\n",
        "        \"cllimit\": 500,\n",
        "        \"format\": \"json\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        r = requests.get(url, params=params, headers=HEADERS, timeout=30)\n",
        "        if not r.headers.get(\"Content-Type\", \"\").startswith(\"application/json\"):\n",
        "            return None\n",
        "\n",
        "        data = r.json()\n",
        "        page = next(iter(data[\"query\"][\"pages\"].values()))\n",
        "\n",
        "        wikitext = page.get(\"revisions\", [{}])[0].get(\"slots\", {}).get(\"main\", {}).get(\"*\", \"\")\n",
        "        plaintext = page.get(\"extract\", \"\")\n",
        "        timestamp = page.get(\"revisions\", [{}])[0].get(\"timestamp\", \"\")\n",
        "        images = len(page.get(\"images\", []))\n",
        "        categories = len(page.get(\"categories\", []))\n",
        "\n",
        "        return {\n",
        "            \"wikitext\": wikitext,\n",
        "            \"plaintext\": plaintext,\n",
        "            \"timestamp\": timestamp,\n",
        "            \"images\": images,\n",
        "            \"categories\": categories\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching data for {title}: {e}\")\n",
        "        return None\n",
        "\n",
        "def analyze_from_cached_data(title, cached_data):\n",
        "    \"\"\"Run all analyses using cached data\"\"\"\n",
        "    if not cached_data:\n",
        "        return get_empty_metrics()\n",
        "\n",
        "    wikitext = cached_data[\"wikitext\"]\n",
        "    plaintext = cached_data[\"plaintext\"]\n",
        "    timestamp = cached_data[\"timestamp\"]\n",
        "\n",
        "    metrics = {}\n",
        "\n",
        "    days_since_edit = 0\n",
        "    if timestamp:\n",
        "        last_edit = datetime.strptime(timestamp, \"%Y-%m-%dT%H:%M:%SZ\")\n",
        "        days_since_edit = round((datetime.now(timezone.utc).replace(tzinfo=None) - last_edit).days)\n",
        "\n",
        "    word_count = len(plaintext.split()) if plaintext else 0\n",
        "    citation_needed_count = wikitext.lower().count(\"citation needed\")\n",
        "\n",
        "    metrics[\"Days Since Last Edit\"] = days_since_edit\n",
        "    metrics[\"Word Count\"] = word_count\n",
        "    metrics[\"Citations Needed\"] = citation_needed_count\n",
        "    metrics[\"Images\"] = cached_data[\"images\"]\n",
        "    metrics[\"Categories\"] = cached_data[\"categories\"]\n",
        "\n",
        "    source_metrics = analyze_source_quality_from_text(wikitext)\n",
        "    metrics.update(source_metrics)\n",
        "\n",
        "    neutrality_metrics = analyze_neutrality_from_text(plaintext)\n",
        "    metrics.update(neutrality_metrics)\n",
        "\n",
        "    readability_metrics = analyze_readability_from_text(plaintext)\n",
        "    metrics.update(readability_metrics)\n",
        "\n",
        "    sentiment_metrics = detect_sentiment_bias_from_text(plaintext)\n",
        "    metrics.update(sentiment_metrics)\n",
        "\n",
        "    metrics[\"Citations\"] = get_citation_count_from_text(wikitext)\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def analyze_source_quality_from_text(wikitext):\n",
        "    \"\"\"Analyze source quality from wikitext - works with all citation formats\"\"\"\n",
        "\n",
        "    cite_patterns = {\n",
        "        \"journal\": r'{{cite journal',\n",
        "        \"book\": r'{{cite book',\n",
        "        \"web\": r'{{cite web',\n",
        "        \"news\": r'{{cite news',\n",
        "    }\n",
        "\n",
        "    source_types = {k: len(re.findall(v, wikitext, re.IGNORECASE)) for k, v in cite_patterns.items()}\n",
        "    total_typed_sources = sum(source_types.values())\n",
        "\n",
        "    years = []\n",
        "    current_year = datetime.now().year\n",
        "\n",
        "    citation_blocks = re.findall(r'{{cite[^}]+}}', wikitext, re.IGNORECASE | re.DOTALL)\n",
        "    for block in citation_blocks:\n",
        "        year_matches = re.findall(r'\\|(?:year|date|publication-date|access-date)\\s*=\\s*[^\\d]*(\\d{4})', block, re.IGNORECASE)\n",
        "        for year_str in year_matches:\n",
        "            year = int(year_str)\n",
        "            if 1800 <= year <= current_year:\n",
        "                years.append(year)\n",
        "                break\n",
        "\n",
        "    ref_blocks = re.findall(r'<ref[^>]*>(.*?)</ref>', wikitext, re.IGNORECASE | re.DOTALL)\n",
        "    for ref in ref_blocks:\n",
        "        year_patterns = [\n",
        "            r'\\((\\d{4})\\)',\n",
        "            r'[,\\s](\\d{4})[,\\.\\s]',\n",
        "            r'\"(\\d{4})\"',\n",
        "            r'(\\d{4})-\\d{2}-\\d{2}',\n",
        "        ]\n",
        "\n",
        "        for pattern in year_patterns:\n",
        "            year_matches = re.findall(pattern, ref)\n",
        "            if year_matches:\n",
        "                year = int(year_matches[0])\n",
        "                if 1800 <= year <= current_year:\n",
        "                    years.append(year)\n",
        "                    break\n",
        "\n",
        "    short_footnotes = re.findall(r'{{(?:sfn|harvnb|harv)[^}]*\\|[^}]*?(\\d{4})', wikitext, re.IGNORECASE)\n",
        "    for year_str in short_footnotes:\n",
        "        year = int(year_str)\n",
        "        if 1800 <= year <= current_year:\n",
        "            years.append(year)\n",
        "\n",
        "    citation_templates = re.findall(r'{{citation[^}]+}}', wikitext, re.IGNORECASE | re.DOTALL)\n",
        "    for block in citation_templates:\n",
        "        year_matches = re.findall(r'\\|(?:year|date|publication-date)\\s*=\\s*[^\\d]*(\\d{4})', block, re.IGNORECASE)\n",
        "        for year_str in year_matches:\n",
        "            year = int(year_str)\n",
        "            if 1800 <= year <= current_year:\n",
        "                years.append(year)\n",
        "                break\n",
        "\n",
        "    recent_sources = len([y for y in years if current_year - y <= 5]) if years else 0\n",
        "    avg_source_age = (current_year - sum(years) / len(years)) if years else 0\n",
        "\n",
        "    quality_score = 0\n",
        "\n",
        "    if total_typed_sources > 0:\n",
        "        scholarly_ratio = (source_types[\"journal\"] + source_types[\"book\"]) / total_typed_sources\n",
        "        quality_score += scholarly_ratio * 50\n",
        "\n",
        "    source_diversity = len([v for v in source_types.values() if v > 0])\n",
        "    quality_score += source_diversity * 10\n",
        "\n",
        "    if avg_source_age < 10:\n",
        "        quality_score += 25\n",
        "    elif avg_source_age < 20:\n",
        "        quality_score += 15\n",
        "\n",
        "    return {\n",
        "        \"Journal Sources\": source_types[\"journal\"],\n",
        "        \"Book Sources\": source_types[\"book\"],\n",
        "        \"Web Sources\": source_types[\"web\"],\n",
        "        \"News Sources\": source_types[\"news\"],\n",
        "        \"Avg Source Age\": round(avg_source_age, 1),\n",
        "        \"Recent Sources (5yr)\": recent_sources,\n",
        "        \"Source Quality Score\": round(min(quality_score, 100), 1)\n",
        "    }\n",
        "\n",
        "def analyze_neutrality_from_text(text):\n",
        "    \"\"\"Detect POV/bias issues\"\"\"\n",
        "    if not text:\n",
        "        return {\n",
        "            \"Hedging Words\": 0,\n",
        "            \"Peacock Words\": 0,\n",
        "            \"Weasel Words\": 0,\n",
        "            \"Value Judgments\": 0,\n",
        "            \"Neutrality Score\": 100\n",
        "        }\n",
        "\n",
        "    text_lower = text.lower()\n",
        "\n",
        "    peacock_terms = [\n",
        "        \"legendary\", \"iconic\", \"acclaimed\", \"prestigious\", \"renowned\",\n",
        "        \"celebrated\", \"world-class\", \"premier\", \"leading\", \"foremost\",\n",
        "        \"groundbreaking\", \"seminal\", \"pivotal\", \"revolutionary\", \"extraordinary\",\n",
        "        \"remarkable\", \"outstanding\", \"exceptional\", \"unparalleled\", \"best\"\n",
        "    ]\n",
        "    peacock_count = sum(text_lower.count(term) for term in peacock_terms)\n",
        "\n",
        "    weasel_patterns = [\n",
        "        r'\\bsome (people|experts|scholars|critics|observers|say|believe)',\n",
        "        r'\\bmany (believe|argue|claim|suggest|think|feel)',\n",
        "        r'\\bit (is said|has been said|is believed|is widely|is commonly)',\n",
        "        r'\\bmost (people|experts|scholars)',\n",
        "        r'\\bwidely (regarded|considered|accepted|believed)',\n",
        "        r'\\boften (considered|regarded|viewed)',\n",
        "        r'\\bgenerally (accepted|believed|considered)',\n",
        "    ]\n",
        "    weasel_count = sum(len(re.findall(pattern, text_lower)) for pattern in weasel_patterns)\n",
        "\n",
        "    hedging_words = [\"perhaps\", \"possibly\", \"maybe\", \"might\", \"could\", \"may\", \"seemingly\"]\n",
        "    hedging_count = sum(text_lower.count(word) for word in hedging_words)\n",
        "\n",
        "    value_words = [\n",
        "        \"unfortunately\", \"fortunately\", \"clearly\", \"obviously\", \"naturally\",\n",
        "        \"of course\", \"undoubtedly\", \"certainly\", \"arguably\", \"notably\",\n",
        "        \"importantly\", \"surprisingly\", \"interestingly\", \"regrettably\"\n",
        "    ]\n",
        "    value_count = sum(text_lower.count(word) for word in value_words)\n",
        "\n",
        "    word_count = len(text.split())\n",
        "    neutrality_score = 100\n",
        "\n",
        "    if word_count > 0:\n",
        "        neutrality_score -= (peacock_count / word_count * 1000) * 10\n",
        "        neutrality_score -= (weasel_count / word_count * 1000) * 15\n",
        "        neutrality_score -= (value_count / word_count * 1000) * 8\n",
        "\n",
        "    neutrality_score = max(0, min(100, neutrality_score))\n",
        "\n",
        "    return {\n",
        "        \"Hedging Words\": hedging_count,\n",
        "        \"Peacock Words\": peacock_count,\n",
        "        \"Weasel Words\": weasel_count,\n",
        "        \"Value Judgments\": value_count,\n",
        "        \"Neutrality Score\": round(neutrality_score, 1)\n",
        "    }\n",
        "\n",
        "def analyze_readability_from_text(text):\n",
        "    \"\"\"Calculate reading level\"\"\"\n",
        "    if not text or len(text) < 100:\n",
        "        return {\n",
        "            \"Flesch-Kincaid Grade\": 0,\n",
        "            \"Reading Level\": \"Unknown\"\n",
        "        }\n",
        "\n",
        "    try:\n",
        "        flesch_grade = textstat.flesch_kincaid_grade(text)\n",
        "        flesch_ease = textstat.flesch_reading_ease(text)\n",
        "\n",
        "        if flesch_ease >= 90:\n",
        "            level = \"Elementary (5th grade)\"\n",
        "        elif flesch_ease >= 80:\n",
        "            level = \"Middle School (6-7th)\"\n",
        "        elif flesch_ease >= 70:\n",
        "            level = \"High School (8-9th)\"\n",
        "        elif flesch_ease >= 60:\n",
        "            level = \"High School (10-12th)\"\n",
        "        elif flesch_ease >= 50:\n",
        "            level = \"College\"\n",
        "        elif flesch_ease >= 30:\n",
        "            level = \"College Graduate\"\n",
        "        else:\n",
        "            level = \"Professional/Academic\"\n",
        "\n",
        "        return {\n",
        "            \"Flesch-Kincaid Grade\": round(flesch_grade, 1),\n",
        "            \"Reading Level\": level\n",
        "        }\n",
        "    except:\n",
        "        return {\n",
        "            \"Flesch-Kincaid Grade\": 0,\n",
        "            \"Reading Level\": \"Error\"\n",
        "        }\n",
        "\n",
        "def detect_sentiment_bias_from_text(text):\n",
        "    \"\"\"Detect sentiment bias\"\"\"\n",
        "    if not text:\n",
        "        return {\n",
        "            \"Polarity\": 0,\n",
        "            \"Subjectivity\": 0,\n",
        "            \"VADER Compound\": 0,\n",
        "            \"Sentiment\": \"Neutral\"\n",
        "        }\n",
        "\n",
        "    try:\n",
        "        blob = TextBlob(text)\n",
        "        polarity = blob.sentiment.polarity\n",
        "        subjectivity = blob.sentiment.subjectivity\n",
        "\n",
        "        analyzer = SentimentIntensityAnalyzer()\n",
        "        vader_scores = analyzer.polarity_scores(text)\n",
        "\n",
        "        if abs(polarity) < 0.1 and subjectivity < 0.3:\n",
        "            sentiment = \"Neutral & Objective\"\n",
        "        elif abs(polarity) < 0.1:\n",
        "            sentiment = \"Neutral but Subjective\"\n",
        "        elif polarity > 0.2:\n",
        "            sentiment = \"Positive Bias Detected\"\n",
        "        elif polarity < -0.2:\n",
        "            sentiment = \"Negative Bias Detected\"\n",
        "        else:\n",
        "            sentiment = \"Slight Bias\"\n",
        "\n",
        "        return {\n",
        "            \"Polarity\": round(polarity, 3),\n",
        "            \"Subjectivity\": round(subjectivity, 3),\n",
        "            \"VADER Compound\": round(vader_scores['compound'], 3),\n",
        "            \"Sentiment\": sentiment\n",
        "        }\n",
        "    except:\n",
        "        return {\n",
        "            \"Polarity\": 0,\n",
        "            \"Subjectivity\": 0,\n",
        "            \"VADER Compound\": 0,\n",
        "            \"Sentiment\": \"Error\"\n",
        "        }\n",
        "\n",
        "def get_citation_count_from_text(wikitext):\n",
        "    \"\"\"Count citations from wikitext\"\"\"\n",
        "    if not wikitext:\n",
        "        return 0\n",
        "\n",
        "    named_refs = set()\n",
        "    unnamed_count = 0\n",
        "    ref_pattern = r'<ref(?:\\s+[^>]*)?>'\n",
        "    all_refs = re.findall(ref_pattern, wikitext, re.IGNORECASE | re.DOTALL)\n",
        "\n",
        "    for ref in all_refs:\n",
        "        if ref.strip().endswith('/>'):\n",
        "            continue\n",
        "        name_match = re.search(r'name\\s*=\\s*[\"\\']([^\"\\']+)[\"\\']', ref, re.IGNORECASE)\n",
        "        if name_match:\n",
        "            named_refs.add(name_match.group(1))\n",
        "        else:\n",
        "            unnamed_count += 1\n",
        "\n",
        "    ref_count = len(named_refs) + unnamed_count\n",
        "\n",
        "    sfn_count = len(re.findall(r'\\{\\{sfn[a-z]*\\|', wikitext, re.IGNORECASE))\n",
        "    harv_count = len(re.findall(r'\\{\\{harv[a-z]*\\|', wikitext, re.IGNORECASE))\n",
        "    r_count = len(re.findall(r'\\{\\{rp?\\|', wikitext, re.IGNORECASE))\n",
        "    efn_count = len(re.findall(r'\\{\\{efn[a-z]*\\|', wikitext, re.IGNORECASE))\n",
        "\n",
        "    footnote_count = sfn_count + efn_count\n",
        "    return max(ref_count, footnote_count, harv_count, r_count)\n",
        "\n",
        "def get_remaining_data(title):\n",
        "    \"\"\"Get data that requires separate API calls\"\"\"\n",
        "    url = \"https://en.wikipedia.org/w/api.php\"\n",
        "\n",
        "    sections = \"\"\n",
        "    try:\n",
        "        params = {\n",
        "            \"action\": \"parse\",\n",
        "            \"page\": title,\n",
        "            \"prop\": \"sections\",\n",
        "            \"redirects\": 1,\n",
        "            \"format\": \"json\"\n",
        "        }\n",
        "        r = requests.get(url, params=params, headers=HEADERS, timeout=30)\n",
        "        if r.headers.get(\"Content-Type\", \"\").startswith(\"application/json\"):\n",
        "            parse_data = r.json().get(\"parse\", {})\n",
        "            if parse_data:\n",
        "                section_list = parse_data.get(\"sections\", [])\n",
        "                sections = \", \".join(s[\"line\"] for s in section_list)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    talk_page_size = 0\n",
        "    try:\n",
        "        params = {\n",
        "            \"action\": \"query\",\n",
        "            \"titles\": f\"Talk:{title}\",\n",
        "            \"redirects\": True,\n",
        "            \"prop\": \"revisions\",\n",
        "            \"rvprop\": \"size\",\n",
        "            \"rvlimit\": 1,\n",
        "            \"format\": \"json\"\n",
        "        }\n",
        "        r = requests.get(url, params=params, headers=HEADERS, timeout=30)\n",
        "        if r.headers.get(\"Content-Type\", \"\").startswith(\"application/json\"):\n",
        "            data = r.json()\n",
        "            pages = data.get(\"query\", {}).get(\"pages\", {})\n",
        "            page = next(iter(pages.values()), {})\n",
        "            if int(page.get(\"pageid\", -1)) > 0:\n",
        "                talk_page_size = page.get(\"revisions\", [{}])[0].get(\"size\", 0)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    num_editors, recent_edits = 0, 0\n",
        "    try:\n",
        "        params = {\n",
        "            \"action\": \"query\",\n",
        "            \"titles\": title,\n",
        "            \"redirects\": True,\n",
        "            \"prop\": \"revisions\",\n",
        "            \"rvprop\": \"timestamp|user\",\n",
        "            \"rvlimit\": 500,\n",
        "            \"format\": \"json\"\n",
        "        }\n",
        "        r = requests.get(url, params=params, headers=HEADERS, timeout=30)\n",
        "        if r.headers.get(\"Content-Type\", \"\").startswith(\"application/json\"):\n",
        "            data = r.json()\n",
        "            pages = data.get(\"query\", {}).get(\"pages\", {})\n",
        "            page = next(iter(pages.values()), {})\n",
        "            revisions = page.get(\"revisions\", [])\n",
        "\n",
        "            unique_editors = set()\n",
        "            one_year_ago = datetime.now(timezone.utc) - timedelta(days=365)\n",
        "\n",
        "            for rev in revisions:\n",
        "                user = rev.get(\"user\", \"\")\n",
        "                if user:\n",
        "                    unique_editors.add(user)\n",
        "                timestamp_str = rev.get(\"timestamp\", \"\")\n",
        "                if timestamp_str:\n",
        "                    timestamp = datetime.strptime(timestamp_str, \"%Y-%m-%dT%H:%M:%SZ\").replace(tzinfo=timezone.utc)\n",
        "                    if timestamp >= one_year_ago:\n",
        "                        recent_edits += 1\n",
        "\n",
        "            num_editors = len(unique_editors)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    views = 0\n",
        "    try:\n",
        "        end = datetime.now(timezone.utc).replace(tzinfo=None)\n",
        "        start = end - timedelta(days=90)\n",
        "        encoded_title = quote(title.replace(' ', '_'))\n",
        "        pv_url = (\n",
        "            f\"https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/\"\n",
        "            f\"en.wikipedia/all-access/user/\"\n",
        "            f\"{encoded_title}/daily/\"\n",
        "            f\"{start:%Y%m%d}/{end:%Y%m%d}\"\n",
        "        )\n",
        "        r = requests.get(pv_url, headers=HEADERS, timeout=30)\n",
        "        if r.headers.get(\"Content-Type\", \"\").startswith(\"application/json\"):\n",
        "            data = r.json()\n",
        "            views = sum(d[\"views\"] for d in data.get(\"items\", []))\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    return {\n",
        "        \"Section Names\": sections,\n",
        "        \"Talk Page Size\": talk_page_size,\n",
        "        \"Total Editors\": num_editors,\n",
        "        \"Edits Last Year\": recent_edits,\n",
        "        \"Last 3 Months Views\": views\n",
        "    }\n",
        "\n",
        "def get_empty_metrics():\n",
        "    \"\"\"Return empty metrics dict for failed articles\"\"\"\n",
        "    return {\n",
        "        \"Days Since Last Edit\": 0,\n",
        "        \"Word Count\": 0,\n",
        "        \"Section Names\": \"\",\n",
        "        \"Citations\": 0,\n",
        "        \"Citations Needed\": 0,\n",
        "        \"Images\": 0,\n",
        "        \"Categories\": 0,\n",
        "        \"Total Editors\": 0,\n",
        "        \"Edits Last Year\": 0,\n",
        "        \"Talk Page Size\": 0,\n",
        "        \"Last 3 Months Views\": 0,\n",
        "        \"Journal Sources\": 0,\n",
        "        \"Book Sources\": 0,\n",
        "        \"Web Sources\": 0,\n",
        "        \"News Sources\": 0,\n",
        "        \"Avg Source Age\": 0,\n",
        "        \"Recent Sources (5yr)\": 0,\n",
        "        \"Source Quality Score\": 0,\n",
        "        \"Hedging Words\": 0,\n",
        "        \"Peacock Words\": 0,\n",
        "        \"Weasel Words\": 0,\n",
        "        \"Value Judgments\": 0,\n",
        "        \"Neutrality Score\": 0,\n",
        "        \"Flesch-Kincaid Grade\": 0,\n",
        "        \"Reading Level\": \"Unknown\",\n",
        "        \"Polarity\": 0,\n",
        "        \"Subjectivity\": 0,\n",
        "        \"VADER Compound\": 0,\n",
        "        \"Sentiment\": \"Unknown\",\n",
        "        \"Article Class\": \"Unknown\"\n",
        "    }\n",
        "\n",
        "def process_single_article(title):\n",
        "    \"\"\"Process a single article - to be run in parallel\"\"\"\n",
        "    try:\n",
        "        cached_data = get_all_article_data(title)\n",
        "        metrics = analyze_from_cached_data(title, cached_data)\n",
        "        remaining = get_remaining_data(title)\n",
        "        metrics.update(remaining)\n",
        "        metrics[\"Article\"] = title\n",
        "\n",
        "        # Get article class\n",
        "        article_class = get_article_class(title)\n",
        "        metrics[\"Article Class\"] = article_class\n",
        "\n",
        "        time.sleep(REQUEST_DELAY)\n",
        "        return metrics\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {title}: {e}\")\n",
        "        empty = get_empty_metrics()\n",
        "        empty[\"Article\"] = title\n",
        "        return empty\n",
        "\n",
        "# MAIN EXECUTION\n",
        "print(\"=\"*60)\n",
        "print(f\"Searching Wikipedia for: '{SEARCH_QUERY}'\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "articles = search_wikipedia_articles(SEARCH_QUERY, MAX_ARTICLES)\n",
        "\n",
        "if not articles:\n",
        "    print(\"\\nNo articles found.\")\n",
        "    exit()\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Processing {len(articles)} articles with {MAX_WORKERS} parallel workers...\")\n",
        "print(f\"{'='*60}\\n\")\n",
        "\n",
        "rows = []\n",
        "start_time = time.time()\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
        "    future_to_article = {executor.submit(process_single_article, title): title for title in articles}\n",
        "\n",
        "    for i, future in enumerate(as_completed(future_to_article), 1):\n",
        "        article = future_to_article[future]\n",
        "        try:\n",
        "            result = future.result()\n",
        "            rows.append(result)\n",
        "            print(f\"[{i}/{len(articles)}] Completed: {article}\")\n",
        "\n",
        "            if i % CHECKPOINT_INTERVAL == 0:\n",
        "                df_checkpoint = pd.DataFrame(rows)\n",
        "                timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "                checkpoint_file = f\"{SEARCH_QUERY.replace(' ', '_')}_{timestamp}_checkpoint.csv\"\n",
        "                df_checkpoint.to_csv(checkpoint_file, index=False)\n",
        "                elapsed = time.time() - start_time\n",
        "                remaining_time = (elapsed / i) * (len(articles) - i)\n",
        "                print(f\"Checkpoint saved: {checkpoint_file}\")\n",
        "                print(f\"Elapsed: {elapsed/60:.1f}min | Estimated remaining: {remaining_time/60:.1f}min\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Failed: {article} - {e}\")\n",
        "\n",
        "# REORDER COLUMNS AND DISPLAY\n",
        "if rows:\n",
        "    df_unordered = pd.DataFrame(rows)\n",
        "\n",
        "    column_order = [\n",
        "        \"Article\",\n",
        "        \"Article Class\",\n",
        "        \"Last 3 Months Views\",\n",
        "        \"Word Count\",\n",
        "        \"Talk Page Size\",\n",
        "        \"Days Since Last Edit\",\n",
        "        \"Edits Last Year\",\n",
        "        \"Total Editors\",\n",
        "        \"Images\",\n",
        "        \"Citations\",\n",
        "        \"Citations Needed\",\n",
        "        \"Journal Sources\",\n",
        "        \"Book Sources\",\n",
        "        \"Web Sources\",\n",
        "        \"News Sources\",\n",
        "        \"Avg Source Age\",\n",
        "        \"Recent Sources (5yr)\",\n",
        "        \"Source Quality Score\",\n",
        "        \"Hedging Words\",\n",
        "        \"Peacock Words\",\n",
        "        \"Weasel Words\",\n",
        "        \"Value Judgments\",\n",
        "        \"Neutrality Score\",\n",
        "        \"Flesch-Kincaid Grade\",\n",
        "        \"Reading Level\",\n",
        "        \"Polarity\",\n",
        "        \"Subjectivity\",\n",
        "        \"VADER Compound\",\n",
        "        \"Sentiment\",\n",
        "        \"Categories\",\n",
        "        \"Section Names\"\n",
        "    ]\n",
        "\n",
        "    df = df_unordered[[col for col in column_order if col in df_unordered.columns]]\n",
        "\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    filename = f\"{SEARCH_QUERY.replace(' ', '_')}_{MAX_ARTICLES}.csv\"\n",
        "    df.to_csv(filename, index=False)\n",
        "\n",
        "    try:\n",
        "        drive_filename = f\"/content/drive/MyDrive/{SEARCH_QUERY.replace(' ', '_')}_{MAX_ARTICLES}.csv\"\n",
        "        df.to_csv(drive_filename, index=False)\n",
        "        print(f\"Saved to Google Drive: {drive_filename}\")\n",
        "    except:\n",
        "        print(\"Google Drive not mounted, saved locally only\")\n",
        "\n",
        "    elapsed_total = time.time() - start_time\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Successfully saved {len(df)} articles to: {filename}\")\n",
        "    print(f\"Total time: {elapsed_total/60:.1f} minutes ({elapsed_total/len(df):.2f}s per article)\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n",
        "    from itables import init_notebook_mode, show\n",
        "    init_notebook_mode(all_interactive=True)\n",
        "\n",
        "    print(\"Interactive Data Table\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"How to use:\")\n",
        "    print(\"SEARCH: Use the search box to filter across all columns\")\n",
        "    print(\"SORT: Click column headers to sort (shift+click for multi-column)\")\n",
        "    print(\"COLUMNS: Click 'Column visibility' button to show/hide columns\")\n",
        "    print(\"PAGES: Use dropdown to change rows per page (10/25/50/100)\")\n",
        "    print(\"EXPORT: Click 'CSV' or 'Excel' to download\")\n",
        "    print(\"=\"*60)\n",
        "    print()\n",
        "\n",
        "    show(df,\n",
        "        scrollX=True,\n",
        "        scrollY=\"600px\",\n",
        "        paging=True,\n",
        "        lengthMenu=[10, 25, 50, 100],\n",
        "        pageLength=25,\n",
        "        buttons=['copy', 'csv', 'excel', 'colvis'],\n",
        "        order=[[2, 'desc']],\n",
        "        columnDefs=[{\"className\": \"dt-left\", \"targets\": \"_all\"}]\n",
        "    )\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"Summary Statistics\")\n",
        "    print(f\"{'='*60}\")\n",
        "    numeric_cols = ['Last 3 Months Views', 'Word Count', 'Talk Page Size', 'Citations',\n",
        "                   'Days Since Last Edit', 'Source Quality Score', 'Neutrality Score',\n",
        "                   'Flesch-Kincaid Grade', 'Polarity', 'Subjectivity']\n",
        "    available_cols = [col for col in numeric_cols if col in df.columns]\n",
        "    print(df[available_cols].describe().round(1))\n",
        "\n",
        "    # Show article class distribution\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"Article Class Distribution\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(df['Article Class'].value_counts())\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo data to save\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "raE9fq6S3WLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2d9Y2nsj8gAS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "pd.set_option('display.width', 1000)\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "# Load the CSV file\n",
        "path = \"/content/feminist_theory_10.csv\" #@param {type:\"string\"}\n",
        "df = pd.read_csv(path)\n",
        "\n",
        "# Function to calculate percentile rank (0-100 scale)\n",
        "def percentile_rank(series):\n",
        "    \"\"\"Convert values to percentile ranks (0-100)\"\"\"\n",
        "    return series.rank(pct=True) * 100\n",
        "\n",
        "# ============================================\n",
        "# CREATE LINKS FIRST (before percentile calculations)\n",
        "# ============================================\n",
        "\n",
        "# Wikipedia link\n",
        "df['Wikipedia Link'] = df['Article'].apply(\n",
        "    lambda x: f\"https://en.wikipedia.org/wiki/{x.replace(' ', '_')}\"\n",
        ")\n",
        "\n",
        "# IIT Library search link\n",
        "df['IIT Library Link'] = df[\"Article\"].str.replace(' ', '%20').apply(\n",
        "    lambda x: f\"https://i-share-iit.primo.exlibrisgroup.com/discovery/search?query=any,contains,{x},AND&tab=Everything&search_scope=MyInst_and_CI&sortby=rank&vid=01CARLI_IIT:CARLI_IIT&mfacet=tlevel,include,peer_reviewed,1&lang=en&mode=advanced&offset=0\"\n",
        ")\n",
        "\n",
        "# ============================================\n",
        "# CALCULATE PERCENTILES FOR ALL METRICS\n",
        "# ============================================\n",
        "\n",
        "# Activity/Collaboration metrics\n",
        "df['Days Since Edit Percentile'] = 100 - percentile_rank(df['Days Since Last Edit'])\n",
        "df['Edits Last Year Percentile'] = percentile_rank(df['Edits Last Year'])\n",
        "df['Total Editors Percentile'] = percentile_rank(df['Total Editors'])\n",
        "df['Talk Page Percentile'] = percentile_rank(df['Talk Page Size'])\n",
        "\n",
        "# Popularity\n",
        "df['Views Percentile'] = percentile_rank(df['Last 3 Months Views'])\n",
        "\n",
        "# Content quality\n",
        "df['Word Count Percentile'] = percentile_rank(df['Word Count'])\n",
        "df['Images Percentile'] = percentile_rank(df['Images'])\n",
        "df['Categories Percentile'] = percentile_rank(df['Categories'])\n",
        "df['Citations Percentile'] = percentile_rank(df['Citations'])\n",
        "df['Citation Needed Percentile'] = 100 - percentile_rank(df['Citations Needed'])\n",
        "df['Citation/Word Ratio Percentile'] = percentile_rank(df['Citations'] / df['Word Count'].replace(0, 1))\n",
        "\n",
        "# Source quality metrics\n",
        "df['Source Quality Percentile'] = percentile_rank(df['Source Quality Score'])\n",
        "df['Journal Sources Percentile'] = percentile_rank(df['Journal Sources'])\n",
        "df['Book Sources Percentile'] = percentile_rank(df['Book Sources'])\n",
        "df['Recent Sources Percentile'] = percentile_rank(df['Recent Sources (5yr)'])\n",
        "df['Source Age Percentile'] = 100 - percentile_rank(df['Avg Source Age'])\n",
        "\n",
        "# Neutrality and bias metrics\n",
        "df['Neutrality Percentile'] = percentile_rank(df['Neutrality Score'])\n",
        "df['Peacock Words Percentile'] = 100 - percentile_rank(df['Peacock Words'])\n",
        "df['Weasel Words Percentile'] = 100 - percentile_rank(df['Weasel Words'])\n",
        "df['Value Judgments Percentile'] = 100 - percentile_rank(df['Value Judgments'])\n",
        "\n",
        "# Readability metrics\n",
        "df['Reading Level Percentile'] = 100 - percentile_rank(df['Flesch-Kincaid Grade'])\n",
        "\n",
        "# Sentiment metrics\n",
        "df['Polarity Neutrality Percentile'] = 100 - percentile_rank(df['Polarity'].abs())\n",
        "df['Objectivity Percentile'] = 100 - percentile_rank(df['Subjectivity'])\n",
        "\n",
        "# ============================================\n",
        "# CREATE COMPOSITE SCORES\n",
        "# ============================================\n",
        "\n",
        "# Collaboration Score\n",
        "df['Collaboration Score'] = (\n",
        "    (df['Total Editors Percentile'] + df['Talk Page Percentile']) / 2\n",
        ").round().astype(int)\n",
        "\n",
        "# Aliveness Score\n",
        "df['Aliveness Score'] = (\n",
        "    (df['Days Since Edit Percentile'] + df['Edits Last Year Percentile']) / 2\n",
        ").round().astype(int)\n",
        "\n",
        "# Popularity Score\n",
        "df['Popularity Score'] = df['Views Percentile'].round().astype(int)\n",
        "\n",
        "# Quality Score\n",
        "df['Quality Score'] = (\n",
        "    df['Citation/Word Ratio Percentile'] * 0.25 +\n",
        "    df['Images Percentile'] * 0.10 +\n",
        "    df['Categories Percentile'] * 0.10 +\n",
        "    df['Citation Needed Percentile'] * 0.05 +\n",
        "    df['Source Quality Percentile'] * 0.25 +\n",
        "    df['Neutrality Percentile'] * 0.15 +\n",
        "    df['Objectivity Percentile'] * 0.10\n",
        ").round().astype(int)\n",
        "\n",
        "# Scholarly Source Score\n",
        "df['Scholarly Source Score'] = (\n",
        "    df['Source Quality Percentile'] * 0.30 +\n",
        "    df['Journal Sources Percentile'] * 0.30 +\n",
        "    df['Book Sources Percentile'] * 0.20 +\n",
        "    df['Recent Sources Percentile'] * 0.10 +\n",
        "    df['Source Age Percentile'] * 0.10\n",
        ").round().astype(int)\n",
        "\n",
        "# NPOV Score\n",
        "df['NPOV Score'] = (\n",
        "    df['Neutrality Percentile'] * 0.40 +\n",
        "    df['Objectivity Percentile'] * 0.30 +\n",
        "    df['Peacock Words Percentile'] * 0.15 +\n",
        "    df['Weasel Words Percentile'] * 0.15\n",
        ").round().astype(int)\n",
        "\n",
        "# Accessibility Score\n",
        "df['Accessibility Score'] = df['Reading Level Percentile'].round().astype(int)\n",
        "\n",
        "# ============================================\n",
        "# DISPLAY RESULTS\n",
        "# ============================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"COMPOSITE SCORES CALCULATED\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nScore Definitions:\")\n",
        "print(\"- Collaboration: Average of (Total Editors + Talk Page Size)\")\n",
        "print(\"- Aliveness: Average of (Days Since Edit [inverted] + Edits Last Year)\")\n",
        "print(\"- Popularity: Page Views\")\n",
        "print(\"- Quality: Weighted average of content metrics, sources, and neutrality\")\n",
        "print(\"- Scholarly Source: Quality and recency of academic sources\")\n",
        "print(\"- NPOV Score: Neutral Point of View compliance\")\n",
        "print(\"- Accessibility: Reading level\")\n",
        "print(\"\\nAll scores are on a 0-100 percentile scale.\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Display core composite scores with links\n",
        "display(df[['Article', 'Collaboration Score',\n",
        "            'Aliveness Score', 'Popularity Score', 'Quality Score', 'Scholarly Source Score',\n",
        "            'NPOV Score', 'Accessibility Score', 'Wikipedia Link', 'IIT Library Link']])\n",
        "\n",
        "# Summary statistics\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Composite Score Statistics:\")\n",
        "print(\"=\"*60)\n",
        "score_cols = ['Collaboration Score', 'Aliveness Score', 'Popularity Score', 'Quality Score',\n",
        "              'Scholarly Source Score', 'NPOV Score', 'Accessibility Score']\n",
        "print(df[score_cols].describe().round(1).T)\n",
        "\n",
        "# ============================================\n",
        "# TOP ARTICLES BY EACH SCORE\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TOP 5 ARTICLES BY EACH SCORE:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\nMost Collaborative:\")\n",
        "print(df.nlargest(5, 'Collaboration Score')[['Article', 'Collaboration Score', 'Total Editors', 'Talk Page Size']])\n",
        "\n",
        "print(\"\\nMost Alive:\")\n",
        "print(df.nlargest(5, 'Aliveness Score')[['Article', 'Aliveness Score', 'Days Since Last Edit', 'Edits Last Year']])\n",
        "\n",
        "print(\"\\nMost Popular:\")\n",
        "print(df.nlargest(5, 'Popularity Score')[['Article', 'Popularity Score', 'Last 3 Months Views']])\n",
        "\n",
        "print(\"\\nHighest Quality:\")\n",
        "print(df.nlargest(5, 'Quality Score')[['Article', 'Quality Score', 'Citations', 'Source Quality Score', 'Neutrality Score']])\n",
        "\n",
        "print(\"\\nBest Scholarly Sources:\")\n",
        "print(df.nlargest(5, 'Scholarly Source Score')[['Article', 'Scholarly Source Score', 'Journal Sources', 'Book Sources']])\n",
        "\n",
        "print(\"\\nMost Neutral (NPOV):\")\n",
        "print(df.nlargest(5, 'NPOV Score')[['Article', 'NPOV Score', 'Neutrality Score', 'Objectivity Percentile']])\n",
        "\n",
        "print(\"\\nMost Accessible:\")\n",
        "print(df.nlargest(5, 'Accessibility Score')[['Article', 'Accessibility Score', 'Reading Level', 'Flesch-Kincaid Grade']])\n",
        "\n",
        "# ============================================\n",
        "# SAVE RESULTS\n",
        "# ============================================\n",
        "\n",
        "# Drop percentile columns (keep only final scores)\n",
        "percentile_cols = [col for col in df.columns if 'Percentile' in col]\n",
        "df_final = df.drop(columns=percentile_cols)\n",
        "\n",
        "# Save to local\n",
        "filename_with_scores = path.split('/')[-1].replace('.csv', '_WITH_SCORES.csv')\n",
        "df_final.to_csv(filename_with_scores, index=False)\n",
        "print(f\"\\nSaved locally to: {filename_with_scores}\")\n",
        "\n",
        "# Save to Google Drive\n",
        "try:\n",
        "    drive_filename = \"/content/drive/MyDrive/\" + filename_with_scores\n",
        "    df_final.to_csv(drive_filename, index=False)\n",
        "    print(f\"Saved to Google Drive: {drive_filename}\")\n",
        "except:\n",
        "    print(\"Google Drive not mounted\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "yd5pD4PkxMYT"
      },
      "outputs": [],
      "source": [
        "#combine dfs\n",
        "path1 = \"\" #@param {type:\"string\"}\n",
        "path2 = \"\" #@param {type:\"string\"}\n",
        "# Read CSVs into DataFrame\n",
        "First_Sheet = pd.read_csv(path1)\n",
        "Second_Sheet = pd.read_csv(path2)\n",
        "dfs = [First_Sheet,Second_Sheet]\n",
        "combined_df = pd.concat(dfs, axis=0, ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9KZaqPX9jmQI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "path = \"/content/feminist_theory_10.csv\" #@param {type:\"string\"}\n",
        "# Read CSV into DataFrame\n",
        "df = pd.read_csv(path)\n",
        "show(df,\n",
        "        scrollX=True,\n",
        "        scrollY=\"600px\",\n",
        "        paging=True,\n",
        "        lengthMenu=[10, 25, 50, 100],\n",
        "        pageLength=25,\n",
        "        buttons=['copy', 'csv', 'excel', 'colvis'],\n",
        "        order=[[1, 'desc']],  # Sort by \"Last 3 Months Views\" descending by default\n",
        "        columnDefs=[{\"className\": \"dt-left\", \"targets\": \"_all\"}]\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output, HTML\n",
        "import requests\n",
        "import time\n",
        "import re\n",
        "import random\n",
        "from collections import Counter\n",
        "\n",
        "EMAIL = \"ajose3@hawk.illinoistech.edu\" #@param {type:\"string\"}\n",
        "HEADERS = {\n",
        "    \"User-Agent\": f\"Wikipedia_Topic_Explorer/1.0 (Educational research; {EMAIL}; Python/requests)\"\n",
        "}\n",
        "\n",
        "def search_wikipedia_multiple(query, top_n=3):\n",
        "    \"\"\"\n",
        "    Search Wikipedia, get top 20 results, randomly sample 3 for variety.\n",
        "    \"\"\"\n",
        "    url = \"https://en.wikipedia.org/w/api.php\"\n",
        "\n",
        "    params = {\n",
        "        \"action\": \"query\",\n",
        "        \"list\": \"search\",\n",
        "        \"srsearch\": query,\n",
        "        \"srlimit\": 10,\n",
        "        \"format\": \"json\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        r = requests.get(url, params=params, headers=HEADERS, timeout=30)\n",
        "        data = r.json()\n",
        "\n",
        "        all_results = []\n",
        "        for item in data.get(\"query\", {}).get(\"search\", []):\n",
        "            all_results.append({\n",
        "                \"title\": item[\"title\"],\n",
        "                \"snippet\": item.get(\"snippet\", \"\")\n",
        "            })\n",
        "\n",
        "        if len(all_results) >= top_n:\n",
        "            return random.sample(all_results, top_n)\n",
        "        else:\n",
        "            return all_results\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error searching Wikipedia: {e}\")\n",
        "        return []\n",
        "\n",
        "def get_categories_and_wikiprojects(query, num_articles=10):\n",
        "    \"\"\"\n",
        "    Get most common categories and WikiProjects from search results.\n",
        "    \"\"\"\n",
        "    url = \"https://en.wikipedia.org/w/api.php\"\n",
        "\n",
        "    # Get more articles for better category analysis\n",
        "    search_params = {\n",
        "        \"action\": \"query\",\n",
        "        \"list\": \"search\",\n",
        "        \"srsearch\": query,\n",
        "        \"srlimit\": num_articles,\n",
        "        \"format\": \"json\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        r = requests.get(url, params=search_params, headers=HEADERS, timeout=30)\n",
        "        data = r.json()\n",
        "        article_titles = [item[\"title\"] for item in data.get(\"query\", {}).get(\"search\", [])]\n",
        "\n",
        "        all_categories = []\n",
        "        all_wikiprojects = []\n",
        "\n",
        "        skip_terms = ['stub', 'articles needing', 'articles with', 'pages', 'wikipedia',\n",
        "                     'template', 'all ', 'cs1', 'webarchive', 'coordinates', 'commons',\n",
        "                     'use dmy', 'use mdy', 'living people', 'year', 'births', 'deaths']\n",
        "\n",
        "        for title in article_titles:\n",
        "            # Get categories\n",
        "            cat_params = {\n",
        "                \"action\": \"query\",\n",
        "                \"titles\": title,\n",
        "                \"prop\": \"categories\",\n",
        "                \"cllimit\": 100,\n",
        "                \"format\": \"json\"\n",
        "            }\n",
        "\n",
        "            r = requests.get(url, params=cat_params, headers=HEADERS, timeout=30)\n",
        "            cat_data = r.json()\n",
        "            page = next(iter(cat_data.get(\"query\", {}).get(\"pages\", {}).values()))\n",
        "\n",
        "            for cat in page.get(\"categories\", []):\n",
        "                cat_name = cat[\"title\"].replace(\"Category:\", \"\")\n",
        "                if not any(skip in cat_name.lower() for skip in skip_terms):\n",
        "                    all_categories.append(cat_name)\n",
        "\n",
        "            # Get WikiProjects from talk page\n",
        "            talk_params = {\n",
        "                \"action\": \"query\",\n",
        "                \"titles\": f\"Talk:{title}\",\n",
        "                \"prop\": \"revisions\",\n",
        "                \"rvprop\": \"content\",\n",
        "                \"rvslots\": \"main\",\n",
        "                \"format\": \"json\"\n",
        "            }\n",
        "\n",
        "            r = requests.get(url, params=talk_params, headers=HEADERS, timeout=30)\n",
        "            talk_data = r.json()\n",
        "            talk_page = next(iter(talk_data.get(\"query\", {}).get(\"pages\", {}).values()))\n",
        "\n",
        "            if int(talk_page.get(\"pageid\", -1)) > 0:\n",
        "                talk_text = talk_page.get(\"revisions\", [{}])[0].get(\"slots\", {}).get(\"main\", {}).get(\"*\", \"\")\n",
        "\n",
        "                # Find WikiProject templates\n",
        "                wikiproject_pattern = r'{{WikiProject\\s+([^}|]+)'\n",
        "                projects = re.findall(wikiproject_pattern, talk_text, re.IGNORECASE)\n",
        "                all_wikiprojects.extend(projects)\n",
        "\n",
        "            time.sleep(0.1)\n",
        "\n",
        "        # Count occurrences\n",
        "        category_counts = Counter(all_categories)\n",
        "        wikiproject_counts = Counter(all_wikiprojects)\n",
        "\n",
        "        return {\n",
        "            \"categories\": category_counts.most_common(10),\n",
        "            \"wikiprojects\": wikiproject_counts.most_common(10)\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting categories/projects: {e}\")\n",
        "        return {\"categories\": [], \"wikiprojects\": []}\n",
        "\n",
        "def get_conceptnet_related(query, limit=15):\n",
        "    \"\"\"\n",
        "    Get related concepts from ConceptNet API.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        encoded_query = query.replace(' ', '_').lower()\n",
        "        url = f\"http://api.conceptnet.io/query?node=/c/en/{encoded_query}&limit=100\"\n",
        "\n",
        "        r = requests.get(url, timeout=30)\n",
        "\n",
        "        if r.status_code != 200:\n",
        "            return []\n",
        "\n",
        "        data = r.json()\n",
        "\n",
        "        related_concepts = set()\n",
        "\n",
        "        for edge in data.get(\"edges\", []):\n",
        "            weight = edge.get(\"weight\", 0)\n",
        "\n",
        "            if weight < 1.0:\n",
        "                continue\n",
        "\n",
        "            start = edge.get(\"start\", {})\n",
        "            end = edge.get(\"end\", {})\n",
        "\n",
        "            for node in [start, end]:\n",
        "                label = node.get(\"label\", \"\")\n",
        "                language = node.get(\"language\", \"\")\n",
        "                node_term = node.get(\"term\", \"\")\n",
        "\n",
        "                if language == \"en\":\n",
        "                    if not label and node_term:\n",
        "                        term_parts = node_term.split('/')\n",
        "                        if len(term_parts) >= 3:\n",
        "                            label = term_parts[-1].replace('_', ' ')\n",
        "\n",
        "                    if label:\n",
        "                        clean_label = label.replace(\"_\", \" \").strip()\n",
        "                        if (clean_label.lower() != query.lower() and\n",
        "                            len(clean_label) > 2 and\n",
        "                            clean_label not in related_concepts):\n",
        "                            related_concepts.add(clean_label)\n",
        "\n",
        "        result = sorted(list(related_concepts))[:limit]\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing ConceptNet: {e}\")\n",
        "        return []\n",
        "\n",
        "def make_wikipedia_link(title):\n",
        "    \"\"\"\n",
        "    Create a clickable HTML link to a Wikipedia article.\n",
        "    \"\"\"\n",
        "    url_title = title.replace(' ', '_')\n",
        "    url = f\"https://en.wikipedia.org/wiki/{url_title}\"\n",
        "    return f'<a href=\"{url}\" target=\"_blank\">{title}</a>'\n",
        "\n",
        "def make_category_link(category_name):\n",
        "    \"\"\"\n",
        "    Create a clickable link to a Wikipedia category page.\n",
        "    \"\"\"\n",
        "    url_cat = category_name.replace(' ', '_')\n",
        "    url = f\"https://en.wikipedia.org/wiki/Category:{url_cat}\"\n",
        "    return f'<a href=\"{url}\" target=\"_blank\">{category_name}</a>'\n",
        "\n",
        "def make_wikiproject_link(project_name):\n",
        "    \"\"\"\n",
        "    Create a clickable link to a WikiProject page.\n",
        "    \"\"\"\n",
        "    url_project = project_name.strip().replace(' ', '_')\n",
        "    url = f\"https://en.wikipedia.org/wiki/Wikipedia:WikiProject_{url_project}\"\n",
        "    return f'<a href=\"{url}\" target=\"_blank\">WikiProject {project_name}</a>'\n",
        "\n",
        "def get_see_also_links(article_title, max_links=10):\n",
        "    \"\"\"\n",
        "    Extract links from See Also section.\n",
        "    \"\"\"\n",
        "    url = \"https://en.wikipedia.org/w/api.php\"\n",
        "\n",
        "    parse_params = {\n",
        "        \"action\": \"parse\",\n",
        "        \"page\": article_title,\n",
        "        \"prop\": \"sections|wikitext\",\n",
        "        \"format\": \"json\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        r = requests.get(url, params=parse_params, headers=HEADERS, timeout=30)\n",
        "        data = r.json()\n",
        "\n",
        "        if \"parse\" not in data:\n",
        "            return []\n",
        "\n",
        "        wikitext = data[\"parse\"].get(\"wikitext\", {}).get(\"*\", \"\")\n",
        "\n",
        "        section_pattern = r\"==\\s*See also\\s*==(.*?)(?:==|$)\"\n",
        "        section_match = re.search(section_pattern, wikitext, re.IGNORECASE | re.DOTALL)\n",
        "\n",
        "        if not section_match:\n",
        "            return []\n",
        "\n",
        "        see_also_text = section_match.group(1)\n",
        "        links = re.findall(r'\\[\\[([^]|]+)(?:\\|[^]]+)?\\]\\]', see_also_text)\n",
        "\n",
        "        clean_links = []\n",
        "        for link in links:\n",
        "            if not link.startswith(('Category:', 'File:', 'Image:', 'Wikipedia:')):\n",
        "                clean_links.append(link)\n",
        "\n",
        "        return clean_links[:max_links]\n",
        "\n",
        "    except Exception as e:\n",
        "        return []\n",
        "\n",
        "def get_filtered_links_from_article(article_title, max_links=12):\n",
        "    \"\"\"\n",
        "    Get conceptual links from the article.\n",
        "    \"\"\"\n",
        "    url = \"https://en.wikipedia.org/w/api.php\"\n",
        "\n",
        "    links_params = {\n",
        "        \"action\": \"query\",\n",
        "        \"titles\": article_title,\n",
        "        \"prop\": \"links\",\n",
        "        \"pllimit\": 300,\n",
        "        \"plnamespace\": 0,\n",
        "        \"format\": \"json\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        r = requests.get(url, params=links_params, headers=HEADERS, timeout=30)\n",
        "        data = r.json()\n",
        "        page = next(iter(data.get(\"query\", {}).get(\"pages\", {}).values()))\n",
        "\n",
        "        if \"links\" not in page:\n",
        "            return []\n",
        "\n",
        "        all_links = [link[\"title\"] for link in page[\"links\"]]\n",
        "\n",
        "        conceptual_keywords = [\n",
        "            'theory', 'philosophy', 'studies', 'criticism', 'ism',\n",
        "            'epistemology', 'methodology', 'approach', 'framework',\n",
        "            'perspective', 'analysis', 'research', 'science'\n",
        "        ]\n",
        "\n",
        "        exclude_patterns = [\n",
        "            r'^\\d{4}$',\n",
        "            r'List of',\n",
        "            r'Index of',\n",
        "            r'^[A-Z]{2,4}$',\n",
        "            r'University',\n",
        "            r'Press$',\n",
        "            r'Publishing',\n",
        "            r'Books$',\n",
        "            r'ISBN'\n",
        "        ]\n",
        "\n",
        "        filtered = []\n",
        "        for link in all_links:\n",
        "            if any(re.search(pattern, link) for pattern in exclude_patterns):\n",
        "                continue\n",
        "\n",
        "            if any(keyword in link.lower() for keyword in conceptual_keywords):\n",
        "                filtered.append(link)\n",
        "\n",
        "        if len(filtered) > max_links:\n",
        "            return random.sample(filtered, max_links)\n",
        "        return filtered\n",
        "\n",
        "    except Exception as e:\n",
        "        return []\n",
        "\n",
        "def get_category_siblings(article_title, max_results=10):\n",
        "    \"\"\"\n",
        "    Get other articles in the same meaningful categories.\n",
        "    \"\"\"\n",
        "    url = \"https://en.wikipedia.org/w/api.php\"\n",
        "\n",
        "    cat_params = {\n",
        "        \"action\": \"query\",\n",
        "        \"titles\": article_title,\n",
        "        \"prop\": \"categories\",\n",
        "        \"cllimit\": 50,\n",
        "        \"format\": \"json\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        r = requests.get(url, params=cat_params, headers=HEADERS, timeout=30)\n",
        "        cat_data = r.json()\n",
        "        page = next(iter(cat_data.get(\"query\", {}).get(\"pages\", {}).values()))\n",
        "        categories = [cat[\"title\"] for cat in page.get(\"categories\", [])]\n",
        "\n",
        "        skip_terms = ['stub', 'articles', 'pages', 'wikipedia', 'template',\n",
        "                     'all ', 'cs1', 'webarchive', 'coordinates', 'commons',\n",
        "                     'use dmy', 'use mdy', 'living', 'year', 'century']\n",
        "\n",
        "        meaningful_cats = []\n",
        "        for cat in categories:\n",
        "            if not any(skip in cat.lower() for skip in skip_terms):\n",
        "                meaningful_cats.append(cat)\n",
        "\n",
        "        all_siblings = []\n",
        "\n",
        "        for category in meaningful_cats[:3]:\n",
        "            member_params = {\n",
        "                \"action\": \"query\",\n",
        "                \"list\": \"categorymembers\",\n",
        "                \"cmtitle\": category,\n",
        "                \"cmlimit\": 20,\n",
        "                \"cmnamespace\": 0,\n",
        "                \"format\": \"json\"\n",
        "            }\n",
        "\n",
        "            r = requests.get(url, params=member_params, headers=HEADERS, timeout=30)\n",
        "            member_data = r.json()\n",
        "\n",
        "            for member in member_data.get(\"query\", {}).get(\"categorymembers\", []):\n",
        "                title = member[\"title\"]\n",
        "                if title != article_title and title not in all_siblings:\n",
        "                    all_siblings.append(title)\n",
        "\n",
        "            time.sleep(0.1)\n",
        "\n",
        "        if len(all_siblings) > max_results:\n",
        "            return random.sample(all_siblings, max_results)\n",
        "        return all_siblings\n",
        "\n",
        "    except Exception as e:\n",
        "        return []\n",
        "\n",
        "def explore_topic_multiple_paths(query):\n",
        "    \"\"\"\n",
        "    Get related topics from multiple sources and pathways.\n",
        "    \"\"\"\n",
        "    wiki_results = search_wikipedia_multiple(query, top_n=3)\n",
        "    conceptnet_related = get_conceptnet_related(query, limit=12)\n",
        "    cat_and_projects = get_categories_and_wikiprojects(query, num_articles=15)\n",
        "\n",
        "    pathways = []\n",
        "\n",
        "    for i, result in enumerate(wiki_results):\n",
        "        article_title = result[\"title\"]\n",
        "\n",
        "        pathway = {\n",
        "            \"title\": article_title,\n",
        "            \"see_also\": get_see_also_links(article_title, 8),\n",
        "            \"related_concepts\": get_filtered_links_from_article(article_title, 10),\n",
        "            \"category_siblings\": get_category_siblings(article_title, 8)\n",
        "        }\n",
        "\n",
        "        pathways.append(pathway)\n",
        "        time.sleep(0.2)\n",
        "\n",
        "    return {\n",
        "        \"wikipedia_matches\": [r[\"title\"] for r in wiki_results],\n",
        "        \"pathways\": pathways,\n",
        "        \"conceptnet_alternatives\": conceptnet_related,\n",
        "        \"categories\": cat_and_projects[\"categories\"],\n",
        "        \"wikiprojects\": cat_and_projects[\"wikiprojects\"]\n",
        "    }\n",
        "\n",
        "def create_smart_search_explorer():\n",
        "\n",
        "    topic_input = widgets.Text(\n",
        "        value='feminist theory',\n",
        "        placeholder='Enter a topic',\n",
        "        description='Topic:',\n",
        "        style={'description_width': '100px'},\n",
        "        layout=widgets.Layout(width='500px')\n",
        "    )\n",
        "\n",
        "    search_button = widgets.Button(\n",
        "        description='Explore Topic',\n",
        "        button_style='info',\n",
        "        layout=widgets.Layout(width='200px', height='40px')\n",
        "    )\n",
        "\n",
        "    output = widgets.Output()\n",
        "\n",
        "    def on_search_clicked(b):\n",
        "        with output:\n",
        "            clear_output()\n",
        "            query = topic_input.value.strip()\n",
        "\n",
        "            if not query:\n",
        "                print(\"Please enter a topic.\")\n",
        "                return\n",
        "\n",
        "            print(\"TOPIC EXPLORATION\")\n",
        "            print(\"=\"*70)\n",
        "            print(f\"Exploring: '{query}'\")\n",
        "            print(\"=\"*70)\n",
        "            print(\"\\nAnalyzing multiple pathways (30-40 seconds)...\\n\")\n",
        "\n",
        "            results = explore_topic_multiple_paths(query)\n",
        "\n",
        "            html_output = \"<div style='font-family: monospace;'>\"\n",
        "\n",
        "            # Most Common Categories\n",
        "            if results[\"categories\"]:\n",
        "                html_output += \"<h3>MOST COMMON CATEGORIES FOR THIS TOPIC</h3>\"\n",
        "                html_output += \"<hr>\"\n",
        "                html_output += \"<p>These categories appear most frequently across articles about this topic:</p>\"\n",
        "                html_output += \"<ol>\"\n",
        "                for cat_name, count in results[\"categories\"]:\n",
        "                    html_output += f\"<li>{make_category_link(cat_name)} <em>({count} articles)</em></li>\"\n",
        "                html_output += \"</ol>\"\n",
        "\n",
        "            # Most Common WikiProjects\n",
        "            if results[\"wikiprojects\"]:\n",
        "                html_output += \"<h3>MOST ACTIVE WIKIPROJECTS</h3>\"\n",
        "                html_output += \"<hr>\"\n",
        "                html_output += \"<p>These WikiProjects are most involved with articles on this topic:</p>\"\n",
        "                html_output += \"<ol>\"\n",
        "                for project_name, count in results[\"wikiprojects\"]:\n",
        "                    html_output += f\"<li>{make_wikiproject_link(project_name)} <em>({count} articles)</em></li>\"\n",
        "                html_output += \"</ol>\"\n",
        "\n",
        "            # Wikipedia matches\n",
        "            html_output += \"<h3>WIKIPEDIA ARTICLES FOUND</h3>\"\n",
        "            html_output += \"<hr>\"\n",
        "            if results[\"wikipedia_matches\"]:\n",
        "                html_output += \"<ol>\"\n",
        "                for match in results[\"wikipedia_matches\"]:\n",
        "                    html_output += f\"<li>{make_wikipedia_link(match)}</li>\"\n",
        "                html_output += \"</ol>\"\n",
        "            else:\n",
        "                html_output += \"<p>No Wikipedia articles found.</p>\"\n",
        "\n",
        "            # Pathways\n",
        "            for i, pathway in enumerate(results[\"pathways\"], 1):\n",
        "                html_output += f\"<h3>PATHWAY {i}: Based on '{pathway['title']}'</h3>\"\n",
        "                html_output += \"<hr>\"\n",
        "\n",
        "                if pathway[\"see_also\"]:\n",
        "                    html_output += \"<h4>Editor-curated related topics:</h4>\"\n",
        "                    html_output += \"<ol>\"\n",
        "                    for topic in pathway[\"see_also\"]:\n",
        "                        html_output += f\"<li>{make_wikipedia_link(topic)}</li>\"\n",
        "                    html_output += \"</ol>\"\n",
        "\n",
        "                if pathway[\"related_concepts\"]:\n",
        "                    html_output += \"<h4>Related concepts from article:</h4>\"\n",
        "                    html_output += \"<ol>\"\n",
        "                    for topic in pathway[\"related_concepts\"]:\n",
        "                        html_output += f\"<li>{make_wikipedia_link(topic)}</li>\"\n",
        "                    html_output += \"</ol>\"\n",
        "\n",
        "                if pathway[\"category_siblings\"]:\n",
        "                    html_output += \"<h4>Similar topics (same categories):</h4>\"\n",
        "                    html_output += \"<ol>\"\n",
        "                    for topic in pathway[\"category_siblings\"]:\n",
        "                        html_output += f\"<li>{make_wikipedia_link(topic)}</li>\"\n",
        "                    html_output += \"</ol>\"\n",
        "\n",
        "            # ConceptNet alternatives\n",
        "            if results[\"conceptnet_alternatives\"]:\n",
        "                html_output += \"<h3>ALTERNATIVE EXPLORATION ANGLES</h3>\"\n",
        "                html_output += \"<hr>\"\n",
        "                html_output += \"<p>Related concepts from semantic knowledge graph:</p>\"\n",
        "                html_output += \"<ol>\"\n",
        "                for concept in results[\"conceptnet_alternatives\"]:\n",
        "                    html_output += f\"<li>{make_wikipedia_link(concept)}</li>\"\n",
        "                html_output += \"</ol>\"\n",
        "\n",
        "            html_output += \"<hr>\"\n",
        "            html_output += \"<p><strong>Exploration complete. Click any link to open the Wikipedia article.</strong></p>\"\n",
        "            html_output += \"</div>\"\n",
        "\n",
        "            display(HTML(html_output))\n",
        "\n",
        "    search_button.on_click(on_search_clicked)\n",
        "\n",
        "    ui = widgets.VBox([\n",
        "        widgets.HTML(\"<h2>Multi-Path Topic Explorer</h2>\"),\n",
        "        widgets.HTML(\"<p>Discovers related topics, categories, and WikiProjects from Wikipedia and semantic knowledge graphs.</p>\"),\n",
        "        topic_input,\n",
        "        search_button,\n",
        "        output\n",
        "    ])\n",
        "\n",
        "    display(ui)\n",
        "\n",
        "create_smart_search_explorer()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "TlT6XLKO4oEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3iU9TZPB4yhc"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta, timezone\n",
        "from urllib.parse import quote\n",
        "import time\n",
        "import re\n",
        "from IPython.display import display\n",
        "EMAIL = \"ajose3@hawk.illinoistech.edu\" #@param {type:\"string\"}\n",
        "import random\n",
        "\n",
        "HEADERS = {\n",
        "    \"User-Agent\": f\"Wikiproject_article_finder/1.0 (Educational research; {EMAIL}; Python/requests)\"\n",
        "}\n",
        "WIKIPROJECT = \"psychology\" #@param {type:\"string\"}\n",
        "CLASS = \"C-Class\" #@param {type:\"string\"}\n",
        "MAX_ARTICLES = \"0\" #@param {type:\"string\"}\n",
        "try:\n",
        "  MAX_ARTICLES = int(MAX_ARTICLES)  # Try conversion\n",
        "except (ValueError, TypeError):\n",
        "  MAX_ARTICLES = None\n",
        "LANG = \"en\"\n",
        "REQUEST_DELAY = 0.03\n",
        "CHECKPOINT_INTERVAL = 100\n",
        "\n",
        "\n",
        "\n",
        "def get_project_articles(project, klass, MAX_ARTICLES=None):\n",
        "    \"\"\"\n",
        "    Fetch article titles for a given WikiProject and class.\n",
        "    \"\"\"\n",
        "    category = f\"Category:{klass}_{project}_articles\"\n",
        "    url = \"https://en.wikipedia.org/w/api.php\"\n",
        "\n",
        "    print(f\"Fetching from: {category}\")\n",
        "\n",
        "    titles = []\n",
        "    cmcontinue = None\n",
        "    page_count = 0\n",
        "\n",
        "    while True:\n",
        "        page_count += 1\n",
        "        params = {\n",
        "            \"action\": \"query\",\n",
        "            \"list\": \"categorymembers\",\n",
        "            \"cmtitle\": category,\n",
        "            \"cmlimit\": 500,\n",
        "            \"format\": \"json\"\n",
        "        }\n",
        "        if cmcontinue:\n",
        "            params[\"cmcontinue\"] = cmcontinue\n",
        "\n",
        "        r = requests.get(url, params=params, headers=HEADERS, timeout=30)\n",
        "\n",
        "        if not r.headers.get(\"Content-Type\", \"\").startswith(\"application/json\"):\n",
        "            print(\"Non-JSON response, retrying...\")\n",
        "            time.sleep(2)\n",
        "            continue\n",
        "\n",
        "        data = r.json()\n",
        "        members = data.get(\"query\", {}).get(\"categorymembers\", [])\n",
        "\n",
        "        print(f\"  Page {page_count}: Found {len(members)} items\")\n",
        "\n",
        "        for p in members:\n",
        "            title = p[\"title\"]\n",
        "            if title.startswith(\"Talk:\"):\n",
        "                article_title = title[5:]\n",
        "                titles.append(article_title)\n",
        "            elif not title.startswith(\"Category:\"):\n",
        "                titles.append(title)\n",
        "\n",
        "            if MAX_ARTICLES and len(titles) >= MAX_ARTICLES:\n",
        "                unique = sorted(set(titles))[:MAX_ARTICLES]\n",
        "                print(f\"Reached limit of {MAX_ARTICLES} articles\")\n",
        "                return unique\n",
        "\n",
        "        cmcontinue = data.get(\"continue\", {}).get(\"cmcontinue\")\n",
        "        if not cmcontinue:\n",
        "            break\n",
        "\n",
        "        time.sleep(REQUEST_DELAY)\n",
        "\n",
        "    unique_titles = sorted(set(titles))\n",
        "    print(f\"Total unique articles found: {len(unique_titles)}\")\n",
        "\n",
        "    return unique_titles\n",
        "\n",
        "def get_article_metadata(title):\n",
        "    \"\"\"\n",
        "    Get basic metadata: days since edit, word count, citation needed count\n",
        "    \"\"\"\n",
        "    url = \"https://en.wikipedia.org/w/api.php\"\n",
        "    params = {\n",
        "        \"action\": \"query\",\n",
        "        \"titles\": title,\n",
        "        \"redirects\": True,\n",
        "        \"prop\": \"revisions|extracts\",\n",
        "        \"rvprop\": \"content|timestamp\",\n",
        "        \"rvslots\": \"main\",\n",
        "        \"explaintext\": True,\n",
        "        \"exlimit\": 1,\n",
        "        \"format\": \"json\"\n",
        "    }\n",
        "    r = requests.get(url, params=params, headers=HEADERS, timeout=30)\n",
        "    if not r.headers.get(\"Content-Type\", \"\").startswith(\"application/json\"):\n",
        "        return 0, 0, 0\n",
        "\n",
        "    data = r.json()\n",
        "    page = next(iter(data[\"query\"][\"pages\"].values()))\n",
        "\n",
        "    # Calculate days since last edit\n",
        "    last_edit_str = page.get(\"revisions\", [{}])[0].get(\"timestamp\", \"\")\n",
        "    days_since_edit = 0\n",
        "    if last_edit_str:\n",
        "        last_edit = datetime.strptime(last_edit_str, \"%Y-%m-%dT%H:%M:%SZ\")\n",
        "        days_since_edit = round((datetime.now(timezone.utc).replace(tzinfo=None) - last_edit).days)\n",
        "\n",
        "    # Word count\n",
        "    extract = page.get(\"extract\", \"\")\n",
        "    word_count = len(extract.split()) if extract else 0\n",
        "\n",
        "    # Count \"citation needed\"\n",
        "    wikitext = page.get(\"revisions\", [{}])[0].get(\"slots\", {}).get(\"main\", {}).get(\"*\", \"\")\n",
        "    citation_needed_count = wikitext.lower().count(\"citation needed\")\n",
        "\n",
        "    return days_since_edit, word_count, citation_needed_count\n",
        "\n",
        "def get_sections(title):\n",
        "    url = \"https://en.wikipedia.org/w/api.php\"\n",
        "    params = {\n",
        "        \"action\": \"parse\",\n",
        "        \"page\": title,\n",
        "        \"prop\": \"sections\",\n",
        "        \"redirects\": 1,\n",
        "        \"format\": \"json\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        r = requests.get(url, params=params, headers=HEADERS, timeout=30)\n",
        "        if not r.headers.get(\"Content-Type\", \"\").startswith(\"application/json\"):\n",
        "            return \"\"\n",
        "        parse_data = r.json().get(\"parse\", {})\n",
        "        if not parse_data:\n",
        "            return \"\"\n",
        "        sections = parse_data.get(\"sections\", [])\n",
        "        section_names = \", \".join(s[\"line\"] for s in sections)\n",
        "        return section_names\n",
        "    except:\n",
        "        return \"\"\n",
        "\n",
        "def get_citation_count(title):\n",
        "    \"\"\"\n",
        "    Universal citation counter that handles ALL citation formats:\n",
        "    - Standard <ref> tags\n",
        "    - {{sfn}}, {{sfnp}}, {{sfnm}} (short footnotes with variants)\n",
        "    - {{harv}}, {{harvnb}}, {{harvp}}, etc. (Harvard citations)\n",
        "    - {{r}}, {{rp}} (reference shortcuts)\n",
        "    - {{efn}} (explanatory footnotes)\n",
        "    - {{citation needed}} tags\n",
        "    - List-defined references\n",
        "    \"\"\"\n",
        "    url = \"https://en.wikipedia.org/w/api.php\"\n",
        "    params = {\n",
        "        \"action\": \"query\",\n",
        "        \"prop\": \"revisions\",\n",
        "        \"rvprop\": \"content\",\n",
        "        \"rvslots\": \"main\",\n",
        "        \"titles\": title,\n",
        "        \"redirects\": 1,\n",
        "        \"format\": \"json\"\n",
        "    }\n",
        "    try:\n",
        "        r = requests.get(url, params=params, headers=HEADERS, timeout=30)\n",
        "        if not r.headers.get(\"Content-Type\", \"\").startswith(\"application/json\"):\n",
        "            return 0\n",
        "\n",
        "        data = r.json()\n",
        "        pages = data.get(\"query\", {}).get(\"pages\", {})\n",
        "        page = next(iter(pages.values()), {})\n",
        "\n",
        "        if \"revisions\" not in page:\n",
        "            return 0\n",
        "\n",
        "        content = page.get(\"revisions\", [{}])[0].get(\"slots\", {}).get(\"main\", {}).get(\"*\", \"\")\n",
        "\n",
        "        # Method 1: Count standard <ref> tags (unique named refs + unnamed refs)\n",
        "        named_refs = set()\n",
        "        unnamed_count = 0\n",
        "\n",
        "        ref_pattern = r'<ref(?:\\s+[^>]*)?>'\n",
        "        all_refs = re.findall(ref_pattern, content, re.IGNORECASE | re.DOTALL)\n",
        "\n",
        "        for ref in all_refs:\n",
        "            if ref.strip().endswith('/>'):\n",
        "                continue\n",
        "\n",
        "            name_match = re.search(r'name\\s*=\\s*[\"\\']([^\"\\']+)[\"\\']', ref, re.IGNORECASE)\n",
        "            if name_match:\n",
        "                named_refs.add(name_match.group(1))\n",
        "            else:\n",
        "                unnamed_count += 1\n",
        "\n",
        "        ref_count = len(named_refs) + unnamed_count\n",
        "\n",
        "        # Method 2: Count ALL sfn variants (sfn, sfnp, sfnm, sfnmp, etc.)\n",
        "        sfn_pattern = r'\\{\\{sfn[a-z]*\\|'\n",
        "        sfn_count = len(re.findall(sfn_pattern, content, re.IGNORECASE))\n",
        "\n",
        "        # Method 3: Count ALL harv variants (harv, harvnb, harvp, harvtxt, etc.)\n",
        "        harv_pattern = r'\\{\\{harv[a-z]*\\|'\n",
        "        harv_count = len(re.findall(harv_pattern, content, re.IGNORECASE))\n",
        "\n",
        "        # Method 4: Count {{r}} and {{rp}} (reference shortcuts)\n",
        "        r_pattern = r'\\{\\{rp?\\|'\n",
        "        r_count = len(re.findall(r_pattern, content, re.IGNORECASE))\n",
        "\n",
        "        # Method 5: Count {{efn}} (explanatory footnotes)\n",
        "        efn_pattern = r'\\{\\{efn[a-z]*\\|'\n",
        "        efn_count = len(re.findall(efn_pattern, content, re.IGNORECASE))\n",
        "\n",
        "        # Method 6: Count list-defined references\n",
        "        ldr_count = 0\n",
        "        ldr_match = re.search(r'\\{\\{reflist\\|refs=(.*?)\\n\\}\\}', content, re.IGNORECASE | re.DOTALL)\n",
        "        if ldr_match:\n",
        "            ldr_content = ldr_match.group(1)\n",
        "            ldr_count = len(re.findall(r'<ref name=', ldr_content, re.IGNORECASE))\n",
        "\n",
        "        # Combine footnote-style citations (sfn + efn count together, as they're often used together)\n",
        "        footnote_count = sfn_count + efn_count\n",
        "\n",
        "        # Use the highest count from all methods\n",
        "        # (articles typically use ONE main citation style)\n",
        "        total_citations = max(ref_count, footnote_count, harv_count, r_count, ldr_count)\n",
        "\n",
        "        return total_citations\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"   ⚠ Citation error: {e}\")\n",
        "        return 0\n",
        "\n",
        "def get_images_and_categories(title):\n",
        "    \"\"\"\n",
        "    Get image count and category count\n",
        "    \"\"\"\n",
        "    url = \"https://en.wikipedia.org/w/api.php\"\n",
        "    params = {\n",
        "        \"action\": \"query\",\n",
        "        \"titles\": title,\n",
        "        \"redirects\": True,\n",
        "        \"prop\": \"images|categories\",\n",
        "        \"imlimit\": 500,\n",
        "        \"cllimit\": 500,\n",
        "        \"format\": \"json\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        r = requests.get(url, params=params, headers=HEADERS, timeout=30)\n",
        "        if not r.headers.get(\"Content-Type\", \"\").startswith(\"application/json\"):\n",
        "            return 0, 0\n",
        "\n",
        "        data = r.json()\n",
        "        page = next(iter(data[\"query\"][\"pages\"].values()))\n",
        "\n",
        "        images = len(page.get(\"images\", []))\n",
        "        categories = len(page.get(\"categories\", []))\n",
        "\n",
        "        return images, categories\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Images/categories error: {e}\")\n",
        "        return 0, 0\n",
        "\n",
        "def get_edit_statistics(title):\n",
        "    \"\"\"\n",
        "    Get edit statistics: total editors and edits in last year\n",
        "    \"\"\"\n",
        "    url = \"https://en.wikipedia.org/w/api.php\"\n",
        "\n",
        "    params = {\n",
        "        \"action\": \"query\",\n",
        "        \"titles\": title,\n",
        "        \"redirects\": True,\n",
        "        \"prop\": \"revisions\",\n",
        "        \"rvprop\": \"timestamp|user\",\n",
        "        \"rvlimit\": 500,\n",
        "        \"format\": \"json\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        r = requests.get(url, params=params, headers=HEADERS, timeout=30)\n",
        "        if not r.headers.get(\"Content-Type\", \"\").startswith(\"application/json\"):\n",
        "            return 0, 0\n",
        "\n",
        "        data = r.json()\n",
        "        pages = data.get(\"query\", {}).get(\"pages\", {})\n",
        "        page = next(iter(pages.values()), {})\n",
        "\n",
        "        revisions = page.get(\"revisions\", [])\n",
        "\n",
        "        unique_editors = set()\n",
        "        recent_edits = 0\n",
        "        one_year_ago = datetime.now(timezone.utc) - timedelta(days=365)\n",
        "\n",
        "        for rev in revisions:\n",
        "            user = rev.get(\"user\", \"\")\n",
        "            if user:\n",
        "                unique_editors.add(user)\n",
        "\n",
        "            # Count recent edits (last year)\n",
        "            timestamp_str = rev.get(\"timestamp\", \"\")\n",
        "            if timestamp_str:\n",
        "                timestamp = datetime.strptime(timestamp_str, \"%Y-%m-%dT%H:%M:%SZ\").replace(tzinfo=timezone.utc)\n",
        "                if timestamp >= one_year_ago:\n",
        "                    recent_edits += 1\n",
        "\n",
        "        return len(unique_editors), recent_edits\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Edit statistics error: {e}\")\n",
        "        return 0, 0\n",
        "\n",
        "def get_talk_page_size(title):\n",
        "    \"\"\"\n",
        "    Get the size of the talk page in bytes\n",
        "    \"\"\"\n",
        "    url = \"https://en.wikipedia.org/w/api.php\"\n",
        "    params = {\n",
        "        \"action\": \"query\",\n",
        "        \"titles\": f\"Talk:{title}\",\n",
        "        \"redirects\": True,\n",
        "        \"prop\": \"revisions\",\n",
        "        \"rvprop\": \"size\",\n",
        "        \"rvlimit\": 1,\n",
        "        \"format\": \"json\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        r = requests.get(url, params=params, headers=HEADERS, timeout=30)\n",
        "        if not r.headers.get(\"Content-Type\", \"\").startswith(\"application/json\"):\n",
        "            return 0\n",
        "\n",
        "        data = r.json()\n",
        "        pages = data.get(\"query\", {}).get(\"pages\", {})\n",
        "        page = next(iter(pages.values()), {})\n",
        "\n",
        "        # Check if page exists (missing pages have negative IDs)\n",
        "        if int(page.get(\"pageid\", -1)) < 0:\n",
        "            return 0\n",
        "\n",
        "        size = page.get(\"revisions\", [{}])[0].get(\"size\", 0)\n",
        "        return size\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Talk page error: {e}\")\n",
        "        return 0\n",
        "\n",
        "def get_pageviews_3mo(title):\n",
        "    end = datetime.now(timezone.utc).replace(tzinfo=None)\n",
        "    start = end - timedelta(days=90)\n",
        "    encoded_title = quote(title.replace(' ', '_'))\n",
        "    url = (\n",
        "        f\"https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/\"\n",
        "        f\"en.wikipedia/all-access/user/\"\n",
        "        f\"{encoded_title}/daily/\"\n",
        "        f\"{start:%Y%m%d}/{end:%Y%m%d}\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        r = requests.get(url, headers=HEADERS, timeout=30)\n",
        "        if not r.headers.get(\"Content-Type\", \"\").startswith(\"application/json\"):\n",
        "            return 0\n",
        "        data = r.json()\n",
        "        return sum(d[\"views\"] for d in data.get(\"items\", []))\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "# Main execution\n",
        "print(\"=\"*60)\n",
        "print(f\"Fetching {CLASS} {WIKIPROJECT} articles from Wikipedia\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "articles = get_project_articles(WIKIPROJECT, CLASS, MAX_ARTICLES)  # Change to None for all\n",
        "\n",
        "if not articles:\n",
        "    print(\"\\nNo articles found.\")\n",
        "    exit()\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Processing {len(articles)} articles...\")\n",
        "print(f\"Estimated time: {len(articles) * 6 * REQUEST_DELAY / 60:.1f} - {len(articles) * 6 * 0.5 / 60:.1f} minutes\")\n",
        "print(f\"{'='*60}\\n\")\n",
        "\n",
        "rows = []\n",
        "start_time = time.time()\n",
        "\n",
        "for i, title in enumerate(articles, 1):\n",
        "    print(f\"[{i}/{len(articles)}] {title}\")\n",
        "    try:\n",
        "        days_since_edit, word_count, citation_needed = get_article_metadata(title)\n",
        "        sections = get_sections(title)\n",
        "        citations = get_citation_count(title)\n",
        "        views = get_pageviews_3mo(title)\n",
        "        images, categories = get_images_and_categories(title)\n",
        "        num_editors, recent_edits = get_edit_statistics(title)\n",
        "        talk_page_size = get_talk_page_size(title)\n",
        "\n",
        "        rows.append({\n",
        "            \"Article\": title,\n",
        "            \"Days Since Last Edit\": days_since_edit,\n",
        "            \"Word Count\": word_count,\n",
        "            \"Section Names\": sections,\n",
        "            \"Citations\": citations,\n",
        "            \"Citation Needed Count\": citation_needed,\n",
        "            \"Images\": images,\n",
        "            \"Categories\": categories,\n",
        "            \"Total Editors\": num_editors,\n",
        "            \"Edits Last Year\": recent_edits,\n",
        "            \"Talk Page Size (bytes)\": talk_page_size,\n",
        "            \"Last 3 Months Views\": views\n",
        "        })\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        rows.append({\n",
        "            \"Article\": title,\n",
        "            \"Days Since Last Edit\": 0,\n",
        "            \"Word Count\": 0,\n",
        "            \"Section Names\": \"\",\n",
        "            \"Citations\": 0,\n",
        "            \"Citation Needed Count\": 0,\n",
        "            \"Images\": 0,\n",
        "            \"Categories\": 0,\n",
        "            \"Total Editors\": 0,\n",
        "            \"Edits Last Year\": 0,\n",
        "            \"Talk Page Size (bytes)\": 0,\n",
        "            \"Last 3 Months Views\": 0\n",
        "        })\n",
        "\n",
        "    # Checkpoint saves\n",
        "    if i % CHECKPOINT_INTERVAL == 0:\n",
        "        df_checkpoint = pd.DataFrame(rows)\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        checkpoint_file = f\"{WIKIPROJECT}_{CLASS}_{EMAIL}_{MAX_ARTICLES}.csv\"\n",
        "        df_checkpoint.to_csv(checkpoint_file, index=False)\n",
        "        elapsed = time.time() - start_time\n",
        "        remaining = (elapsed / i) * (len(articles) - i)\n",
        "        print(f\"Checkpoint saved: {checkpoint_file}\")\n",
        "        print(f\"Elapsed: {elapsed/60:.1f}min | Estimated remaining: {remaining/60:.1f}min\")\n",
        "\n",
        "    time.sleep(REQUEST_DELAY)\n",
        "\n",
        "# Save final CSV and display dataframe\n",
        "if rows:\n",
        "    df = pd.DataFrame(rows)\n",
        "    #timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    filename = f\"{WIKIPROJECT}_{CLASS}_{EMAIL}_{MAX_ARTICLES}.csv\"\n",
        "    df.to_csv(filename, index=False)\n",
        "    filename = f\"/content/drive/MyDrive/{WIKIPROJECT}_{CLASS}_{EMAIL}_{MAX_ARTICLES}.csv\"\n",
        "    df.to_csv(filename, index=False)\n",
        "\n",
        "    elapsed_total = time.time() - start_time\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Successfully saved {len(df)} articles to: {filename}\")\n",
        "    print(f\"Total time: {elapsed_total/60:.1f} minutes\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n",
        "    # Display editable dataframe\n",
        "    print(\"Editable DataFrame:\")\n",
        "    display(df)\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"Summary statistics:\")\n",
        "    print(f\"{'='*60}\")\n",
        "    numeric_cols = ['Days Since Last Edit', 'Word Count', 'Citations', 'Citation Needed Count',\n",
        "                   'Images', 'Categories', 'Total Editors', 'Edits Last Year',\n",
        "                   'Talk Page Size (bytes)', 'Last 3 Months Views']\n",
        "    print(round(df[numeric_cols].describe(), 1))\n",
        "else:\n",
        "    print(\"\\nNo data to save\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1pPWf-puxWKAhrKYk3VVPRoa43gyAVqVN",
      "authorship_tag": "ABX9TyMkV5MNVdlMFpj+pCZGxld2",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}